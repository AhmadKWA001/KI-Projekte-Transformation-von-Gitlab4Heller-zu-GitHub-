{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_datapath= \"G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/data _from_external_device/CSV-Dateien/\"\n",
    "orginal_data= \"G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/validate/validate_Data/\"\n",
    "temp_file=''\n",
    "log_file= ''\n",
    "log_selected_columns= ['t_kss_tank', 't_raum', 't_spindel_rueck', 't_spindel_vor', 'volumenstrom', 'v_maschienenkuehler', 't_z_achse' ]#'t_y_achse', 'ch9', 'ch20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_date_format= \"%Y-%m-%d %H:%M:%S.%f\" #\"%Y.%m.%d %H:%M:%S\"# \"2022-02-25 15:30:00\"\n",
    "df_logs_date_format= \"%Y-%m-%d %H:%M:%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateparse (time_str):    \n",
    "    return pd.to_datetime(time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_numbers(df: pd.DataFrame, columns_list: list):\n",
    "    for column in columns_list:\n",
    "        new_column= df[column].str.replace(pat='\\+( )+',repl= '+',regex= True).str.replace(pat= '-( )+', repl= '-',regex= True)\n",
    "        df[column]= pd.to_numeric(new_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block(start_date:datetime, period: int, count_insertions: int, features: list):\n",
    "    sample_dict= {'date': [ dateparse(start_date) + timedelta(seconds= i*period) for i in range(1,count_insertions + 1)]}\n",
    "    for column in features:\n",
    "        sample_dict[column]= [np.NaN for i in range(count_insertions)] \n",
    "    #insertion_block= pd.DataFrame({'date': [ datetime.strptime(start_date,\"%Y-%m-%d %H:%M:%S\") + timedelta(seconds= i*period) for i in range(1,count_insertions + 1)],'t_bett': [np.NaN for i in range(count_insertions)],'t_motor':[np.NaN for i in range(count_insertions)], 't_spindel': [np.NaN for i in range(count_insertions)],'z_welle_ok':[np.NaN for i in range(count_insertions)]})\n",
    "    insertion_block= pd.DataFrame(sample_dict)\n",
    "    \n",
    "    return insertion_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_block(original_data: pd.DataFrame, period:int,  count_insertions: int, features):\n",
    "    output_df= None\n",
    "    for index, row in original_data.iterrows():\n",
    "        if index + 1 == len(original_data):\n",
    "            output_df= pd.concat([output_df,row.to_frame().T])\n",
    "        else:\n",
    "            start_date= row['date']\n",
    "            block= create_block(start_date= start_date, period= period, count_insertions= count_insertions, features= features)\n",
    "            #print('block', block)\n",
    "            if index == 0:\n",
    "                output_df= pd.concat([row.to_frame().T, block], axis=0, ignore_index= True)\n",
    "            else:\n",
    "                output_df= pd.concat([output_df, row.to_frame().T, block], axis=0, ignore_index= True)\n",
    "        #print(output_df[['date', 't_spindel_vor']])\n",
    "    return output_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sensor Files in one Data Frame and insert Nan blocks to prepare for the interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir= str(log_datapath)\n",
    "os.chdir(working_dir)\n",
    "result_df= None\n",
    "for file in glob.glob('*.CSV'):\n",
    "    print ('Current File: ',file)\n",
    "    content = open(file).readlines()\n",
    "    lines = [line_num for line_num, line_content in enumerate(content) if \"Data\" in line_content]\n",
    "    skipped_rows= list(range(0,lines[0]+1))\n",
    "    skipped_rows.append(lines[0]+2)\n",
    "    df_logs= pd.read_csv(file, sep=',',skiprows= skipped_rows, infer_datetime_format= True, parse_dates= True, date_parser= dateparse)#,index_col=['Date&Time'])\n",
    "    df_logs.drop(columns=[\"Alarm1-10\",\"Alarm11-20\",\"AlarmOut\",\"ms\",\"Number\"], inplace=True)\n",
    "    df_logs.rename(columns={'Date&Time':'date','CH1': 'T_KSS_TANK', 'CH2':'T_RAUM', 'CH3':'T_SPINDEL_RUECK', 'CH4':'T_SPINDEL_Vor', 'CH5':'Volumenstrom','CH6':'V_MASCHIENENKUEHLER','CH8':'T_Z_ACHSE'}, inplace= True)# 'CH7':'T_Y_ACHSE',\n",
    "    df_logs.columns= df_logs.columns.str.lower()\n",
    "    dates= df_logs['date']\n",
    "    df_logs= df_logs[log_selected_columns]\n",
    "    df_logs['date']= dates\n",
    "    df_logs= handle_special_numbers(df_logs, columns_list=['t_kss_tank', 't_raum', 't_spindel_vor','t_spindel_rueck', 'volumenstrom', 'v_maschienenkuehler', 't_z_achse'])\n",
    "    df_logs['t_spindel_rueck'] += 0.2\n",
    "    ###Upsample up to 1 second\n",
    "    df_logs= insert_block(df_logs, period= 1, count_insertions=19, features= log_selected_columns)\n",
    "    if result_df is None:\n",
    "        result_df= df_logs\n",
    "    else:\n",
    "        result_df= pd.concat([result_df, df_logs])\n",
    "    print('Current Size=',  len(result_df))\n",
    "df_logs= result_df\n",
    "df_logs_fields= df_logs.columns.to_list()\n",
    "print (df_logs_fields)\n",
    "print(df_logs.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs= result_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of the features and their required types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_dict= { column:'float' for column in log_selected_columns}\n",
    "float_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set feature types as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs= df_logs.astype(float_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_interpolated = df_logs.interpolate(method='linear', order=3, axis= 0,inplace= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_interpolated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_interpolated.to_csv(str(log_datapath+'combinded_interpolated_1s.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load files from other datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_datapath= \"G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/data _from_external_device/integration/\"\n",
    "df_logs= pd.read_csv(str(log_datapath+'combinded_interpolated_1s.csv'), infer_datetime_format= True, parse_dates= True, date_parser= dateparse)#pd.read_csv(filepath_or_buffer= str(log_datapath+'combinded_interpolated_1s.csv'))\n",
    "temp_data= pd.read_csv(str(log_datapath+'all_data.csv'), infer_datetime_format= True, parse_dates= True, date_parser= dateparse)#pd.read_csv(filepath_or_buffer= str(log_datapath+'all_data.csv'))\n",
    "df_logs= df_logs.drop(columns=['Unnamed: 0'])\n",
    "# df_logs['date']= datetime.strftime(df_logs['date'],format= df_logs_date_format)\n",
    "# df_logs['date']= datetime.strptime(df_logs['date'],format= df_logs_date_format)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a part of the required features in the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_list(df: pd.DataFrame, df_name: str):\n",
    "    df_logs_fields= df.columns.to_list()\n",
    "    fields= \"\"\n",
    "    for field in df_logs_fields:\n",
    "        if fields == \"\":\n",
    "            fields =  \"{df_name}.\".format(df_name= df_name) + field\n",
    "        else:\n",
    "            fields += \", {df_name}.\".format(df_name= df_name) + field\n",
    "    return fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of features for each datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_fields=get_field_list(df_logs, \"df_logs\")\n",
    "temp_data_fields=get_field_list(temp_data, \"temp_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the query to aggregate the two data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Apply Query\n",
    "query= \"\"\"\n",
    "select * from \n",
    "    (select temp_data.start as start, temp_data.end as end,{temp_data_fields}, min(df_logs.ts)  as ts from \n",
    "        (\n",
    "        (select  {temp_data_fields}, strptime(temp_data.date,'{temp_data_date_format}') as start, (strptime(temp_data.date,'{temp_data_date_format}') + interval 5 Second) as end   from temp_data) as temp_data\n",
    "        join\n",
    "        (select {df_logs_fields}, strptime(df_logs.date ,'{df_logs_date_format}') as ts from df_logs) as df_logs\n",
    "        on (df_logs.ts >= temp_data.start) and ( df_logs.ts < temp_data.end)\n",
    "        )\n",
    "    group by temp_data.start, temp_data.end, {temp_data_fields}) combined_logs\n",
    "join\n",
    "    df_logs\n",
    "on (combined_logs.ts = strptime(df_logs.date ,'{df_logs_date_format}'))\n",
    "order by start Asc\n",
    "\"\"\".format(temp_data_fields= temp_data_fields, df_logs_fields= df_logs_fields,\n",
    "            temp_data_date_format= temp_data_date_format, df_logs_date_format= df_logs_date_format)\n",
    "results= ddb.query(query)\n",
    "result_df= results.to_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the correctness of aggregation based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[['start','date', 'end', 'ts', 'date_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove doubled columns and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= result_df.drop(columns=['start', 'end', 'ts'])\n",
    "new_df.to_csv(str(log_datapath+'aggregated_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= new_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "#plt.savefig(images_analysis_path.format(data_mode,'row_data_corr.png') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_nr= new_df['experiment_nr'].min()\n",
    "max_nr= new_df['experiment_nr'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min_nr, max_nr+1):\n",
    "    to_save_df= new_df[new_df['experiment_nr']== i]\n",
    "    if (len(to_save_df) > 0):\n",
    "        to_save_df.to_csv(str(log_datapath+ str(i)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns= [\n",
    " 't_bett',\n",
    " 't_motor',\n",
    " 't_spindle',\n",
    " 'DRZ2',\n",
    " 'M8',\n",
    " 'M121',\n",
    " 'M127',\n",
    " 'M7',\n",
    " 'welle_z_iterpolated',\n",
    " 't_kss_tank',\n",
    " 't_raum',\n",
    " 't_spindel_rueck',\n",
    " 't_spindel_vor',\n",
    " 'volumenstrom',\n",
    " 'v_maschienenkuehler',\n",
    " 't_z_achse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min_nr, max_nr+1):\n",
    "    to_save_df= new_df[new_df['experiment_nr']== i]\n",
    "    if (len(to_save_df) > 0):\n",
    "        fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns, vertical_spacing=0.02)\n",
    "        for j in range(len(selected_columns)):\n",
    "            fig.add_trace(go.Scatter(x= to_save_df['date'], y= to_save_df[selected_columns[j]], name=selected_columns[j], mode= 'lines'), row= j+1, col= 1)\n",
    "            fig.update_yaxes(title_text= selected_columns[j], row= i+1, col= 1)\n",
    "        fig.update_layout(height=900, width=1200, title_text=str(i))\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98516c4905fb8467ece250085a11958ff6a81ba629b2fa3655ee37336959c16d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
