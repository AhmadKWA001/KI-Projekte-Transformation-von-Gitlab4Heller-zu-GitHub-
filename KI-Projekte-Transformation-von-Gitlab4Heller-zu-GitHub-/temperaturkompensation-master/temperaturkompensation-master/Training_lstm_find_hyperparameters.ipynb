{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import silu, swish, linear, tanh, leaky_relu, sigmoid, relu, elu, selu, gelu, linear\n",
    "from keras.layers import Input, LSTM, Dense, GaussianNoise, BatchNormalization, Activation, PReLU, SimpleRNN\n",
    "from keras.initializers.initializers_v2 import RandomNormal, RandomUniform\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "from keras.optimizers import nadam_v2, rmsprop_v2, adamax_v2\n",
    "from keras.losses import mean_absolute_error as mae, mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from keras.models import Sequential\n",
    "import onnx\n",
    "import tf2onnx\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, KernelPCA,IncrementalPCA,SparsePCA\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "%load_ext tensorboard\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'analysis_version'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for expreiments .csvs\n",
    "source_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/'\n",
    "testdata_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/Testing_data/'\n",
    "preprocessor_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "model_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "analysis_path= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Analysis_&_feature_Engineering\\\\'\n",
    "log_dir = \"C:/Temperaturkompensation/logs/\" + \\\n",
    "    datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "weights_path = str(model_dir+\"/weights\")\n",
    "image_path = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "def load_model(path):\n",
    "    model = onnx.load_model(path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import LossFunctionWrapper, mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras.backend as k\n",
    "\n",
    "\n",
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(\n",
    "                    tf.reduce_max(tf.abs((y_true - y_pred))),\n",
    "                    mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "\n",
    "def smoothed_max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    a=1.5\n",
    "    diffs= tf.abs((y_true - y_pred))\n",
    "    muls= tf.multiply(a, diffs)\n",
    "    exps= tf.exp(muls)\n",
    "    weighted_muls= tf.multiply(diffs, exps)\n",
    "    nominater= tf.reduce_sum(weighted_muls)\n",
    "    denominator= tf.reduce_sum(exps)\n",
    "    boltzmann_operator= nominater/denominator\n",
    "    error = tf.add(boltzmann_operator, mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "\n",
    "def max_loss_v1(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.multiply(tf.reduce_max(tf.abs((y_true - y_pred))), 1), tf.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return error\n",
    "\n",
    "def max_loss_v2(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.pow(tf.reduce_max(tf.abs((y_true - y_pred))),2), mean_squared_error(y_true, y_pred))\n",
    "    return error\n",
    "\n",
    "def max_loss_v3_mae_std(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mean_ae= mean_absolute_error(y_true, y_pred)\n",
    "    #vars= tf.pow(tf.subtract(tf.abs(y_true - y_pred),mean_ae),2)\n",
    "    vars= tf.abs(tf.subtract(tf.abs(y_true - y_pred),mean_ae))\n",
    "    errors_std= tf.reduce_mean(vars)\n",
    "    #error = tf.add(mean_squared_error(y_true, y_pred), errors_std)\n",
    "    error = tf.add(mean_ae, errors_std)\n",
    "    return error\n",
    "\n",
    "def loss_mse_vars(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mean_ae= mean_absolute_error(y_true, y_pred)\n",
    "    variances= tf.pow(tf.subtract(tf.abs(y_true - y_pred),mean_ae),2)\n",
    "    var= tf.reduce_mean(variances)\n",
    "    error = tf.add(mean_squared_error(y_true, y_pred), var)\n",
    "    return error\n",
    "\n",
    "# def calc_fidelity(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "#     # session= k.get_session()\n",
    "#     # y_true_array= session.run(y_true)\n",
    "#     # y_pred_array= session.run(y_pred)\n",
    "#     # cc= np.corrcoef(y_true_array,y_pred_array)\n",
    "#     NRMSE= tf.divide(tf.sqrt(mean_squared_error(y_true, y_pred)),tf.subtract(tf.reduce_max(y_true),tf.reduce_min(y_true)))\n",
    "#     fidelity= tf.abs(tf.multiply(100.0, tf.subtract(1.0,NRMSE)))\n",
    "#     return fidelity\n",
    "    \n",
    "special_loss = LossFunctionWrapper(smoothed_max_loss)\n",
    "#fidelity_wrapper= LossFunctionWrapper(calc_fidelity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### window  represents time period by each entry in the buffer\n",
    "### Shift represents the jump from value to next one in the buffer\n",
    "### sampling rate \n",
    "def generateDataSource(signal_input=None, input_columns: list = [], output_length: int = 1, signal_output=None, window=1, shift=1, sample_rate=1):\n",
    "    #subsequence_len= (window -1) *shift + 1\n",
    "    subsequence_len= (window) *shift\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    #num_samples = int((Signal_Length - (window-1) * shift))\n",
    "    #num_samples = int((Signal_Length - window + 1) / shift)\n",
    "    num_samples = 1 + int((Signal_Length - subsequence_len) / sample_rate)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i] = np.asarray([signal_input[i*sample_rate + j * shift] for j in range(0,window)])\n",
    "        y[i] = signal_output[i*sample_rate + (window-1) * shift :i*sample_rate+ (window-1) * shift + output_length]\n",
    "#        print('x[{}]= {}'.format(i, x[i]))\n",
    "#        print('y[{}]= {}'.format(i, y[i]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.asarray([\n",
    "    [1,1,1,1,1],\n",
    "    [2,2,2,2,2],\n",
    "    [3,3,3,3,3],\n",
    "    [4,4,4,4,4],\n",
    "    [5,5,5,5,5],\n",
    "    [6,6,6,6,6],\n",
    "    [7,7,7,7,7],\n",
    "    [8,8,8,8,8],\n",
    "    [9,9,9,9,9],\n",
    "    [10,10,10,10,10],\n",
    "    [11,1,1,1,1],\n",
    "    [12,2,2,2,2],\n",
    "    [13,3,3,3,3],\n",
    "    [14,4,4,4,4],\n",
    "    [15,5,5,5,5],\n",
    "    [16,6,6,6,6],\n",
    "    [17,7,7,7,7],\n",
    "    [18,8,8,8,8],\n",
    "    [19,9,9,9,9],\n",
    "    [100,10,10,10,10]\n",
    "])\n",
    "y= np.asanyarray([11,22,33,44,55,66,77,88,99,1010,11,22,33,44,55,66,77,88,99,1010])\n",
    "generateDataSource(signal_input=x,output_length=1, signal_output=y,window=3,shift=2,sample_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement special preprocessing steps\n",
    "# 1 calculate new Feature\n",
    "# 2 make rounding\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def special_preprocess(data: np.ndarray):\n",
    "    delta_t_spindel_bett = data[:, 1] - data[:, 0]\n",
    "    delta_t_spindel_bett = delta_t_spindel_bett.reshape((-1, 1))\n",
    "    result = np.concatenate((data[:, 2:5], delta_t_spindel_bett), axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def rounding(signals: np.ndarray):\n",
    "    ##rounding t_motor to nearest integer\n",
    "    signals[:,1]= np.round(signals[:,1])\n",
    "    signals= np.round(signals, 2)\n",
    "    return signals\n",
    "\n",
    "def rounding2(signals: np.ndarray):\n",
    "    return np.round(signals, 5)\n",
    "\n",
    "\n",
    "def remove_t_bett(data: np.ndarray):\n",
    "    result = data[:, 1:]\n",
    "    result[:, 0] = result[:, 0] - data[:, 0]\n",
    "    result[:, 1] = result[:, 1] - data[:, 0]\n",
    "    return result\n",
    "\n",
    "def remove_t_bett_not_delete(data: np.ndarray):\n",
    "    result = data.copy()\n",
    "    result[:, 1] = result[:, 1] - result[:, 0]\n",
    "    result[:, 2] = result[:, 2] - result[:, 0]\n",
    "    return result\n",
    "\n",
    "data = np.asarray([[24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0]])\n",
    "data.shape\n",
    "print(remove_t_bett(data))\n",
    "print(data)\n",
    "\n",
    "window = 60\n",
    "shift = 6\n",
    "sampling_rate= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_Columns= ['diff', 'drhz','t_bett',]#,'t_kluelung']# , 'diff'\n",
    "# [ 't_bett','t_spindel', 't_y_achse', 't_z_achse', 'drhz', 't_motor','t_kss_tank', 't_raum', 't_spindel_vor', 't_spindel_rueck','t_schwenkantrieb']\n",
    "\n",
    "output_variable = 'welle_z_iterpolated'#'welle_z_upsampled'#'z_welle_ok'  #'z_tcp_ok'# 'welle_z'#''#'smoothed_z_welle_ok'#\n",
    "smoothed_output= 'smoothed_welle_z_iterpolated'\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "pca = PCA(n_components=6)\n",
    "#pca= IncrementalPCA(n_components=6)\n",
    "# Data Parameters\n",
    "selected_Columns = ['t_bett', 't_spindle','t_motor','M8','M121', 'M127', 'M7']#['t_bett', 't_spindle','t_motor', 'M8','M121', 'M127', 'M7']\n",
    "#,('smoothing_data', FunctionTransformer(smoothing_data))\n",
    "proposed_pipline = Pipeline(steps=[('t_bett_removal', FunctionTransformer(remove_t_bett)), ('rounding', FunctionTransformer(rounding)), ('stdscaler', scaler),('pca', pca), ('rounding2', FunctionTransformer(rounding2))])  # Pipeline(steps=[('rounding', FunctionTransformer(rounding))])#\n",
    "preprocessor_name = \"preprocessor.p\"\n",
    "post_processer= StandardScaler(with_mean=True, with_std=True)#MinMaxScaler(feature_range=(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smoothing_data(input: pd.DataFrame, window= 240):\n",
    "#     shift_value= int(-window/2)\n",
    "#     data= input.copy()\n",
    "#     # data['t_bett_smoothed']= data['t_bett'].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data['t_motor_smoothed']= data['t_motor'].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data['t_spindle_smoothed']= data['t_spindle'].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data[str(output_variable+'_smoothed')]= data[output_variable].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data['t_bett']= data['t_bett'].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data['t_motor']= data['t_motor'].rolling(window= window).mean().shift(shift_value)\n",
    "#     # data['t_spindle']= data['t_spindle'].rolling(window= window).mean().shift(shift_value)\n",
    "#     data[smoothed_output]= data[output_variable].rolling(window= window).mean().shift(shift_value)\n",
    "#     data= data[ window+1:-window]###Remove nan\n",
    "#     return data\n",
    "#     #return input\n",
    "def smoothing_data(input: pd.DataFrame, window= 240):\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files():\n",
    "    for file in glob.glob('*.csv'):\n",
    "        if file == 'combined.csv':\n",
    "            continue\n",
    "        print('Current File: ', file)\n",
    "        df = pd.read_csv(file)\n",
    "        signals = df\n",
    "        # print('Nans: ',signals.isnull().values.any())\n",
    "        # print('places: ', signals.loc[pd.isnull(signals).any(1), :].index.values)\n",
    "        prev_time=None\n",
    "        drop_list= []\n",
    "        ####Clean redundant data\n",
    "        print('Before Filtering, ', len(signals))\n",
    "        for index,row in signals.iterrows():\n",
    "            current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            if prev_time is None:\n",
    "                prev_time= current_time\n",
    "            elif (current_time - prev_time).total_seconds() < 5:\n",
    "                drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "                df= df[df['date'] != row['date']]\n",
    "                current_time= prev_time\n",
    "            prev_time= current_time\n",
    "        #################Smoothing########################\n",
    "        df.to_csv(path_or_buf=str('filtered_'+file))\n",
    "        signals= df\n",
    "        print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode= 'all_smooth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\Edgebox\\\\Versuchsdaten\\\\5s_data\\\\'\n",
    "sn.set(font_scale=1.8)\n",
    "working_dir = source_dir\n",
    "os.chdir(working_dir+'validate_Data\\\\')\n",
    "training_data = None\n",
    "targets = None\n",
    "needed_columns = selected_Columns.copy()\n",
    "needed_columns.append(output_variable)\n",
    "min_len= 100000000000\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    df = pd.read_csv(file)\n",
    "    print('Current File: ', file)\n",
    "    #print('current len: ', len(df))\n",
    "    #if min_len > len(df):\n",
    "    #    min_len= len(df)\n",
    "    #print('Min:', min_len)\n",
    "    smoothed_signals= smoothing_data(df, window= 24)\n",
    "    signals= smoothed_signals[needed_columns]\n",
    "    #signals.t_motor= signals.t_motor.round()\n",
    "    if training_data is None:\n",
    "        training_data = signals\n",
    "    else:\n",
    "        training_data = pd.concat([training_data, signals],axis=0, ignore_index= True)\n",
    "    training_data= training_data[needed_columns]\n",
    "    print(len(training_data))\n",
    "training_data.reset_index(inplace=True)\n",
    "training_data= training_data[needed_columns]\n",
    "print(training_data.columns)\n",
    "print('Nans: ',training_data.isnull().values.any())\n",
    "print('places: ', training_data.loc[pd.isnull(training_data).any(1), :].index.values)\n",
    "print(len(training_data))\n",
    "\n",
    "# correlations = training_data.corr()\n",
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# plt.title('Heat-Map für die Korrelationsmatrix')\n",
    "# sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "# plt.show()\n",
    "# fig, ax= plt.subplots(figsize=(15,15))\n",
    "# plt.plot(training_data['t_spindel'],label= 't_spindel')\n",
    "# plt.plot(training_data['t_motor'],label= 't_motor')\n",
    "# plt.plot(training_data['t_spindel_vor'],label= 't_spindel_vor')\n",
    "# plt.plot(training_data['t_spindel_rueck'],label= 't_spindel_rueck')\n",
    "# plt.plot(training_data['t_raum'],label= 't_raum')\n",
    "# plt.plot(training_data['M8']*50,label= 'M8')\n",
    "# plt.plot(training_data['M7']*50,label= 'M7')\n",
    "# plt.plot(training_data['M127']*50,label= 'M127')\n",
    "# plt.plot(training_data['M121']*50,label= 'M121')\n",
    "# plt.plot(training_data['t_kss_tank'],label= 't_kss_tank')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "#selected_columns= needed_columns\n",
    "# y_axis_names = needed_columns\n",
    "# fig = make_subplots(rows=len(needed_columns)+1, cols=1, shared_xaxes=True,\n",
    "#                     print_grid=True, subplot_titles=needed_columns, vertical_spacing=0.02)\n",
    "# for i in range(len(needed_columns)):\n",
    "#     # x= training_data['start'],\n",
    "#     fig.add_trace(go.Line(y=training_data[needed_columns[i]]), row=i+1, col=1)\n",
    "#     fig.update_yaxes(title_text=y_axis_names[i], row=i+1, col=1)\n",
    "# fig.add_trace(go.Line(y=targets), row=len(needed_columns) +\n",
    "#               1, col=1)  # x= training_data['start'],\n",
    "# fig.update_yaxes(title_text=output_variable, row=len(needed_columns), col=1)\n",
    "# fig.update_layout(height=1200, width=1200, title_text=\"LF-Data\")\n",
    "# fig.show()\n",
    "# print('input shape: ', training_data.shape)\n",
    "# print('output shape: ', targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_analysis_path= str(analysis_path+'{}_{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.loc[pd.isnull(training_data).any(1), :].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= training_data.hist(figsize= (15,15))\n",
    "fig= ax[0][0].get_figure()\n",
    "plt.xlabel('values')\n",
    "plt.ylabel('counts')\n",
    "#plt.savefig(images_analysis_path.format(data_mode,'row_data_hist.png') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= training_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Mode: '+ data_mode+' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "#plt.savefig(images_analysis_path.format(data_mode,'row_data_corr.png') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Standardization\n",
    "scaler= StandardScaler(with_mean= True, with_std= True)\n",
    "scaled_data= scaler.fit_transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_df= pd.DataFrame(scaled_data, columns= list(training_data.columns))\n",
    "scaled_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= scaled_data_df.hist(figsize= (15,15))\n",
    "fig= ax[0][0].get_figure()\n",
    "#plt.savefig(images_analysis_path.format(data_mode,'scaled_data_hist.png') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.asanyarray(np.isnan(scaled_data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= scaled_data_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Mode: '+ data_mode+' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "#plt.savefig(images_analysis_path.format(data_mode,'scaled_data_corr.png') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca=PCA(n_components=7)\n",
    "# data_decomposed= pca.fit_transform(scaled_data_df[['t_bett',\t't_spindle','t_motor',\t'M8',\t'M121',\t'M127','M7']])\n",
    "# print('explanation_ratio= ',  pca.explained_variance_ratio_, 'precision: ', pca.get_precision())\n",
    "# data_decomposed_df= pd.DataFrame(data_decomposed)\n",
    "# #data_decomposed_df['org_label']= training_data[output_variable]\n",
    "# #data_decomposed_df['scaled_label']= scaled_data_df[output_variable]\n",
    "# data_decomposed_df['scaled_smoothed_label']= scaled_data_df[output_variable]\n",
    "# ax= data_decomposed_df.hist(figsize= (15,15))\n",
    "# fig= ax[0][0].get_figure()\n",
    "# plt.savefig(images_analysis_path.format(data_mode,'decomp_data_hist.png') )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations= data_decomposed_df.corr()\n",
    "# fig, ax = plt.subplots(figsize=(25, 25))\n",
    "# plt.title('Mode: '+ data_mode+' Heat-Map für die Korrelationsmatrix')\n",
    "# ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "# plt.savefig(images_analysis_path.format(data_mode,'decomp_data_corr.png') )\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data source from all signals and create Training dataset\n",
    "working_dir = source_dir\n",
    "os.chdir(working_dir)\n",
    "results: np.ndarray = None\n",
    "targets: np.ndarray = None\n",
    "pipeline = None\n",
    "reset = True\n",
    "training_data = None\n",
    "output= None\n",
    "vis_columns= []\n",
    "batch_size= 1000000000000000000\n",
    "#selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    smoothed_signals= smoothing_data(df, window= 24)\n",
    "    # fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "    #                 print_grid=True,  vertical_spacing=0.02)\n",
    "    # fig.add_trace(go.Line(y=smoothed_signals[output_variable], name='smoothed_signal Loss'), row=1, col=1)\n",
    "    # fig.add_trace(go.Line(y=smoothed_signals[output_variable].ewm(alpha=0.1).mean(), name='Origial_signal'), row=1, col=1)\n",
    "    # fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "    # fig.update_yaxes(title_text='Max_Loss', row=1, col=1)\n",
    "    # fig.update_layout(height=900, width=900, title_text='training_curve')\n",
    "    # fig.show()\n",
    "    print('signals size= ', len(smoothed_signals))\n",
    "    if smoothed_signals.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    if batch_size > len(smoothed_signals):\n",
    "        batch_size= len(smoothed_signals)\n",
    "    if training_data is None:\n",
    "        output= smoothed_signals[output_variable]\n",
    "        training_data = smoothed_signals[selected_Columns]\n",
    "    else:\n",
    "        output= pd.concat([output, smoothed_signals[output_variable]], axis=0, ignore_index= True)\n",
    "        training_data = pd.concat([training_data, smoothed_signals[selected_Columns]], axis=0, ignore_index= True)\n",
    "    training_data= training_data[selected_Columns]\n",
    "    print(len(training_data),' ',len(output))\n",
    "# vis_columns= selected_Columns.copy()\n",
    "# #vis_columns.append(str(output_variable+'_smoothed'))\n",
    "# #training_data[output_variable]= output.tolist()\n",
    "# testdf= training_data[vis_columns]\n",
    "# correlations= pd.DataFrame(testdf, columns= vis_columns).corr()\n",
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# plt.title('Mode: '+ data_mode+' Heat-Map für die Korrelationsmatrix')\n",
    "# ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "# plt.show()\n",
    "print('Length of training data: ', len(training_data))\n",
    "training_data= training_data.to_numpy()\n",
    "print('Input Nans: ',np.isnan(training_data).any())\n",
    "print('Possible batch_size= ', batch_size)\n",
    "###################################################################################################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= None\n",
    "results: np.ndarray = None\n",
    "targets: np.ndarray = None\n",
    "if pipeline is None:\n",
    "    try:\n",
    "        if not reset:\n",
    "            pipeline = load(str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is loaded successfully')\n",
    "        else:\n",
    "            pipeline = proposed_pipline\n",
    "            pipeline = pipeline.fit(training_data)\n",
    "            dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is saved successfully')\n",
    "            print('explained_variance_ratio_= ', pipeline.steps[3][1].explained_variance_ratio_)\n",
    "    except FileNotFoundError as err:\n",
    "        pipeline = proposed_pipline\n",
    "        pipeline = pipeline.fit(training_data)\n",
    "        dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "        initial_type = [('float_input', FloatTensorType(\n",
    "            [None,  training_data.shape[1]]))]\n",
    "        onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "        with open(str(preprocessor_dir + \"preprocessor.onnx\"), \"wb\") as f:\n",
    "            f.write(onx.SerializeToString())\n",
    "        print('preprocessor is saved successfully')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and transform training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    smoothed_signals= smoothing_data(df, window= 24)\n",
    "    print('signals size= ', len(smoothed_signals))\n",
    "    if smoothed_signals.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    labels = (1000* smoothed_signals[output_variable]).round(3)#(1000* df[output_variable]).round(3)#.round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)#1000* df[output_variable].round(3)#)\n",
    "    smoothed_signals = smoothed_signals[selected_Columns]\n",
    "    smoothed_signals = smoothed_signals.to_numpy()\n",
    "    print('shape before transformation: ', smoothed_signals.shape)\n",
    "    signals_tranformed = pipeline.transform(smoothed_signals)\n",
    "    partitions, target = generateDataSource(signal_input=signals_tranformed, input_columns=selected_Columns, output_length=1, signal_output=labels, window=window, shift=shift, sample_rate=sampling_rate)\n",
    "    if results is None:\n",
    "        results = partitions\n",
    "        targets = target\n",
    "    else:\n",
    "        results = np.concatenate((results, partitions), axis=0)\n",
    "        targets = np.concatenate((targets, target), axis=0)\n",
    "    print('input: ', results.shape, ' output:', targets.shape)\n",
    "print(targets.min(), '    ',targets.max())\n",
    "targets_post= post_processer.fit_transform(targets.reshape(-1,1))\n",
    "dump(post_processer,str(preprocessor_dir+'postprocesser.p') )\n",
    "print('input: ', results.shape, ' output:', targets_post.shape)\n",
    "print(targets_post.min(), '    ',targets_post.max())\n",
    "print('Input Nans: ',np.isnan(results).any())\n",
    "print('Output Nans: ',np.isnan(targets_post).any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data source from all signals\n",
    "working_dir = testdata_dir\n",
    "os.chdir(working_dir)\n",
    "test_results: np.ndarray = None\n",
    "test_targets: np.ndarray = None\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    signals= smoothing_data(df, window= 24)\n",
    "    print('signals size= ', len(signals))\n",
    "    if signals.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    labels = (1000* signals[output_variable]).round(3)#(1000* df[output_variable]).round(3)#.round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)#1000* df[output_variable].round(3)#)\n",
    "    signals = signals[selected_Columns]\n",
    "    signals = signals.to_numpy()\n",
    "    print('shape before transformation: ', signals.shape)\n",
    "    signals_transformed = pipeline.transform(signals)\n",
    "    partitions, target = generateDataSource(signal_input=signals_transformed, input_columns=selected_Columns, output_length=1, signal_output=labels, window=window, shift=shift, sample_rate=sampling_rate)\n",
    "    if test_results is None:\n",
    "        test_results = partitions\n",
    "        test_targets = target\n",
    "    else:\n",
    "        test_results = np.concatenate((test_results, partitions), axis=0)\n",
    "        test_targets = np.concatenate((test_targets, target), axis=0)\n",
    "    print('input: ', test_results.shape, ' output:', test_targets.shape)\n",
    "print(test_targets.min(), '    ',test_targets.max())\n",
    "test_targets_post= post_processer.transform(test_targets.reshape((-1,1)))\n",
    "print('input: ', test_results.shape, ' output:', test_targets_post.shape)\n",
    "print(test_targets_post.min(), '    ',test_targets_post.max())\n",
    "print('Input Nans: ',np.isnan(test_results).any())\n",
    "print('Output Nans: ',np.isnan(test_targets_post).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(old_value, old_min, old_max, new_max, new_min):\n",
    "    new_value= ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "    old_min = 0\n",
    "    old_max = 0\n",
    "    y_train_rounded = np.round(y_train,decimals=3)\n",
    "    N = len(y_train_rounded)\n",
    "    y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "    weights = np.ones(shape=(N,),dtype= np.float32)\n",
    "    for i in range(0, N):\n",
    "        current_value = y_train_rounded[i]\n",
    "        occurences = np.count_nonzero(y_train_rounded == current_value)\n",
    "        if occurences <= occurance_threshold:\n",
    "             weights[i]=  N/(occurences + 1 )#N/(occurences + y_train_rounded[i])#N/occurences # 0\n",
    "        else:\n",
    "            weights[i] =  N/(occurences + 1)#(8 if y_train_rounded[i] < 20 else 1)\n",
    "    old_min= weights.min()\n",
    "    old_max= weights.max()\n",
    "    new_min = 1\n",
    "    new_max = 1.1\n",
    "    #weights_scaled= np.apply_along_axis(map, 1, weights)\n",
    "    weights_scaled= np.asanyarray([ map(weights[i], old_min, old_max, new_max, new_min) for i in range(0, weights.shape[0])]).reshape(weights.shape)\n",
    "    #weights = weights - weights.min() + 1\n",
    "    #weights = (weights - weights.min())/(weights.max() - weights.min())\n",
    "    fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=weights_scaled,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 1, col= 1)\n",
    "    fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 2, col= 1)\n",
    "    # fig.update_yaxes(title_text= 'weight', row= 1, col= 1)\n",
    "    # fig.update_xaxes(title_text= 'label value', row= 1, col= 1)\n",
    "    fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "    fig.show()\n",
    "\n",
    "    return weights_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Model_LSTM(input_shape, activation_Function = selu,\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.00,\n",
    "    l2_v = 0.00,\n",
    "    structure= [],\n",
    "    n_hidden_layers= 1,\n",
    "    n_units=1,\n",
    "    optimizer= 'Nadam'):\n",
    "    #structure=  [n_units for i in range(0,n_hidden_layers)]#[50,50,40,40,30,30,20]##   \n",
    "    unroll = False\n",
    "    kernal_init = 'he_normal'#RandomUniform()  # \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    for i in range(1,len(structure)+1):\n",
    "        layer_size= structure[i-1]\n",
    "        if i == len(structure):\n",
    "            model.add(LSTM(layer_size,stateful= False,return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        else:\n",
    "            model.add(LSTM(layer_size,stateful= False, return_sequences=True,unroll=unroll, kernel_initializer= kernal_init,dropout=dropout,  kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation_Function))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(PReLU())\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss= special_loss, metrics=[ max_loss])#'Adagrad'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training,x_testing,y_training,y_testing= train_test_split(results, targets, test_size=0.1,random_state= 42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Block\n",
    "checkpoint = ModelCheckpoint(filepath=str(model_dir+'best_model.h5'), monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'activation_Function': ['relu', 'tanh', 'elu', 'selu'],\n",
    "    'n_units':range(5,50,5),\n",
    "    'n_hidden_layers': range(1,6,1),\n",
    "    'optimizer': ['Nadam', 'Adamax', 'Adagrad'],\n",
    "    'epochs': range(100,600,100),\n",
    "    'batch_size': range(500,2000,100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring_function= make_scorer(score_func= max_loss, greater_is_better= False, needs_proba= False,)\n",
    "#'dropout': np.linspace(start=0, stop=0.5, num=10),'l1_v': np.linspace(start=0, stop=0.001, num=10),'l2_v': np.linspace(start=0, stop=0.001, num=10),\n",
    "params= {\n",
    "    'n_units':range(10,25,5),\n",
    "    'n_hidden_layers': range(1,4,1)\n",
    "   }\n",
    "model_wrapper= KerasRegressor(build_fn= build_Model_LSTM, input_shape= (results.shape[1], results.shape[2]),activation_Function= 'relu', n_units= 1, n_hidden_layers= 1, optimizer= 'Nadam',  epochs= 500, batch_size= 50, callbacks=[earlyStopping], verbose=2, workers=32, use_multiprocessing=True)\n",
    "random_grid= RandomizedSearchCV(estimator= model_wrapper, param_distributions= params, n_iter=100, scoring= mean_absolute_error, n_jobs= -1, cv=10,verbose=3)\n",
    "random_grid= GridSearchCV(estimator= model_wrapper,param_grid= params, scoring= scoring_function, n_jobs= -1, cv=10,verbose=3)\n",
    "fitted= random_grid.fit(X= results, y= targets, shuffle= False,batch_size= 1200)\n",
    "print(random_grid.best_score_)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = calculate_weights(y_train=y_training, occurance_threshold= 5)\n",
    "model = build_Model_LSTM( (x_training.shape[1], x_training.shape[2]),activation_Function = selu,\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.0000,\n",
    "    l2_v = 0.0000,\n",
    "    n_hidden_layers= 2,\n",
    "    n_units=10,\n",
    "    structure=[5],\n",
    "    optimizer= 'Nadam')\n",
    "reset= True##############################################################################################################\n",
    "try:\n",
    "    if not reset:\n",
    "        #model.set_weights(model_w)\n",
    "        #model.load_weights(filepath=weights_path)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print(model.output_shape)\n",
    "print(model.summary())\n",
    "summary = model.fit(x=x_training, y=y_training, shuffle=False, batch_size= int(batch_size/2),  epochs=15000, validation_data=(x_testing, y_testing),  #int(batch_size * 0.5)\n",
    "                    callbacks=[earlyStopping], verbose=2, workers=32, use_multiprocessing=True, sample_weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "input_signature = [tf.TensorSpec([None, results.shape[1], results.shape[2]], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model=model, input_signature=input_signature, opset=10)\n",
    "onnx.save_model(onnx_model, str(model_dir+'model.onnx'))\n",
    "print(\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w= model.get_weights()\n",
    "#model.save_weights(weights_path, save_format='h5')l\n",
    "#model.save_weights(weights_path, save_format='tf')\n",
    "#model.save(str(model_dir+'testmodel.h5'), save_format='h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(filepath=weights_path)\n",
    "metric= 'mae_loss_loss'\n",
    "metric_loss = summary.history['loss']\n",
    "metric_val_loss = summary.history['val_loss']\n",
    "max_loss_res= summary.history['max_loss']\n",
    "val_max_loss_res= summary.history['val_max_loss']\n",
    "#mae_loss= summary.history['mean_absolute_error']\n",
    "#val_mae_loss= summary.history['val_mean_absolute_error']\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "                    print_grid=True,  vertical_spacing=0.02)\n",
    "fig.add_trace(go.Line(y=max_loss_res, name='Training Max_Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=val_max_loss_res , name='Validation Max_Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=metric_loss, name='Training {}'.format(metric)), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=metric_val_loss, name='Validation {}'.format(metric)), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=mae_loss, name='Training {}'.format('mae')), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=val_mae_loss, name='Validation {}'.format('mae')), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_layout(height=900, width=900, title_text='Training Curve: mae vs. {}'.format(metric))\n",
    "fig.show()\n",
    "pio.write_image(fig, str(image_path+'mae_vs_{}.png'.format(metric)), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####Save Model as onnx from weights\n",
    "# from keras.models import load_model\n",
    "# import tf2onnx\n",
    "# import onnx\n",
    "# import tensorflow as tf\n",
    "# model= Sequential()\n",
    "# model= load_model(filepath= str(model_dir+'model.h5'), compile= False)\n",
    "# model.load_weights(weights_path)\n",
    "# model.save(str(model_dir+'best_model.h5'), save_format='h5')\n",
    "# input_signature= [tf.TensorSpec([None,results.shape[1], results.shape[2]],tf.float32, name= 'x')]\n",
    "# onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "# onnx.save_model(onnx_model, str(model_dir+'Best_model.onnx'))\n",
    "# print (\".onnx model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tolerance = 5\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline = x_steps + tolerance * np.sin(np.pi/4)\n",
    "# Create Tolerance Lines\n",
    "tolerance = 10\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline_actual = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline_actual = x_steps + tolerance * np.sin(np.pi/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tolerance = 5\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline = x_steps + tolerance * np.sin(np.pi/4)\n",
    "# Create Tolerance Lines\n",
    "tolerance = 10\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline_actual = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline_actual = x_steps + tolerance * np.sin(np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# model= build_Model(X_train= results,y_train= targets)\n",
    "# model.load_weights(filepath= weights_path)\n",
    "# model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "#model = Sequential()\n",
    "#model = load_model(filepath=str(model_dir+'model.h5'), compile=False)\n",
    "#pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print(model.summary())\n",
    "print(model.input_shape)\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(results)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#org_targets= post_processer.inverse_transform(targets_post)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "pred.reshape((-1,))\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(targets, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'training_results.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "pred.reshape((-1,))\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(targets, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'training_results.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####Save Model as onnx from weights\n",
    "# from keras.models import load_model\n",
    "# import tf2onnx\n",
    "# import onnx\n",
    "# import tensorflow as tf\n",
    "# model= Sequential()\n",
    "# model= load_model(filepath= str(model_dir+'model.h5'), compile= False)\n",
    "# model.load_weights(weights_path)\n",
    "# model.save(str(model_dir+'best_model.h5'), save_format='h5')\n",
    "# input_signature= [tf.TensorSpec([None,results.shape[1], results.shape[2]],tf.float32, name= 'x')]\n",
    "# onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "# onnx.save_model(onnx_model, str(model_dir+'Best_model.onnx'))\n",
    "# print (\".onnx model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#model = Sequential()\n",
    "#model = load_model(filepath=str(model_dir+'model.h5'), compile=False)\n",
    "#pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "# print(model.summary())\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(test_results)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Ergebnisse des KI-Modells')\n",
    "plt.xlabel('Gemessene Verlagerung (Welle) Mm')\n",
    "plt.ylabel('Vorhersage der Verlagerung (Welle) Mm')\n",
    "plt.plot(x_steps, upperline, label='5 Mm Obere Grenze', color='green')\n",
    "plt.scatter(test_targets, pred, label='Vorhersage', color='blue')\n",
    "# plt.scatter(y_train,pred_train, label= 'predictions_train', color= 'blue')\n",
    "# plt.scatter(y_test,pred_test, label= 'predictions_test', color= 'magenta')\n",
    "plt.plot(x_steps, lowerline, label='5 Mm Untere Grenze', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimale Linie')\n",
    "plt.plot(x_steps, lowerline_actual, label='10 Mm Obere Grenze', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='10 Mm Untere Grenze', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'testing_results.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comparasion between Komp and Z_tcp_ok\n",
    "current_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/combined_file/'\n",
    "file = str(current_dir + 'combined.csv')\n",
    "df = pd.read_csv(file)\n",
    "z_tcp_ok = df[output_variable]\n",
    "komp = -0.1*df['komp']\n",
    "print('mse= ', np.mean(mean_squared_error(z_tcp_ok, komp)),\n",
    "      ' mae= ', np.mean(mean_absolute_error(z_tcp_ok, komp)))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Komp and Z_TCP_OK')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Komp')\n",
    "plt.grid()\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(z_tcp_ok, komp, label='komp', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path= str(source_dir + 'results/')\n",
    "#selected_Columns= ['t_spindel', 't_motor']\n",
    "# Validate the model based on validataion data\n",
    "data_path = str(source_dir + 'validate_Data/')\n",
    "#image_path= str(source_dir + 'results/')\n",
    "#pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print('preprocessor is loaded successfully')\n",
    "#model = Sequential()\n",
    "#model = load_model(str(model_dir+'model.h5'), compile=False)\n",
    "print('Model is loaded successfully')\n",
    "os.chdir(data_path)\n",
    "errors = []\n",
    "error_df = pd.DataFrame()\n",
    "scatter_mode= 'lines'\n",
    "visualisation_selected_Columns= ['t_bett', 't_motor', 't_spindel', 'M8','M121', 'M127', 'M7',output_variable, 'prediction_Error' ]\n",
    "for file in glob.glob('*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    #df = remove_peeks(df, output_variable, 4)\n",
    "    print('current File: ', file)\n",
    "    #df['t_spindel']= df['t_spindle']\n",
    "    #signals = preprocess_data(df, kuelung=['M8', 'M121', 'M127', 'M7'])\n",
    "    \n",
    "    signals = df[selected_Columns]\n",
    "    \n",
    "    signals = signals.to_numpy()\n",
    "    signals = pipeline.transform(signals)\n",
    "    # (1000 * df[str('smoothed_' + output_variable)]).round(2)#\n",
    "    output = (1000* df[output_variable]).round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)\n",
    "    partitions, target = generateDataSource(\n",
    "        signal_input=signals, input_columns=selected_Columns, output_length=1, signal_output=output, window=window, shift=shift, sample_rate=sampling_rate)\n",
    "    target = target.reshape((-1,))\n",
    "    predictions = model.predict(partitions)\n",
    "    predictions = np.reshape(predictions, newshape=target.shape)\n",
    "    diff = np.abs(target - predictions)\n",
    "    mae_v = ' mae= ' + str(np.mean(mean_absolute_error(target, predictions).round(2))) +' Micro_Meter '  \n",
    "    max_v = ' max= ' + str(np.max(np.abs(target - predictions).round(2))) +' Micro_Meter '  \n",
    "\n",
    "    print(mae_v)\n",
    "    print(max_v)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    plt.xlabel('Zeit')\n",
    "    plt.ylabel('Verlagerung (Welle) Mm')\n",
    "    plt.title(str(file + mae_v + max_v))\n",
    "    plt.plot(target, label='gemessene Verlagerung (Welle)')\n",
    "    plt.plot(predictions, label= 'prediction')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Visualization of lf-data with predicted results\n",
    "visualisation_selected_Columns = selected_Columns.copy()\n",
    "visualisation_selected_Columns.append(output_variable)\n",
    "visualisation_df = df[window-1:]\n",
    "visualisation_df['predictions'] = predictions\n",
    "fig = make_subplots(rows= 1, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "###verlagerung\n",
    "fig.add_trace(go.Line(y= 1000 * visualisation_df[output_variable], name='Echte z_welle'), row= 1, col=1)\n",
    "fig.add_trace(go.Line(y= visualisation_df['predictions'], name='KI_z_welle'), row= 1, col=1)\n",
    "fig.update_yaxes(title_text='Verlagerung Mic-Meter', row= 1, col=1)\n",
    "fig.update_xaxes(title_text='Zeit (1= 2 Minuten)', row=1, col=1)\n",
    "fig.update_layout(height=900, width=1200, title_text=str(file + mae_v + max_v))\n",
    "fig.show()\n",
    "    #pio.write_image(fig, str(image_path + file+'_Verlagerung.png'), format='png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply predictions on the experiments of validation and save results as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path= str(source_dir + 'results/')\n",
    "#selected_Columns= ['t_spindel', 't_motor']\n",
    "# Validate the model based on validataion data\n",
    "data_path = str(source_dir + 'validate_Data/')\n",
    "#image_path= str(source_dir + 'results/')\n",
    "#pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print('preprocessor is loaded successfully')\n",
    "#model = Sequential()\n",
    "#model = load_model(str(model_dir+'model.h5'), compile=False)\n",
    "#post_processer= load(str(preprocessor_dir+'postprocesser.p'))\n",
    "print('Model is loaded successfully')\n",
    "os.chdir(data_path)\n",
    "errors = []\n",
    "error_df = pd.DataFrame()\n",
    "scatter_mode= 'lines'\n",
    "visualisation_selected_Columns= ['t_bett', 't_motor', 't_spindle', 'M8','M121', 'M127', 'M7',output_variable, 'prediction_Error' ]\n",
    "for file in glob.glob('*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    #df = remove_peeks(df, output_variable, 4)\n",
    "    print('current File: ', file)\n",
    "    df= smoothing_data(df, window= 24)\n",
    "    signals = df[selected_Columns]\n",
    "    signals = signals.to_numpy()\n",
    "    signals = pipeline.transform(signals)\n",
    "    # (1000 * df[str('smoothed_' + output_variable)]).round(2)#\n",
    "    output = (1000* df[output_variable]).round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)\n",
    "    partitions, target = generateDataSource(\n",
    "        signal_input=signals, input_columns=selected_Columns, output_length=1, signal_output=output, window=window, shift=shift, sample_rate=sampling_rate)\n",
    "    target = target.reshape((-1,1))\n",
    "    predictions = model.predict(partitions)\n",
    "    predictions = np.reshape(predictions, newshape=target.shape)\n",
    "    #predictions= post_processer.inverse_transform(predictions)\n",
    "    #target= post_processer.inverse_transform(target)\n",
    "    diff = np.abs(target - predictions)\n",
    "    mae_v = ' mae= ' + str(np.mean(mean_absolute_error(target, predictions))) +' Micro_Meter '  \n",
    "    max_v = ' max= ' + str(np.max(np.abs(target - predictions))) +' Micro_Meter '  \n",
    "    print(mae_v)\n",
    "    print(max_v)\n",
    "    ######################\n",
    "    # Visualization of lf-data with predicted results\n",
    "    visualisation_selected_Columns = selected_Columns.copy()\n",
    "    visualisation_selected_Columns.append(output_variable)\n",
    "    print(abs(len(df)-len(partitions))\n",
    "    visualisation_df = df[abs(len(df)-len(partitions)):]\n",
    "    visualisation_df[str('smoothed_' + output_variable)] = output\n",
    "    visualisation_df['predictions'] = predictions\n",
    "    fig = make_subplots(rows= 9, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    # ##t_bett\n",
    "    # fig.add_trace(go.Line(y=visualisation_df['t_bett'], name='t_bett'), row= 1, col=1)\n",
    "    # fig.update_yaxes(title_text='t_bett C°', row= 1, col=1)\n",
    "    # ##t_bett\n",
    "    # fig.add_trace(go.Line(y=visualisation_df['drhz'], name='drhz'), row= 1, col=1)\n",
    "    # fig.update_yaxes(title_text='drhz RPM', row= 1, col=1)\n",
    "    ###t_motor\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_motor'], name='t_motor'), row= 2, col=1)\n",
    "    fig.update_yaxes(title_text='t_motor C°', row= 2, col=1)\n",
    "    ###t_spindel\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_spindle'], name='t_spindle'), row= 3, col=1)\n",
    "    fig.update_yaxes(title_text='t_spindel C°', row= 3, col=1)\n",
    "    ###M8\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M8'], name='M8'), row= 4, col=1)\n",
    "    fig.update_yaxes(title_text='M8', row= 4, col=1)\n",
    "    ###M121\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M121'], name='M121'), row= 5, col=1)\n",
    "    fig.update_yaxes(title_text='M121', row= 5, col=1)\n",
    "    ###M127\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M127'], name='M127'), row= 6, col=1)\n",
    "    fig.update_yaxes(title_text='M127', row= 6, col=1)\n",
    "    ###M7\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M7'], name='M7'), row= 7, col=1)\n",
    "    fig.update_yaxes(title_text='M7', row= 7, col=1)\n",
    "    ###verlagerung\n",
    "    fig.add_trace(go.Line(y=  1000* visualisation_df[output_variable], name='z_welle_Soll'), row= 8, col=1)\n",
    "    fig.add_trace(go.Line(y=  visualisation_df['predictions'], name='KI_z_welle_Ist'), row= 8, col=1)\n",
    "   ##fig.add_trace(go.Line(y=  1000* df[output_variable], name='z_welle_Soll_smoothed'), row= 8, col=1)\n",
    "    fig.update_yaxes(title_text='Verlagerung Mic-Meter', row= 8, col=1)\n",
    "    ###Fehler\n",
    "    experiment_errors = (1000* visualisation_df[output_variable] - visualisation_df['predictions']).to_list()\n",
    "    fig.add_trace(go.Scatter( y=   experiment_errors, name='Prediction Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    ## Draw the tolerence +-5\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,5), name='+5 Max Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,-5), name='-5 Min Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.update_yaxes(title_text='Fehler Mic-Meter', row= 9, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit (1= 2 Minuten)', row=9, col=1)\n",
    "    fig.update_layout(height=900, width=1200, title_text=str(file + mae_v + max_v))\n",
    "    fig.show()\n",
    "    pio.write_image(fig, str(image_path + file+'.png'), format='png')\n",
    "    current_error_df = pd.DataFrame(\n",
    "        {'verlagerung': visualisation_df['predictions'].to_list(), 'error': experiment_errors})\n",
    "    if error_df is None:\n",
    "        error_df = current_error_df\n",
    "    else:\n",
    "        error_df = pd.concat([error_df, current_error_df])\n",
    "    errors.extend(experiment_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of resulted Data set\n",
    "import plotly.express as px\n",
    "labels = error_df['verlagerung']\n",
    "verlagerung_df = pd.DataFrame({'verlagerung': labels})\n",
    "print(verlagerung_df.head(5))\n",
    "fig = px.histogram(verlagerung_df, x='verlagerung', nbins=100)\n",
    "mean = verlagerung_df['verlagerung'].mean()\n",
    "std = verlagerung_df['verlagerung'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(\n",
    "    fig, str(image_path + 'distribution of displacement.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of resulted Data set\n",
    "import plotly.express as px\n",
    "error_df['abs_error'] = error_df['error'].abs()\n",
    "print(error_df.head(5))\n",
    "fig = px.histogram(error_df, x='verlagerung',\n",
    "                   y='abs_error', nbins=100, histfunc='avg')\n",
    "mean = error_df['abs_error'].mean()\n",
    "std = error_df['abs_error'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(fig, str(\n",
    "    image_path + 'distribution of average prediction error on the displacement.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "error_df = pd.DataFrame({'errors': errors})\n",
    "print(error_df.head(5))\n",
    "fig = px.histogram(error_df, x='errors', nbins=50)\n",
    "mean = error_df['errors'].mean()\n",
    "std = error_df['errors'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(fig, str(\n",
    "    image_path + 'Distribution of the prediction error.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from joblib import load\n",
    "# pipeline= load(str(preprocessor_dir+'preprocessor.p'))\n",
    "# print(pipeline)\n",
    "# initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
    "# onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "# with open(str(preprocessor_dir + \"prepeocessor.onnx\"), \"wb\") as f:\n",
    "#     f.write(onx.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# preprocessor= load(str(preprocessor_dir+\"preprocessor.p\"))\n",
    "# dump(preprocessor,str(preprocessor_dir+\"preprocessor.p\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a58a25343f99ae6e283c189afd7abc602dec6c63d356cacabf499812ae322086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
