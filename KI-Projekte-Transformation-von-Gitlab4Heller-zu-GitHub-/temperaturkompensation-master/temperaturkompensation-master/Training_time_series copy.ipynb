{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing for time series without the output as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import silu, swish, linear, tanh, leaky_relu, sigmoid, relu, elu, selu, gelu, linear\n",
    "from keras.layers import Input, LSTM, Dense, GaussianNoise, BatchNormalization, Activation, PReLU, SimpleRNN\n",
    "from keras.initializers.initializers_v2 import RandomNormal, RandomUniform\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "from keras.optimizers import nadam_v2, rmsprop_v2, adamax_v2\n",
    "from keras.losses import mean_absolute_error as mae, mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from keras.models import Sequential\n",
    "import onnx\n",
    "import tf2onnx\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, KernelPCA,IncrementalPCA,SparsePCA\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'v3.9.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for expreiments .csvs\n",
    "source_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/'\n",
    "testdata_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/Testing_data/'\n",
    "preprocessor_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "model_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "log_dir = \"C:/Temperaturkompensation/logs/\" + \\\n",
    "    datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "weights_path = str(model_dir+\"/weights3/weights\")\n",
    "image_path = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import LossFunctionWrapper, mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras.backend as k\n",
    "\n",
    "\n",
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.multiply(tf.add(tf.multiply(tf.reduce_max(\n",
    "        tf.abs((y_true - y_pred))), 1), mean_absolute_error(y_true, y_pred)), 1)\n",
    "    return error\n",
    "\n",
    "\n",
    "def max_loss_v1(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.multiply(tf.reduce_max(tf.abs((y_true - y_pred))), 1), tf.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return error\n",
    "\n",
    "def max_loss_v2(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.multiply(tf.pow(tf.reduce_max(tf.abs((y_true - y_pred))),2), 1), mean_squared_error(y_true, y_pred))\n",
    "    return error\n",
    "\n",
    "def max_loss_v3_mae_std(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mean_ae= mean_absolute_error(y_true, y_pred)\n",
    "    vars= tf.pow(tf.subtract(tf.abs(y_true - y_pred),mean_ae),2)\n",
    "    errors_std= tf.sqrt(tf.reduce_mean(vars))\n",
    "    #error = tf.add(mean_squared_error(y_true, y_pred), errors_std)\n",
    "    error = tf.add(mean_ae, errors_std)\n",
    "    return error\n",
    "\n",
    "def loss_rmse_std(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mean_ae= mean_absolute_error(y_true, y_pred)\n",
    "    vars= tf.pow(tf.subtract(tf.abs(y_true - y_pred),mean_ae),2)\n",
    "    errors_std= tf.reduce_mean(vars)\n",
    "    error = tf.add(mean_squared_error(y_true, y_pred), errors_std)\n",
    "    #error = tf.add(mean_ae, errors_std)\n",
    "    return error\n",
    "\n",
    "def calc_fidelity(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    # session= k.get_session()\n",
    "    # y_true_array= session.run(y_true)\n",
    "    # y_pred_array= session.run(y_pred)\n",
    "    # cc= np.corrcoef(y_true_array,y_pred_array)\n",
    "    NRMSE= tf.divide(tf.sqrt(mean_squared_error(y_true, y_pred)),tf.subtract(tf.reduce_max(y_true),tf.reduce_min(y_true)))\n",
    "    fidelity= tf.abs(tf.multiply(100.0, tf.subtract(1.0,NRMSE)))\n",
    "    return fidelity\n",
    "    \n",
    "special_loss = LossFunctionWrapper(loss_rmse_std)\n",
    "fidelity_wrapper= LossFunctionWrapper(calc_fidelity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource_deprecated(signal_input=None, input_columns: list = [], output_length: int = 1, signal_output=None, window=1, shift=1):\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    num_samples = int((Signal_Length - window + 1) / shift)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i] = signal_input[i * shift: i * shift + window]\n",
    "        y[i] = signal_output[i * shift + window -\n",
    "                             1: i * shift + window - 1 + output_length]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement special preprocessing steps\n",
    "# 1 calculate new Feature\n",
    "# 2 make rounding\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def special_preprocess(data: np.ndarray):\n",
    "    delta_t_spindel_bett = data[:, 1] - data[:, 0]\n",
    "    delta_t_spindel_bett = delta_t_spindel_bett.reshape((-1, 1))\n",
    "    result = np.concatenate((data[:, 2:5], delta_t_spindel_bett), axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def rounding(signals: np.ndarray):\n",
    "    return np.round(signals, 2)\n",
    "\n",
    "\n",
    "def remove_t_bett(data: np.ndarray):\n",
    "    result = data[:, 1:]\n",
    "    result[:, 0] = result[:, 0] - data[:, 0]\n",
    "    result[:, 1] = result[:, 1] - data[:, 0]\n",
    "    return result\n",
    "\n",
    "\n",
    "data = np.asarray([[24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0],\n",
    "                   [24, 90, 30, 500, 0, 0, 0, 0]])\n",
    "data.shape\n",
    "print(remove_t_bett(data))\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_Columns= ['diff', 'drhz','t_bett',]#,'t_kluelung']# , 'diff'\n",
    "# [ 't_bett','t_spindel', 't_y_achse', 't_z_achse', 'drhz', 't_motor','t_kss_tank', 't_raum', 't_spindel_vor', 't_spindel_rueck','t_schwenkantrieb']\n",
    "\n",
    "output_variable = 'welle_z_iterpolated'#'welle_z_upsampled'#'z_welle_ok'  #'z_tcp_ok'# 'welle_z'#''#'smoothed_z_welle_ok'#\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "pca = PCA(n_components=6)\n",
    "#pca= IncrementalPCA(n_components=6)\n",
    "# Data Parameters\n",
    "selected_Columns = ['t_bett', 't_motor', 't_spindle', 'M8','M121', 'M127', 'M7']#['t_bett', 't_motor', 't_spindel', 'M8','M121', 'M127', 'M7']\n",
    "#,('smoothing_data', FunctionTransformer(smoothing_data))('t_bett_removal', FunctionTransformer(remove_t_bett)),\n",
    "proposed_pipline = Pipeline(steps=[ ('t_bett_removal', FunctionTransformer(remove_t_bett)),('rounding', FunctionTransformer(rounding)), ('stdscaler', scaler)])#('stdscaler', scaler), ('pca', pca)])  # Pipeline(steps=[('rounding', FunctionTransformer(rounding))])#('pca', pca)\n",
    "preprocessor_name = \"preprocessor.p\"\n",
    "postprocessor= 'postprocessor.p'\n",
    "smoothed_output= False\n",
    "window= 30\n",
    "shift= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Model_LSTM(X_train):\n",
    "    activation_Function = selu\n",
    "    dropout = 0.01\n",
    "    l1_v = 0.0008\n",
    "    L2_v = 0.0008\n",
    "    size = 50\n",
    "    n_hidden_layers =4\n",
    "    diff= 10\n",
    "    unroll = False\n",
    "    kernal_init = 'he_normal'#RandomUniform()  # \n",
    "    #optimizer = nadam_v2.Nadam(clipnorm= 1)\n",
    "    optimizer= adamax_v2.Adamax(clipnorm= 1)\n",
    "    model = Sequential()\n",
    "    #activation = PReLU()\n",
    "    model.add(LSTM(size, input_shape=(window, len(selected_Columns)-1), return_sequences=True,kernel_initializer= kernal_init,unroll=unroll, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(activation=activation_Function))\n",
    "    #model.add(PReLU())\n",
    "    for i in range(1, n_hidden_layers+1):\n",
    "        if i== n_hidden_layers:\n",
    "            model.add(LSTM(size - i * diff, return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=L2_v)))\n",
    "        else:\n",
    "            model.add(LSTM(size - i * diff, return_sequences=True,unroll=unroll, dropout=dropout,kernel_initializer= kernal_init,  kernel_regularizer=L1L2(l1=l1_v, l2=L2_v)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation_Function))\n",
    "        #model.add(PReLU())\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss=special_loss, metrics=[max_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(input: pd.DataFrame, window= 12):\n",
    "    # data= input.copy()\n",
    "    # data['t_bett']= data['t_bett'].rolling(window= window).mean()\n",
    "    # data['t_motor']= data['t_motor'].rolling(window= window).mean()\n",
    "    # data['t_spindle']= data['t_spindle'].rolling(window= window).mean()\n",
    "    # #data[output_variable]= data[output_variable].rolling(window= window).mean()\n",
    "    # data= data[ window+1:]###Remove nan\n",
    "    return input#data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Data source from all signals and create Training dataset\n",
    "# working_dir = source_dir\n",
    "# os.chdir(working_dir)\n",
    "# results: np.ndarray = None\n",
    "# targets: np.ndarray = None\n",
    "# pipeline = None\n",
    "# reset = True\n",
    "# training_data = None\n",
    "# #selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "# for file in glob.glob('*.csv'):\n",
    "#     if file == 'combined.csv':\n",
    "#         continue\n",
    "#     print('Current File: ', file)\n",
    "#     df = pd.read_csv(file)\n",
    "#     if df.isnull().values.any():\n",
    "#         print ('with nan')\n",
    "#     signals = df\n",
    "#     prev_time=None\n",
    "#     drop_list= []\n",
    "#     print('Before Filtering, ', len(signals))\n",
    "#     for index,row in signals.iterrows():\n",
    "#         current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "#         if prev_time is None:\n",
    "#             prev_time= current_time\n",
    "#         elif (current_time - prev_time).total_seconds() < 5:\n",
    "#             drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "#             df= df[df['date'] != row['date']]\n",
    "#         prev_time= current_time\n",
    "#     signals= df\n",
    "#     signals= smoothing_data(signals)\n",
    "#     print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))\n",
    "#     signals = signals[selected_Columns]\n",
    "#     if signals.isnull().values.any():\n",
    "#         print ('with nan')\n",
    "#         exit()\n",
    "#     #signals = preprocess_data(signals, kuelung=['M8', 'M121', 'M127', 'M7'])\n",
    "#     signals= signals.to_numpy()\n",
    "#     if training_data is None:\n",
    "#         training_data = signals\n",
    "#     else:\n",
    "#         training_data = np.concatenate((training_data, signals))\n",
    "# #training_data = training_data.to_numpy()\n",
    "# if pipeline is None:\n",
    "#     try:\n",
    "#         if not reset:\n",
    "#             pipeline = load(str(preprocessor_dir + preprocessor_name))\n",
    "#             print('preprocessor is loaded successfully')\n",
    "#         else:\n",
    "#             pipeline = proposed_pipline\n",
    "#             pipeline = pipeline.fit(training_data)\n",
    "#             dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "#             print('preprocessor is saved successfully')\n",
    "#             print('explained_variance_ratio_= ', pipeline.steps[3][1].explained_variance_ratio_)\n",
    "#     except FileNotFoundError as err:\n",
    "#         pipeline = proposed_pipline\n",
    "#         pipeline = pipeline.fit(training_data)\n",
    "#         dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "#         initial_type = [('float_input', FloatTensorType(\n",
    "#             [None,  training_data.shape[1]]))]\n",
    "#         onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "#         with open(str(preprocessor_dir + \"preprocessor.onnx\"), \"wb\") as f:\n",
    "#             f.write(onx.SerializeToString())\n",
    "#         print('preprocessor is saved successfully')\n",
    "# columns = selected_Columns.copy()\n",
    "# columns.append(output_variable)\n",
    "# for file in glob.glob('*.csv'):\n",
    "#     if file == 'combined.csv':\n",
    "#         continue\n",
    "#     print('Current File: ', file)\n",
    "#     df = pd.read_csv(file)\n",
    "#     signals = df\n",
    "#     prev_time=None\n",
    "#     drop_list= []\n",
    "#     print('Before Filtering, ', len(signals))\n",
    "#     for index,row in signals.iterrows():\n",
    "#         current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "#         if prev_time is None:\n",
    "#             prev_time= current_time\n",
    "#         elif (current_time - prev_time).total_seconds() < 5:\n",
    "#             drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "#             df= df[df['date'] != row['date']]\n",
    "#         prev_time= current_time\n",
    "#     signals= df\n",
    "#     signals= smoothing_data(signals)\n",
    "#     print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))\n",
    "#     #signals = preprocess_data(signals, kuelung=['M8', 'M121', 'M127', 'M7'])\n",
    "#     signals = signals[selected_Columns]\n",
    "#     signals = signals.to_numpy()\n",
    "#     print(signals.shape)\n",
    "#     signals = pipeline.transform(signals)\n",
    "#     output = 10000* df[output_variable]#.round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)#1000* df[output_variable].round(3)#\n",
    "#     partitions, target = generateDataSource(\n",
    "#         signal_input=signals, input_columns=selected_Columns, output_length=1, signal_output=output, window=window, shift=shift)\n",
    "#     if results is None:\n",
    "#         results = partitions\n",
    "#         targets = target\n",
    "#     else:\n",
    "#         results = np.concatenate((results, partitions), axis=0)\n",
    "#         targets = np.concatenate((targets, target), axis=0)\n",
    "#     # ####Replace negative values with 0\n",
    "#     # targets[targets < 0] = 0\n",
    "#     print('input: ', results.shape, ' output:', targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Data source from all signals and create Training dataset\n",
    "# data_path = str(source_dir + 'Test_area/')\n",
    "# os.chdir(data_path)\n",
    "working_dir = source_dir\n",
    "os.chdir(working_dir)\n",
    "results: np.ndarray = None\n",
    "targets: np.ndarray = None\n",
    "pipeline = None\n",
    "reset = True\n",
    "all_data = None\n",
    "#selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    signals = df\n",
    "    prev_time=None\n",
    "    drop_list= []\n",
    "    print('Before Filtering, ', len(signals))\n",
    "    for index,row in signals.iterrows():\n",
    "        current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        if prev_time is not None and (current_time  - prev_time).total_seconds() < 5:\n",
    "            drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            df= df[df['date'] != row['date']]\n",
    "            continue\n",
    "        else:\n",
    "            prev_time= current_time\n",
    "    signals= df\n",
    "    signals= smoothing_data(signals)\n",
    "    print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))\n",
    "    if signals.isnull().values.any():\n",
    "        print ('with nan')\n",
    "        exit()\n",
    "    ###Set index to data frame\n",
    "    signals['date']= pd.to_datetime(signals['date'],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    signals.set_index('date',inplace= True)\n",
    "    #signals= signals[selected_Columns]\n",
    "    ####Shifting label once forward#######\n",
    "    signals[output_variable]= signals[output_variable].shift(+1)\n",
    "    #print(all_data.iloc[2])\n",
    "    signals= signals[1:]\n",
    "    if all_data is None:\n",
    "        all_data = signals\n",
    "    else:\n",
    "        all_data = pd.concat([all_data, signals])\n",
    "targets= 1000 * all_data[output_variable]\n",
    "all_data= all_data[selected_Columns]\n",
    "#print(all_data.columns)\n",
    "all_data= np.float32(all_data.to_numpy())\n",
    "targets=    np.float32(targets.to_numpy().reshape((-1,1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline is None:\n",
    "    try:\n",
    "        if not reset:\n",
    "            pipeline = load(str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is loaded successfully')\n",
    "        else:\n",
    "            pipeline = proposed_pipline\n",
    "            pipeline = pipeline.fit(all_data)\n",
    "            dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is saved successfully')\n",
    "            #print('explained_variance_ratio_= ', pipeline.steps[3][1].explained_variance_ratio_)\n",
    "    except FileNotFoundError as err:\n",
    "        pipeline = proposed_pipline\n",
    "        pipeline = pipeline.fit(all_data)\n",
    "        dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "        # initial_type = [('float_input', FloatTensorType(\n",
    "        #     [None,  all_data.shape[1]]))]\n",
    "        # onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "        # with open(str(preprocessor_dir + \"preprocessor.onnx\"), \"wb\") as f:\n",
    "        #     f.write(onx.SerializeToString())\n",
    "        print('preprocessor is saved successfully')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor= StandardScaler(with_mean=True, with_std=True)#MinMaxScaler(feature_range=(-1, 1))#Pipeline(steps=[('minmax', MinMaxScaler((-1,1)))])\n",
    "post_processor= post_processor.fit(targets)#(all_data[: -1])\n",
    "dump(post_processor, str(preprocessor_dir + 'postprocessor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data= pipeline.transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_transformed= post_processor.transform(targets)#preprocessed_data[:,-1]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train, y_test=train_test_split(preprocessed_data, targets_transformed, test_size=0.2, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train= preprocessed_data\n",
    "# y_train= preprocessed_data[:,-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_windowed= TimeseriesGenerator(x_train,targets= y_train, length=window, sampling_rate= 1, batch_size= 1)\n",
    "xtrain= np.asarray([x_train_windowed[i][0] for i in range(0, len(x_train_windowed))]).reshape((-1, window, len(selected_Columns)-1))\n",
    "ytrain= np.asarray([x_train_windowed[i][1] for i in range(0, len(x_train_windowed))])\n",
    "x_test_windowed= TimeseriesGenerator(x_test,targets= y_test, length=window, sampling_rate= 1, batch_size= 1)\n",
    "xtest= np.asarray([x_test_windowed[i][0] for i in range(0, len(x_test_windowed))]).reshape((-1, window, len(selected_Columns)-1))\n",
    "ytest= np.asarray([x_test_windowed[i][1] for i in range(0, len(x_test_windowed))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    print('i= ',i)\n",
    "    print (xtrain[i],' label= ',ytrain[i] )\n",
    "    print('#########################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "    y_train_rounded = np.round(y_train)\n",
    "    N = len(y_train_rounded)\n",
    "    y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "    weights = np.ones(shape=(N,))\n",
    "    for i in range(0, N):\n",
    "        current_value = y_train_rounded[i]\n",
    "        weights[i]= (1 if current_value <=25 else 3.0 * current_value)\n",
    "    fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=weights,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 1, col= 1)\n",
    "    fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 2, col= 1)\n",
    "    # fig.update_yaxes(title_text= 'weight', row= 1, col= 1)\n",
    "    # fig.update_xaxes(title_text= 'label value', row= 1, col= 1)\n",
    "    fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "    fig.show()\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Block\n",
    "checkpoint = ModelCheckpoint(filepath=str(model_dir+'best_model.h5'), monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=200, verbose=1)\n",
    "model = build_Model_LSTM(X_train=xtrain)\n",
    "#reset= False\n",
    "try:\n",
    "    if not reset:\n",
    "        model.load_weights(filepath=weights_path)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print(model.output_shape)\n",
    "print(model.summary())\n",
    "summary = model.fit(x= xtrain,y= ytrain, shuffle=False,  epochs=5000,batch_size=100 , validation_data= (xtest, ytest),  #validation_split=0.2, #\n",
    "                    callbacks=[earlyStopping, checkpoint], verbose=2, workers=32, use_multiprocessing=True)#, sample_weight=weights)\n",
    "model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "input_signature = [tf.TensorSpec(\n",
    "    [None, xtrain.shape[1], xtrain.shape[2]], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model=model, input_signature=input_signature, opset=10)\n",
    "onnx.save_model(onnx_model, str(model_dir+'model.onnx'))\n",
    "print(\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(weights_path, save_format='h5')\n",
    "# model.save_weights(weights_path, save_format='tf')\n",
    "# model.save(str(model_dir+'testmodel.h5'), save_format='h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(filepath=weights_path)\n",
    "loss = summary.history['loss']\n",
    "val_loss = summary.history['val_loss']  \n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "                    print_grid=True,  vertical_spacing=0.02)\n",
    "fig.add_trace(go.Line(y=loss, name='Training Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=val_loss, name='Validation Loss'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Max_Loss', row=1, col=1)\n",
    "fig.update_layout(height=900, width=900, title_text='training_curve')\n",
    "fig.show()\n",
    "pio.write_image(fig, str(image_path+'training_curve.png'), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####Save Model as onnx from weights\n",
    "# from keras.models import load_model\n",
    "# import tf2onnx\n",
    "# import onnx\n",
    "# import tensorflow as tf\n",
    "# model= Sequential()\n",
    "# model= load_model(filepath= str(model_dir+'best_model.h5'), compile= False)\n",
    "# #model.load_weights(weights_path)\n",
    "# #model.save(str(model_dir+'best_model.h5'), save_format='h5')\n",
    "# input_signature= [tf.TensorSpec([None,240, 6],tf.float32, name= 'x')]\n",
    "# onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "# onnx.save_model(onnx_model, str(model_dir+'Best_model.onnx'))\n",
    "# print (\".onnx model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(input: np.ndarray, output: np.ndarray, scaler: MinMaxScaler):\n",
    "    temp_array= np.zeros_like(input)\n",
    "    output_shape= input[:,-1].shape\n",
    "    temp_array[:,-1]= np.reshape(output, newshape= output_shape)\n",
    "    reversed_input= scaler.inverse_transform(temp_array)\n",
    "    post_processed_output=  reversed_input[:, -1]\n",
    "    return post_processed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tolerance = 5\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline = x_steps + tolerance * np.sin(np.pi/4)\n",
    "# Create Tolerance Lines\n",
    "tolerance = 10\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline_actual = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline_actual = x_steps + tolerance * np.sin(np.pi/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# model= build_Model(X_train= results,y_train= targets)\n",
    "# model.load_weights(filepath= weights_path)\n",
    "# model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "model = Sequential()\n",
    "model = load_model(filepath=str(model_dir+'model.h5'), compile=False)\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print(model.summary())\n",
    "print(model.input_shape)\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(xtrain)\n",
    "pred_postprocessed =  post_processor.inverse_transform(pred)\n",
    "true_postprocessed = post_processor.inverse_transform(y_train[window :].reshape(pred.shape))\n",
    "# pred_postprocessed = post_process(for_post_process_input, pred, minmax)\n",
    "# true_postprocessed = post_process(for_post_process_input, y_train[window :].reshape(pred.shape), minmax)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(true_postprocessed, pred_postprocessed, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'training_results.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####Save Model as onnx from weights\n",
    "# from keras.models import load_model\n",
    "# import tf2onnx\n",
    "# import onnx\n",
    "# import tensorflow as tf\n",
    "# model= Sequential()\n",
    "# model= load_model(filepath= str(model_dir+'model.h5'), compile= False)\n",
    "# model.load_weights(weights_path)\n",
    "# model.save(str(model_dir+'best_model.h5'), save_format='h5')\n",
    "# input_signature= [tf.TensorSpec([None,results.shape[1], results.shape[2]],tf.float32, name= 'x')]\n",
    "# onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "# onnx.save_model(onnx_model, str(model_dir+'Best_model.onnx'))\n",
    "# print (\".onnx model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "model = Sequential()\n",
    "model = load_model(filepath=str(model_dir+'model.h5'), compile=False)\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "# print(model.summary())\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(xtest)\n",
    "pred_postprocessed =  post_processor.inverse_transform(pred)\n",
    "true_postprocessed = post_processor.inverse_transform(y_test[window :].reshape(pred.shape))\n",
    "# for_post_process_input= xtest[:, window-1,:]#.reshape((-1,len(selected_Columns)-1))\n",
    "# pred_postprocessed = post_process(for_post_process_input, pred, minmax)\n",
    "# true_postprocessed = post_process(for_post_process_input, y_test[window :].reshape(pred.shape), minmax)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Ergebnisse des KI-Modells')\n",
    "plt.xlabel('Gemessene Verlagerung (Welle) Mm')\n",
    "plt.ylabel('Vorhersage der Verlagerung (Welle) Mm')\n",
    "plt.plot(x_steps, upperline, label='5 Mm Obere Grenze', color='green')\n",
    "plt.scatter(true_postprocessed, pred_postprocessed, label='Vorhersage', color='blue')\n",
    "# plt.scatter(y_train,pred_train, label= 'predictions_train', color= 'blue')\n",
    "# plt.scatter(y_test,pred_test, label= 'predictions_test', color= 'magenta')\n",
    "plt.plot(x_steps, lowerline, label='5 Mm Untere Grenze', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimale Linie')\n",
    "plt.plot(x_steps, lowerline_actual, label='10 Mm Obere Grenze', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='10 Mm Untere Grenze', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'testing_results.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Predictions on experiments sqentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_relevance=5\n",
    "recording_time= 5\n",
    "response_time= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_relevance_array(input_array: np.ndarray, output_shape):\n",
    "    input_time= input_array.shape[1] # in case of 5 Seconds it will be 240 seconds\n",
    "    output_time= output_shape[1] # in case of 2 Minutes it will be 10\n",
    "    step= int(input_time/output_time)\n",
    "    resut_array= np.zeros(output_shape)\n",
    "    for i in range( 0, output_time):\n",
    "        #print('place in result: ', output_time - i - 1, ' place in original: ', input_time - i*step -1)\n",
    "        resut_array[0, output_time - i - 1, :]= input_array[0, input_time - i*step -1,:]\n",
    "    return resut_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Simulate the prediction by passing values with 5 seconds ########\n",
    "inputsignal_list= ['t_bett', 't_motor', 't_spindle', 'M8','M121', 'M127', 'M7']\n",
    "custom_date_parser = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "num_features= len(inputsignal_list)\n",
    "data_path = str(source_dir + 'Test_area/')\n",
    "os.chdir(data_path)\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file, parse_dates= True, date_parser= custom_date_parser)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    input_data= df.copy()\n",
    "    preds= []\n",
    "    shape=(1,window,num_features +1)\n",
    "    output_sec_shape= (1, int(window*np.ceil((time_relevance/recording_time))), num_features + 1)\n",
    "    first_run= True\n",
    "    last_prediction_time= None\n",
    "    outputs= 0\n",
    "    prev_prediction=0\n",
    "    prev_record_time= None\n",
    "    j= -10\n",
    "    for index, data_point in input_data.iterrows():\n",
    "        current_time= datetime.strptime(data_point['date'] , \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        if prev_record_time is not None and (current_time  - prev_record_time).total_seconds() < recording_time:\n",
    "            #print('Escaped! current_time: ',current_time, ' prev_record_time: ',prev_record_time )\n",
    "            #preds.append(out)\n",
    "            df= df[df['date']!= data_point['date']]\n",
    "            continue\n",
    "        else:\n",
    "            prev_record_time= current_time\n",
    "        #print ('index: ', index, ', Date: ', data_point['date'])\n",
    "        new_signal_list = [float(data_point[sig]) for sig in inputsignal_list]\n",
    "        new_signal_list.append(prev_prediction)\n",
    "        #print ('New_signal_list: ', new_signal_list, ' timestamp: ', data_point['date'])\n",
    "        signals_ = pipeline.transform(np.asarray([new_signal_list]))#np.asarray([new_signal_list])#\n",
    "        print ('signals_: ', signals_, ' timestamp: ', data_point['date'])\n",
    "        if first_run:\n",
    "            # AuffÃ¼llen der Struktur - Bei erstem Start ist nur ein Datensatz vorhanden,\n",
    "            # dieser wird mit np.full in alle kopiert\n",
    "            five_sec_array= np.float32(np.full(output_sec_shape, signals_[0]))\n",
    "            array= five_sec_array#extract_time_relevance_array(five_sec_array,shape)\n",
    "            outputs =model.predict(array)\n",
    "            prev_prediction= outputs[0][0]#1000 *data_point[output_variable]#\n",
    "            out = post_processor.inverse_transform(outputs)[0][0] / 1000\n",
    "            print('prev_prediction : ', prev_prediction, 'out: ', out)\n",
    "            preds.append(out)\n",
    "            first_run = False\n",
    "            last_prediction_time= current_time\n",
    "            #print('prediction_time: ', data_point['date'])\n",
    "            #continue\n",
    "        elif (current_time - last_prediction_time).total_seconds()< response_time:\n",
    "                print('No prediction. Just shift')\n",
    "                five_sec_array [0][:-1] = five_sec_array[0][1:]\n",
    "                five_sec_array[0, -1, :] = signals_[0]\n",
    "                preds.append(out)\n",
    "                #continue \n",
    "        else:\n",
    "            #print(\"....Shifting the 5 seconds buffer and insert the new 5s-value\")\n",
    "            five_sec_array [0][:-1] = five_sec_array[0][1:]\n",
    "            five_sec_array[0, -1, :] = signals_[0]\n",
    "            array= five_sec_array#extract_time_relevance_array(five_sec_array, shape)\n",
    "            #array = np.float32(array)\n",
    "            ## Apply predicitions on 2 minutes buffer\n",
    "            outputs =model.predict(array)\n",
    "            prev_prediction= outputs[0][0]#1000 * data_point[output_variable]#outputs[0][0]\n",
    "            out = post_processor.inverse_transform(outputs)[0][0] / 1000\n",
    "            print('prev_prediction : ', prev_prediction, 'out: ', out)\n",
    "            preds.append(out)\n",
    "            last_prediction_time= current_time\n",
    "            #print('prediction_time: ', data_point['date'])\n",
    "        j+= 1\n",
    "        print ('counter= ', j )\n",
    "        print('buffer: ', array, ' Predction= ', prev_prediction)\n",
    "        print('#########################################################################')\n",
    "    df['local_preds']= preds # mm\n",
    "    scatter_mode= 'lines'#'lines'# 'lines+markers'# 'markers'\n",
    "    selected_Columns= ['t_bett','t_motor', 't_spindle' , 'M8', 'M121', 'M127', 'M7']\n",
    "    local_prediction_error= df[output_variable] - df['local_preds']\n",
    "    y_axis_names= selected_Columns\n",
    "    signals= df\n",
    "    #print(preds)\n",
    "    fig= make_subplots(rows=len(selected_Columns)+2 ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_Columns, vertical_spacing=0.02)\n",
    "    for i in range(len(selected_Columns)):\n",
    "        fig.add_trace(go.Scatter(x= signals['date'], y= signals[selected_Columns[i]], name=selected_Columns[i], mode= scatter_mode), row= i+1, col= 1)\n",
    "        fig.update_yaxes(title_text= y_axis_names[i], row= i+1, col= 1)\n",
    "    ##Draw the prediciton and the real values of displacement on Welle\n",
    "    fig.add_trace(go.Scatter(x= signals['date'],y= 1000 * signals[output_variable], name=output_variable, mode= scatter_mode),  row= len(selected_Columns)+1 , col= 1)\n",
    "    fig.add_trace(go.Scatter(x= signals['date'], y=  1000 * signals['local_preds'], name='Local predictions', mode= scatter_mode),  row= len(selected_Columns)+1, col= 1)\n",
    "    fig.update_yaxes(title_text= output_variable, row= len(selected_Columns), col= 1)\n",
    "    fig.add_trace(go.Scatter(x= signals['date'], y=   1000 * local_prediction_error, name='New Model prediction Error', mode= scatter_mode),  row= len(selected_Columns)+2 , col= 1)\n",
    "    ## Draw the tolerence +-5\n",
    "    fig.add_trace(go.Scatter(x= signals['date'], y= np.full_like(local_prediction_error,5), name='+5 Max Error', mode= scatter_mode),  row= len(selected_Columns)+2 , col= 1)\n",
    "    fig.add_trace(go.Scatter(x= signals['date'], y= np.full_like(local_prediction_error,-5), name='-5 Min Error', mode= scatter_mode),  row= len(selected_Columns)+2 , col= 1)\n",
    "    #fig.add_trace(go.Scatter(x= signals['date'], y=   1000*( np.abs(signals[output_variable] - signals[prediction_variable])), name='Old Model prediction Error', mode= scatter_mode),  row= len(selected_Columns)+1 , col= 1)\n",
    "    fig.update_yaxes(title_text= 'Error (Micro-meter)', row= len(selected_Columns)+2, col= 1)\n",
    "    fig.update_layout(height=1200, width=1400, title_text= 'Prediction Results')\n",
    "    #pio.write_image(fig, str(images_path +'versuch_25_09_iso.png'), format='png')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path= str(source_dir + 'results/')\n",
    "#selected_Columns= ['t_spindel', 't_motor']\n",
    "# Validate the model based on validataion data\n",
    "data_path = str(source_dir + 'validate_Data/')\n",
    "#image_path= str(source_dir + 'results/')\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print('preprocessor is loaded successfully')\n",
    "model = Sequential()\n",
    "model = load_model(str(model_dir+'model.h5'), compile=False)\n",
    "print('Model is loaded successfully')\n",
    "os.chdir(data_path)\n",
    "errors = []\n",
    "error_df = pd.DataFrame()\n",
    "scatter_mode= 'lines'\n",
    "visualisation_selected_Columns= ['t_bett', 't_motor', 't_spindel', 'M8','M121', 'M127', 'M7',output_variable, 'prediction_Error' ]\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    signals = df\n",
    "    prev_time=None\n",
    "    drop_list= []\n",
    "    print('Before Filtering, ', len(signals))\n",
    "    for index,row in signals.iterrows():\n",
    "        current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        if prev_time is None:\n",
    "            prev_time= current_time\n",
    "        elif (current_time - prev_time).total_seconds() < 5:\n",
    "            drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            df= df[df['date'] != row['date']]\n",
    "        prev_time= current_time\n",
    "    signals= df\n",
    "    signals= smoothing_data(signals)\n",
    "    print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))\n",
    "    if signals.isnull().values.any():\n",
    "        print ('with nan')\n",
    "        exit()\n",
    "    ###Set index to data frame\n",
    "    signals['date']= pd.to_datetime(signals['date'],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    signals.set_index('date',inplace= True)\n",
    "    signals= signals[selected_Columns]\n",
    "    \n",
    "    # (1000 * df[str('smoothed_' + output_variable)]).round(2)#\n",
    "    output = (1000* df[output_variable]).round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)\n",
    "    partitions, target = generateDataSource(\n",
    "        signal_input=signals, input_columns=selected_Columns, output_length=1, signal_output=output, window=window, shift=1)\n",
    "    target = target.reshape((-1,))\n",
    "    predictions = model.predict(partitions)\n",
    "    predictions = np.reshape(predictions, newshape=target.shape)\n",
    "    diff = np.abs(target - predictions)\n",
    "    mae_v = ' mae= ' + str(np.mean(mean_absolute_error(target, predictions).round(2))) +' Micro_Meter '  \n",
    "    max_v = ' max= ' + str(np.max(np.abs(target - predictions).round(2))) +' Micro_Meter '  \n",
    "    fidelity= ' Fedility= ' +str(calc_fidelity_metric(target, predictions).round(2))+\"%\"\n",
    "    print(mae_v)\n",
    "    print(max_v)\n",
    "    print(fidelity)\n",
    "    ######################\n",
    "    # Visualization of lf-data with predicted results\n",
    "    visualisation_selected_Columns = selected_Columns.copy()\n",
    "    visualisation_selected_Columns.append(output_variable)\n",
    "    visualisation_df = df[window-1:]\n",
    "    visualisation_df['predictions'] = predictions\n",
    "    fig = make_subplots(rows= 1, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    ###verlagerung\n",
    "    fig.add_trace(go.Line(y= 1000 * visualisation_df[output_variable], name='Echte z_welle'), row= 1, col=1)\n",
    "    fig.add_trace(go.Line(y= visualisation_df['predictions'], name='KI_z_welle'), row= 1, col=1)\n",
    "    fig.update_yaxes(title_text='Verlagerung Mic-Meter', row= 1, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit (1= 2 Minuten)', row=1, col=1)\n",
    "    fig.update_layout(height=900, width=1200, title_text=str(file + mae_v + max_v+ fidelity))\n",
    "    fig.show()\n",
    "    #pio.write_image(fig, str(image_path + file+'_Verlagerung.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path= str(source_dir + 'results/')\n",
    "#selected_Columns= ['t_spindel', 't_motor']\n",
    "# Validate the model based on validataion data\n",
    "data_path = str(source_dir + 'validate_Data/')\n",
    "#image_path= str(source_dir + 'results/')\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print('preprocessor is loaded successfully')\n",
    "#model = build_Model_LSTM()\n",
    "model = load_model(str(model_dir+'best_model.h5'), compile=False)\n",
    "print('Model is loaded successfully')\n",
    "os.chdir(data_path)\n",
    "errors = []\n",
    "error_df = pd.DataFrame()\n",
    "scatter_mode= 'lines'\n",
    "visualisation_selected_Columns= ['t_bett', 't_motor', 't_spindle', 'M8','M121', 'M127', 'M7',output_variable, 'prediction_Error' ]\n",
    "for file in glob.glob('*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    #df = remove_peeks(df, output_variable, 4)\n",
    "    print('current File: ', file)\n",
    "    #df['t_spindel']= df['t_spindle']\n",
    "    #signals = preprocess_data(df, kuelung=['M8', 'M121', 'M127', 'M7'])\n",
    "    signals= df\n",
    "    prev_time=None\n",
    "    drop_list= []\n",
    "    print('Before Filtering, ', len(signals))\n",
    "    for index,row in signals.iterrows():\n",
    "        current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        if prev_time is None:\n",
    "            prev_time= current_time\n",
    "        elif (current_time.second - prev_time.second) == 0:\n",
    "            drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            df= df[df['date'] != row['date']]\n",
    "        prev_time= current_time\n",
    "    signals= df\n",
    "    print('After Filtering, ', len(signals), ', deleted= ', len(drop_list))\n",
    "    signals = df[selected_Columns]\n",
    "    signals = signals.to_numpy()\n",
    "    signals = pipeline.transform(signals)\n",
    "    # (1000 * df[str('smoothed_' + output_variable)]).round(2)#\n",
    "    output = (1000* df[output_variable]).round(3)#(1000 * df[str('smoothed_' + output_variable)]).round(3)\n",
    "    partitions, target = generateDataSource(\n",
    "        signal_input=signals, input_columns=selected_Columns, output_length=1, signal_output=output, window=window, shift=1)\n",
    "    target = target.reshape((-1,))\n",
    "    predictions = model.predict(partitions)\n",
    "    predictions = np.reshape(predictions, newshape=target.shape)\n",
    "    diff = np.abs(target - predictions)\n",
    "    mae_v = ' mae= ' + str(np.mean(mean_absolute_error(target, predictions)).round(2)) +' Micro_Meter '  \n",
    "    max_v = ' max= ' + str(np.max(np.abs(target - predictions)).round(2)) +' Micro_Meter '  \n",
    "    #fidelity= ' Fedility= ' +str(calc_fidelity_metric(target, predictions).round(2))+\"%\"\n",
    "    ######################\n",
    "    # ####Apply enterpolation###\n",
    "    # upsampled_variable= output_variable#str('interpolated_'+output_variable)\n",
    "    # signals[upsampled_variable]= signals[output_variable]\n",
    "    # current_welle_z= 1000000\n",
    "    # for index, row in signals.iterrows():\n",
    "    #     if row[upsampled_variable]!= current_welle_z: ### A New value is available\n",
    "    #         current_welle_z= row[upsampled_variable]\n",
    "    #     else:\n",
    "    #         signals.at[index,upsampled_variable]= np.NaN\n",
    "    # signals[upsampled_variable].interpolate(method='linear', order=3, axis= 0,inplace= True)\n",
    "    # output=  signals[output_variable]\n",
    "    #######################\n",
    "    # Visualization of lf-data with predicted results\n",
    "    visualisation_selected_Columns = selected_Columns.copy()\n",
    "    visualisation_selected_Columns.append(output_variable)\n",
    "    visualisation_df = df[window-1:]\n",
    "    visualisation_df[str('smoothed_' + output_variable)] = output\n",
    "    visualisation_df['predictions'] = predictions\n",
    "    fig = make_subplots(rows= 9, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    # ##t_bett\n",
    "    # fig.add_trace(go.Line(y=visualisation_df['t_bett'], name='t_bett'), row= 1, col=1)\n",
    "    # fig.update_yaxes(title_text='t_bett CÂ°', row= 1, col=1)\n",
    "    # ##t_bett\n",
    "    # fig.add_trace(go.Line(y=visualisation_df['drhz'], name='drhz'), row= 1, col=1)\n",
    "    # fig.update_yaxes(title_text='drhz RPM', row= 1, col=1)\n",
    "    ###t_motor\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_motor'], name='t_motor'), row= 2, col=1)\n",
    "    fig.update_yaxes(title_text='t_motor CÂ°', row= 2, col=1)\n",
    "    ###t_spindel\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_spindle'], name='t_spindle'), row= 3, col=1)\n",
    "    fig.update_yaxes(title_text='t_spindel CÂ°', row= 3, col=1)\n",
    "    ###M8\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M8'], name='M8'), row= 4, col=1)\n",
    "    fig.update_yaxes(title_text='M8', row= 4, col=1)\n",
    "    ###M121\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M121'], name='M121'), row= 5, col=1)\n",
    "    fig.update_yaxes(title_text='M121', row= 5, col=1)\n",
    "    ###M127\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M127'], name='M127'), row= 6, col=1)\n",
    "    fig.update_yaxes(title_text='M127', row= 6, col=1)\n",
    "    ###M7\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M7'], name='M7'), row= 7, col=1)\n",
    "    fig.update_yaxes(title_text='M7', row= 7, col=1)\n",
    "    ###verlagerung\n",
    "    fig.add_trace(go.Line(y= 1000 * visualisation_df[output_variable], name='z_welle_Soll'), row= 8, col=1)\n",
    "    fig.add_trace(go.Line(y= visualisation_df['predictions'], name='KI_z_welle_Ist'), row= 8, col=1)\n",
    "    fig.update_yaxes(title_text='Verlagerung Mic-Meter', row= 8, col=1)\n",
    "    ###Fehler\n",
    "    experiment_errors = (1000*visualisation_df[output_variable] - visualisation_df['predictions']).to_list()\n",
    "    fig.add_trace(go.Scatter( y=   experiment_errors, name='Prediction Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    ## Draw the tolerence +-5\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,5), name='+5 Max Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,-5), name='-5 Min Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.update_yaxes(title_text='Fehler Mic-Meter', row= 9, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit (1= 2 Minuten)', row=9, col=1)\n",
    "    fig.update_layout(height=900, width=1200, title_text=str(file + mae_v + max_v))\n",
    "    #fig.show()\n",
    "    pio.write_image(fig, str(image_path + file+'.png'), format='png')\n",
    "    current_error_df = pd.DataFrame(\n",
    "        {'verlagerung': visualisation_df['predictions'].to_list(), 'error': experiment_errors})\n",
    "    if error_df is None:\n",
    "        error_df = current_error_df\n",
    "    else:\n",
    "        error_df = pd.concat([error_df, current_error_df])\n",
    "    errors.extend(experiment_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of resulted Data set\n",
    "import plotly.express as px\n",
    "labels = error_df['verlagerung']\n",
    "verlagerung_df = pd.DataFrame({'verlagerung': labels})\n",
    "print(verlagerung_df.head(5))\n",
    "fig = px.histogram(verlagerung_df, x='verlagerung', nbins=100)\n",
    "mean = verlagerung_df['verlagerung'].mean()\n",
    "std = verlagerung_df['verlagerung'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(\n",
    "    fig, str(image_path + 'distribution of displacement.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of resulted Data set\n",
    "import plotly.express as px\n",
    "error_df['abs_error'] = error_df['error'].abs()\n",
    "print(error_df.head(5))\n",
    "fig = px.histogram(error_df, x='verlagerung',\n",
    "                   y='abs_error', nbins=100, histfunc='avg')\n",
    "mean = error_df['abs_error'].mean()\n",
    "std = error_df['abs_error'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(fig, str(\n",
    "    image_path + 'distribution of average prediction error on the displacement.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "error_df = pd.DataFrame({'errors': errors})\n",
    "print(error_df.head(5))\n",
    "fig = px.histogram(error_df, x='errors', nbins=50)\n",
    "mean = error_df['errors'].mean()\n",
    "std = error_df['errors'].std()\n",
    "fig.update_layout(height=900, width=900, title_text=str(\n",
    "    'Mean= ' + str(mean) + ', std=' + str(std)))\n",
    "fig.show()\n",
    "pio.write_image(fig, str(\n",
    "    image_path + 'Distribution of the prediction error.png'), format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from joblib import load\n",
    "# pipeline= load(str(preprocessor_dir+'preprocessor.p'))\n",
    "# print(pipeline)\n",
    "# initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
    "# onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "# with open(str(preprocessor_dir + \"prepeocessor.onnx\"), \"wb\") as f:\n",
    "#     f.write(onx.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# preprocessor= load(str(preprocessor_dir+\"preprocessor.p\"))\n",
    "# dump(preprocessor,str(preprocessor_dir+\"preprocessor.p\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a58a25343f99ae6e283c189afd7abc602dec6c63d356cacabf499812ae322086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
