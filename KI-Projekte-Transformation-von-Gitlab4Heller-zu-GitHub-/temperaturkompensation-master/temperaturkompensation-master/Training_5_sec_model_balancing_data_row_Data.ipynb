{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import silu, swish, linear, tanh, leaky_relu, sigmoid, relu, elu, selu, gelu, linear\n",
    "from keras.layers import Input, LSTM, Dense, GaussianNoise, BatchNormalization, Activation, PReLU,Dropout\n",
    "from keras.initializers.initializers_v2 import RandomNormal, RandomUniform\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "from keras.optimizers import nadam_v2, rmsprop_v2, adamax_v2,ftrl\n",
    "from keras.losses import mean_absolute_error as mae, mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from keras.models import Sequential\n",
    "import onnx\n",
    "import tf2onnx\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA,KernelPCA, IncrementalPCA, SparsePCA\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'v4.2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for expreiments .csvs\n",
    "source_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/Test_area/'\n",
    "testdata_dir = 'G:/Innovations@HELLER/DN/KI/Temperaturkompensation/2021_Spindelwachstumskompensation_KI_HSU_SC63/Messungen DC100 H5000 M57002/row_files/combines/smoothed/With_cooling_feature_for_Model_training_5s/Testing_data/'\n",
    "preprocessor_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "model_dir = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n",
    "log_dir = \"C:/Temperaturkompensation/logs/\" + \\\n",
    "    datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "weights_path = str(\"C:/weights\")\n",
    "image_path = 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\csvs\\\\smoothed\\\\AI_Model\\\\Backup\\\\Z_Welle\\\\' + model_version + '\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import LossFunctionWrapper, mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras.backend as k\n",
    "\n",
    "\n",
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.multiply(tf.add(tf.multiply(tf.reduce_max(\n",
    "        tf.abs((y_true - y_pred))), 1), mean_absolute_error(y_true, y_pred)), 1)\n",
    "    return error\n",
    "\n",
    "\n",
    "def max_loss_v1(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.multiply(tf.reduce_max(tf.abs((y_true - y_pred))), 1), tf.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return error\n",
    "\n",
    "def max_loss_v2(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(tf.multiply(tf.pow(tf.reduce_max(tf.abs((y_true - y_pred))),2), 1), mean_squared_error(y_true, y_pred))\n",
    "    return error\n",
    "\n",
    "def max_loss_v3_mae_std(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mean_ae= mean_absolute_error(y_true, y_pred)\n",
    "    vars= tf.pow(tf.subtract(tf.abs(y_true - y_pred),mean_ae),2)\n",
    "    errors_std= tf.sqrt(tf.reduce_mean(vars))\n",
    "    #errors_std= tf.reduce_mean(vars)\n",
    "    #error = tf.add(mean_squared_error(y_true, y_pred), errors_std)\n",
    "    error = tf.add(mean_ae, errors_std)\n",
    "    return error\n",
    "\n",
    "def calc_fidelity(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    # session= k.get_session()\n",
    "    # y_true_array= session.run(y_true)\n",
    "    # y_pred_array= session.run(y_pred)\n",
    "    # cc= np.corrcoef(y_true_array,y_pred_array)\n",
    "    NRMSE= tf.divide(tf.sqrt(mean_squared_error(y_true, y_pred)),tf.subtract(tf.reduce_max(y_true),tf.reduce_min(y_true)))\n",
    "    fidelity= tf.abs(tf.multiply(100.0, tf.subtract(1.0,NRMSE)))\n",
    "    return fidelity\n",
    "    \n",
    "special_loss = LossFunctionWrapper(max_loss_v3_mae_std)\n",
    "fidelity_wrapper= LossFunctionWrapper(calc_fidelity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement special preprocessing steps\n",
    "# 1 calculate new Feature\n",
    "# 2 make rounding\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def special_preprocess(data: np.ndarray):\n",
    "    delta_t_spindel_bett = data[:, 1] - data[:, 0]\n",
    "    delta_t_spindel_bett = delta_t_spindel_bett.reshape((-1, 1))\n",
    "    result = np.concatenate((data[:, 2:5], delta_t_spindel_bett), axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def rounding(signals: np.ndarray):\n",
    "    return np.round(signals, 2)\n",
    "\n",
    "\n",
    "def remove_t_bett(data: np.ndarray):\n",
    "    result = data[:, 1:]\n",
    "    result[:, 0] = result[:, 0] - data[:, 0]\n",
    "    result[:, 1] = result[:, 1] - data[:, 0]\n",
    "    return result\n",
    "\n",
    "def remove_t_bett_v2(data: np.ndarray): # 1 X window X features --> 1 X [window X (features-1)]\n",
    "    result= data[:,:,1:].copy()\n",
    "    result[:,:, 0] = result[:,:, 0] - data[:,:, 0]\n",
    "    result[:,:, 1] = result[:,:, 1] - data[:,:, 0]\n",
    "    result= np.reshape(result,newshape=(data.shape[0],-1))\n",
    "    #result= result[:,:-1]\n",
    "    return result   \n",
    "\n",
    "data = np.asarray([[[40, 90, 300], [500, 1500, 5000]],\n",
    "                   [[60, 90, 300], [500, 2000, 6000]],\n",
    "                   [[80, 90, 300], [500, 3000, 7000]],\n",
    "                   [[100, 90, 300], [500, 4000, 8000]]])\n",
    "print(data.shape)\n",
    "new_data= remove_t_bett_v2(data)\n",
    "print(new_data.shape)\n",
    "print(new_data)\n",
    "\n",
    "window = 60\n",
    "shift = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_Columns= ['diff', 'drhz','t_bett',]#,'t_kluelung']# , 'diff'\n",
    "# [ 't_bett','t_spindel', 't_y_achse', 't_z_achse', 'drhz', 't_motor','t_kss_tank', 't_raum', 't_spindel_vor', 't_spindel_rueck','t_schwenkantrieb']\n",
    "\n",
    "output_variable = 'welle_z_iterpolated'#'welle_z_iterpolated'#'welle_z_upsampled'#'z_welle_ok'  #'z_tcp_ok'# 'welle_z'#''#'smoothed_z_welle_ok'#\n",
    "output_variable_smoothed= 'welle_z_iterpolated'#output_variable +'_smoothed' \n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "post_processer = StandardScaler(with_mean=True, with_std=True)#MinMaxScaler(feature_range=(-1, 1))\n",
    "pca = PCA(n_components=6)\n",
    "#pca = IncrementalPCA(n_components=10)\n",
    "# Data Parameters\n",
    "selected_Columns = ['t_bett', 't_motor', 't_spindle', 'M8','M121', 'M127', 'M7']#['t_bett', 't_motor', 't_spindel', 'M8','M121', 'M127', 'M7']\n",
    "#,('smoothing_data', FunctionTransformer(smoothing_data))('remove_t_bett', FunctionTransformer(remove_t_bett_v2)), , ('pca', pca),('std2',StandardScaler(with_mean=True, with_std=True))\n",
    "proposed_pipline = Pipeline(steps=[ ('remove_t_bett', FunctionTransformer(remove_t_bett_v2)),('rounding', FunctionTransformer(rounding)), ('stdscaler', scaler), ('pca', pca)])  # ('std2',StandardScaler(with_mean=True, with_std=True))Pipeline(steps=[('rounding', FunctionTransformer(rounding))])#('pca', pca)\n",
    "preprocessor_name = \"preprocessor.p\"\n",
    "smoothed_output= False\n",
    "reset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generateDataSource(signal_input=None, output_length: int = 1, signal_output=None, window=1, shift=1):\n",
    "#     Signal_Length = signal_input.shape[0]\n",
    "#     num_samples = int((Signal_Length - (window-1) * shift))\n",
    "#     x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "#     y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "#     for i in range(num_samples):\n",
    "#         x[i] = np.asarray([signal_input[i + j * shift] for j in range(0,window)])\n",
    "#         y[i] = signal_output[i + (window-1) * shift :i+ (window-1) * shift + output_length]\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data___(input: pd.DataFrame, window= 240):\n",
    "    shift_value= int(-window/2)\n",
    "    data= input.copy()\n",
    "    # data['t_bett_smoothed']= data['t_bett'].rolling(window= window).mean().shift(shift_value)\n",
    "    # data['t_motor_smoothed']= data['t_motor'].rolling(window= window).mean().shift(shift_value)\n",
    "    # data['t_spindle_smoothed']= data['t_spindle'].rolling(window= window).mean().shift(shift_value)\n",
    "    # data[str(output_variable+'_smoothed')]= data[output_variable].rolling(window= window).mean().shift(shift_value)\n",
    "    data['t_bett']= data['t_bett'].rolling(window= window).mean().shift(shift_value)\n",
    "    data['t_motor']= data['t_motor'].rolling(window= window).mean().shift(shift_value)\n",
    "    data['t_spindle']= data['t_spindle'].rolling(window= window).mean().shift(shift_value)\n",
    "    data[str(output_variable+'_smoothed')]= data[output_variable].rolling(window= window).mean().shift(shift_value)\n",
    "    data= data[ window+1:-window]###Remove nan\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(input: pd.DataFrame, window= 240):\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource_v0(signal_input=None, output_length: int = 1, signal_output=None, window=1, shift=1): ## For flatten version\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    features_count= signal_input.shape[1]\n",
    "    num_samples = int((Signal_Length - (window-1) * shift))\n",
    "    #x = np.zeros(shape=(num_samples, window*(features_count + 1) - 1))\n",
    "    x = np.zeros(shape=(num_samples, window*(features_count)))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i]= np.asarray([signal_input[i + j * shift] for j in range(0,window)]).flatten()\n",
    "        # x[i] = np.concatenate((np.asarray([signal_input[i + j * shift] for j in range(0,window)]).flatten(),\n",
    "        #                         (np.asarray([signal_output[i + j * shift] for j in range(0,window-1)]).flatten())))\n",
    "        y[i] = signal_output[i + (window-1) * shift :i+ (window-1) * shift + output_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource(signal_input=None, output_length: int = 1, signal_output=None, window=1, shift=1):## for3d array without label\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    features_count= signal_input.shape[1]\n",
    "    num_samples = int((Signal_Length - (window-1) * shift))\n",
    "    x = np.zeros(shape=(num_samples,window, features_count))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i]= np.asarray([signal_input[i + j * shift] for j in range(0,window)])\n",
    "        # x[i] = np.concatenate((np.asarray([signal_input[i + j * shift] for j in range(0,window)]).flatten(),\n",
    "        #                         (np.asarray([signal_output[i + j * shift] for j in range(0,window-1)]).flatten())))\n",
    "        y[i] = signal_output[i + (window-1) * shift :i+ (window-1) * shift + output_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource__(signal_input=None,output_length: int = 1, signal_output=None, window=0, shift=0):## for3d array without label\n",
    "    x_train_windowed= TimeseriesGenerator(signal_input,targets= signal_output, length=window, sampling_rate= shift, batch_size= 1)\n",
    "    print('.... ' , x_train_windowed[0])\n",
    "    x= np.asarray([x_train_windowed[i][0].flatten() for i in range(0, len(x_train_windowed))])\n",
    "    x= x[1:]\n",
    "    y= np.asarray([x_train_windowed[i][1] for i in range(0, len(x_train_windowed))])\n",
    "    y= y[:-1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource_v3(signal_input=None, output_length: int = 1, signal_output=None, window=1, shift=1):## for3d array with label\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    features_count= signal_input.shape[1]\n",
    "    num_samples = int((Signal_Length - (window-1) * shift))\n",
    "    #x = np.zeros(shape=(num_samples, window*(features_count + 1) - 1))\n",
    "    x = np.zeros(shape=(num_samples,window, features_count + 1))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i]= np.asarray([np.append(signal_input[i + j * shift],signal_output[i + j * shift]) if j < (window-1) else\n",
    "                           np.append(signal_input[i + j * shift],np.asanyarray([0])) for j in range(0,window)])\n",
    "        y[i] = signal_output[i + (window-1) * shift :i+ (window-1) * shift + output_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.asarray([\n",
    "    [10,1,1,1,1],\n",
    "    [20,2,2,2,2],\n",
    "    [30,3,3,3,3],\n",
    "    [40,4,4,4,4],\n",
    "    [50,5,5,5,5],\n",
    "    [60,6,6,6,6],\n",
    "    [70,7,7,7,7],\n",
    "    [80,8,8,8,8],\n",
    "    [90,9,9,9,9],\n",
    "    [100,10,10,10,10]\n",
    "])\n",
    "y= np.asanyarray([11,22,33,44,55,66,77,88,99,1010])\n",
    "dx, dy= generateDataSource(signal_input=x, signal_output=y,window=2,shift=3)\n",
    "print(dx)\n",
    "print (dy)\n",
    "#dx_new= remove_t_bett_v2(dx)\n",
    "#dx_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data source from all signals and create Training dataset\n",
    "working_dir = source_dir\n",
    "os.chdir(working_dir)\n",
    "x_train: np.ndarray = None\n",
    "y_train: np.ndarray = None\n",
    "#selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    prev_time=None\n",
    "    drop_list= []\n",
    "    signals_filtered= smoothing_data(df, window= 24)\n",
    "    if signals_filtered.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    input_signal= signals_filtered[selected_Columns].to_numpy()\n",
    "    output_signal= 1000 * signals_filtered[output_variable_smoothed].to_numpy()\n",
    "    windowed_data_x, windowed_data_y= generateDataSource(signal_input= input_signal, output_length=1, signal_output=output_signal, window= window, shift=shift)\n",
    "    if x_train is None:\n",
    "        x_train = windowed_data_x\n",
    "        y_train= windowed_data_y\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train, windowed_data_x))\n",
    "        y_train = np.concatenate((y_train, windowed_data_y))\n",
    "\n",
    "    print('input: ', x_train.shape, ' output:', y_train.shape)\n",
    "    print('Input Nans: ',np.isnan(x_train).any())\n",
    "    print('Input Nans: ',np.isnan(y_train).any())\n",
    "    print('###########################')\n",
    "y_train= y_train.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x:np.ndarray, y: np.ndarray):\n",
    "    indecies= list(range(0, x.shape[0]))\n",
    "    tempdf= pd.DataFrame({'idx': indecies,'y': y.reshape((-1,))})\n",
    "    #print(tempdf.head(5))\n",
    "    min_v= tempdf['y'].min()\n",
    "    max_v= tempdf['y'].max()\n",
    "    y_range= range(int(min_v), int(max_v),1)\n",
    "    distributions= []\n",
    "    min_count= len(tempdf)\n",
    "    for i in y_range:\n",
    "        filtered_data= tempdf[(tempdf['y']> i) & (tempdf['y']<= i+1)]\n",
    "        counts= len(filtered_data)\n",
    "        if counts< min_count:\n",
    "            min_count= counts\n",
    "        distributions.append(filtered_data)\n",
    "    x_new= None\n",
    "    y_new= None\n",
    "    for filtered_data in distributions:\n",
    "        resampled_chunk= filtered_data[:min_count]\n",
    "        #print(resampled_chunk.head(3))\n",
    "        if x_new is None:\n",
    "            y_new= resampled_chunk['y'].to_numpy()\n",
    "            x_new= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "        else:\n",
    "            ys= resampled_chunk['y'].to_numpy()\n",
    "            y_new= np.concatenate((y_new,ys))\n",
    "            xs= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "            x_new= np.concatenate((x_new, xs))\n",
    "        #print(x_new.shape,'  ',y_new.shape)\n",
    "        # print('Chunk: ', resampled_chunk.iloc[0:1])\n",
    "        # print('True: ', x[resampled_chunk.iloc[0:1]])\n",
    "    x_new= x_new.reshape((-1, x.shape[1], x.shape[2]))\n",
    "    y_new= y_new.reshape((-1, 1))\n",
    "    print(x_new.shape,'  ',y_new.shape)\n",
    "    return x_new, y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_upsampling(x:np.ndarray, y: np.ndarray):\n",
    "    indecies= list(range(0, x.shape[0]))\n",
    "    tempdf= pd.DataFrame({'idx': indecies,'y': y.reshape((-1,))})\n",
    "    #print(tempdf.head(5))\n",
    "    min_v= tempdf['y'].min()\n",
    "    max_v= tempdf['y'].max()\n",
    "    y_range= range(int(min_v), int(max_v),1)\n",
    "    distributions= []\n",
    "    max_count= 0\n",
    "    for i in y_range:\n",
    "        filtered_data= tempdf[(tempdf['y']> i) & (tempdf['y']<= i+1)]\n",
    "        counts= len(filtered_data)\n",
    "        if counts> max_count:\n",
    "            max_count= counts\n",
    "        distributions.append(filtered_data)\n",
    "    x_new= None\n",
    "    y_new= None\n",
    "    for filtered_data in distributions:\n",
    "        current_length= len(filtered_data)\n",
    "        diff= max_count - current_length\n",
    "        chunk_num= int(diff/current_length) + 1\n",
    "        #print(chunk_num)\n",
    "        for i in range(chunk_num):\n",
    "            resampled_chunk= filtered_data\n",
    "            #print(resampled_chunk.head(3))\n",
    "            if x_new is None:\n",
    "                y_new= resampled_chunk['y'].to_numpy()\n",
    "                x_new= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "            else:\n",
    "                ys= resampled_chunk['y'].to_numpy()\n",
    "                y_new= np.concatenate((y_new,ys))\n",
    "                xs= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "                x_new= np.concatenate((x_new, xs))\n",
    "        ####Add rest values\n",
    "        rest_count= int(diff % current_length)\n",
    "        if rest_count != 0:\n",
    "            resampled_chunk= filtered_data[:rest_count]\n",
    "            if x_new is None:\n",
    "                y_new= resampled_chunk['y'].to_numpy()\n",
    "                x_new= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "            else:\n",
    "                ys= resampled_chunk['y'].to_numpy()\n",
    "                y_new= np.concatenate((y_new,ys))\n",
    "                xs= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "                x_new= np.concatenate((x_new, xs))\n",
    "    x_new= x_new.reshape((-1, x.shape[1], x.shape[2]))\n",
    "    y_new= y_new.reshape((-1, 1))\n",
    "    print(x_new.shape,'  ',y_new.shape)\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_upsampling_random_v(x:np.ndarray, y: np.ndarray):\n",
    "    indecies= list(range(0, x.shape[0]))\n",
    "    tempdf= pd.DataFrame({'idx': indecies,'y': y.reshape((-1,))})\n",
    "    #print(tempdf.head(5))\n",
    "    min_v= tempdf['y'].min()\n",
    "    max_v= tempdf['y'].max()\n",
    "    min_v= int(np.floor(min_v))\n",
    "    max_v= int(np.ceil(max_v))\n",
    "    y_range= range(min_v, max_v,1)\n",
    "    distributions= []\n",
    "    max_count= 0\n",
    "    for i in y_range:\n",
    "        filtered_data= tempdf[(tempdf['y']> i) & (tempdf['y']<= i+1)]\n",
    "        counts= len(filtered_data)\n",
    "        if counts> max_count:\n",
    "            max_count= counts\n",
    "        distributions.append(filtered_data)\n",
    "    x_new= None\n",
    "    y_new= None\n",
    "    for filtered_data in distributions:\n",
    "        current_length= len(filtered_data)\n",
    "        diff= max_count - current_length\n",
    "        chunk_num= int(diff/current_length) + 1\n",
    "        #print(chunk_num)\n",
    "        for i in range(chunk_num):\n",
    "            resampled_chunk= filtered_data\n",
    "            #print(resampled_chunk.head(3))\n",
    "            if x_new is None:\n",
    "                y_new= resampled_chunk['y'].to_numpy()\n",
    "                x_new= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "            else:\n",
    "                ys= resampled_chunk['y'].to_numpy()\n",
    "                y_new= np.concatenate((y_new,ys))\n",
    "                xs= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "                x_new= np.concatenate((x_new, xs))\n",
    "        ####Add rest values\n",
    "        rest_count= int(diff % current_length)\n",
    "        if rest_count != 0:\n",
    "            #resampled_chunk= filtered_data[:rest_count]\n",
    "            #### sample random rest_count values from filtered_Data\n",
    "            resampled_chunk= filtered_data.sample(n= rest_count, random_state=rest_count)\n",
    "            if x_new is None:\n",
    "                y_new= resampled_chunk['y'].to_numpy()\n",
    "                x_new= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "            else:\n",
    "                ys= resampled_chunk['y'].to_numpy()\n",
    "                y_new= np.concatenate((y_new,ys))\n",
    "                xs= np.asanyarray([x[i] for i in resampled_chunk['idx'].to_list()])\n",
    "                x_new= np.concatenate((x_new, xs))\n",
    "    x_new= x_new.reshape((-1, x.shape[1], x.shape[2]))\n",
    "    y_new= y_new.reshape((-1, 1))\n",
    "    print(x_new.shape,'  ',y_new.shape)\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train= balance_data_upsampling_random_v(x= x_train, y= y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data source from all signals and create Training dataset\n",
    "working_dir = testdata_dir\n",
    "os.chdir(working_dir)\n",
    "x_test: np.ndarray = None\n",
    "y_test: np.ndarray = None\n",
    "#selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    #signals = signals[selected_Columns]\n",
    "    signals_filtered= smoothing_data(df, window= 24)\n",
    "    if signals_filtered.isnull().values.any():\n",
    "        print ('with nan')\n",
    "        exit()\n",
    "    input_signal= signals_filtered[selected_Columns].to_numpy()\n",
    "    output_signal= 1000 * signals_filtered[output_variable_smoothed].to_numpy()\n",
    "    windowed_data_x, windowed_data_y= generateDataSource(signal_input= input_signal, output_length=1, signal_output=output_signal, window= window, shift=shift)\n",
    "    if x_test is None:\n",
    "        x_test = windowed_data_x\n",
    "        y_test= windowed_data_y\n",
    "    else:\n",
    "        x_test = np.concatenate((x_test, windowed_data_x))\n",
    "        y_test = np.concatenate((y_test, windowed_data_y))\n",
    "    \n",
    "    print('input: ', x_test.shape, ' output:', y_test.shape)\n",
    "    print('Input Nans: ',np.isnan(x_test).any())\n",
    "    print('Input Nans: ',np.isnan(y_test).any())\n",
    "    print('###########################')\n",
    "y_test= y_test.reshape((-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= None\n",
    "print('input: ', x_train.shape, ' output:', y_train.shape)\n",
    "if pipeline is None:\n",
    "    try:\n",
    "        if not reset:\n",
    "            pipeline = load(str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is loaded successfully')\n",
    "        else:\n",
    "            pipeline = proposed_pipline\n",
    "            pipeline = pipeline.fit(x_train)\n",
    "            #dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is saved successfully')\n",
    "            print('explained_variance_ratio_= ',     pipeline.steps[3][1].explained_variance_ratio_)\n",
    "    except FileNotFoundError as err:\n",
    "        pipeline = proposed_pipline\n",
    "        pipeline = pipeline.fit(x_train)\n",
    "        #dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "        # initial_type = [('float_input', FloatTensorType(\n",
    "        #     [None,  x_train.shape[1]]))]\n",
    "        # onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "        # with open(str(preprocessor_dir + \"preprocessor.onnx\"), \"wb\") as f:\n",
    "        #     f.write(onx.SerializeToString())\n",
    "        print('preprocessor is saved successfully')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processer= post_processer.fit(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_post= y_train#post_processer.transform(y_train)\n",
    "y_test_post= y_test#post_processer.transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed= pipeline.transform(x_train)#[:,[0,4,5]]\n",
    "#x_test_preprocessed= pipeline.transform(x_test)#[:,[0,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df= pd.DataFrame(x_train_preprocessed)\n",
    "# dataset_df['output']= y_train_post\n",
    "# dataset_df.head(5)\n",
    "# dataset_df.to_csv('dataset_balanced_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(x_train_preprocessed)#(np.concatenate([x_train_preprocessed,x_test_preprocessed]))\n",
    "df['output']= y_train_post.flatten()#np.concatenate([y_train_post.flatten(),y_test_post.flatten()])\n",
    "# df= pd.DataFrame(x_test_preprocessed)\n",
    "# df['output']= y_test.flatten()\n",
    "# df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr().round(4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 15))\n",
    "plt.title('Heat-Map für die Korrelationsmatrix')\n",
    "sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='magma', annot_kws={\"size\": 26, 'color': 'black'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(old_value, old_min, old_max, new_max, new_min):\n",
    "    new_value= ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
    "    return new_value\n",
    "def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "\n",
    "    y_train_rounded = np.round(y_train,decimals=3)\n",
    "    N = len(y_train_rounded)\n",
    "    y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "    weights = np.ones(shape=(N,),dtype= np.float32)\n",
    "    for i in range(0, N):\n",
    "        current_value = y_train_rounded[i]\n",
    "        occurences = np.count_nonzero(y_train_rounded == current_value)\n",
    "        if occurences <= occurance_threshold:\n",
    "             weights[i]=  N/(occurences + 1 )#N/(occurences + y_train_rounded[i])#N/occurences # 0\n",
    "        else:\n",
    "            weights[i] =  N/(occurences + 1)#(8 if y_train_rounded[i] < 20 else 1)\n",
    "    old_min= weights.min()\n",
    "    old_max= weights.max()\n",
    "    new_min = 1\n",
    "    new_max = 2\n",
    "    #weights_scaled= np.apply_along_axis(map, 1, weights)\n",
    "    weights_scaled= np.asanyarray([ map(weights[i], old_min, old_max, new_max, new_min) for i in range(0, weights.shape[0])]).reshape(weights.shape)\n",
    "    #weights = weights - weights.min() + 1\n",
    "    #weights = (weights - weights.min())/(weights.max() - weights.min())\n",
    "    fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=weights_scaled,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 1, col= 1)\n",
    "    fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 2, col= 1)\n",
    "    # fig.update_yaxes(title_text= 'weight', row= 1, col= 1)\n",
    "    # fig.update_xaxes(title_text= 'label value', row= 1, col= 1)\n",
    "    fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "    fig.show()\n",
    "\n",
    "    return weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Model(X_train, y_train):\n",
    "    activation_Function = selu\n",
    "    dropout = 0.05\n",
    "    l1_v = 0.0005\n",
    "    L2_v = 0.0005\n",
    "    hidden_layers_structure = [500,400,300,200,100,50,20]#[100,50,40,30,20,10]\n",
    "    unroll = False\n",
    "    kernal_init = RandomUniform()  # 'he_normal'\n",
    "    #optimizer = nadam_v2.Nadam()\n",
    "    optimizer= adamax_v2.Adamax()\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    # model.add(Dense(100, activation=activation_Function))\n",
    "    # #model.add(Dropout(dropout))\n",
    "    # model.add(BatchNormalization()), kernel_initializer='he_normal', bias_initializer= 'he_normal'\n",
    "    for i in range(len(hidden_layers_structure)):\n",
    "        model.add(Dense(hidden_layers_structure[i], bias_regularizer =L1L2(l1=l1_v, l2=L2_v), kernel_regularizer=L1L2(l1=l1_v, l2=L2_v)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(activation_Function))\n",
    "        #model.add(PReLU())\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    #model.compile(optimizer= optimizer, loss=special_loss, metrics=[max_loss,mape])\n",
    "    model.compile(optimizer= optimizer, loss=mae, metrics=[max_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Block\n",
    "from keras.callbacks import LambdaCallback\n",
    "checkpoint = ModelCheckpoint(filepath=str(model_dir+'best_model.h5'), monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(model.layers[0].get_weights()))\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model = build_Model(X_train=x_train_preprocessed, y_train=y_train_post)\n",
    "#\n",
    "#weights = calculate_weights(y_train=y_train_post, occurance_threshold= 5)\n",
    "reset= True\n",
    "try:\n",
    "    if not reset:\n",
    "        model.load_weights(filepath=weights_path)\n",
    "        #model.set_weights(model_weights)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print(model.output_shape)\n",
    "print(model.summary())\n",
    "summary = model.fit(x=x_train_preprocessed, y=y_train_post, shuffle=True, batch_size=64,  epochs=2000,   validation_split= 0.15,#validation_data=(x_test_preprocessed, y_test_post),# \n",
    "                    callbacks=[earlyStopping, checkpoint], verbose=2, workers=64, use_multiprocessing=True)#, sample_weight=weights)\n",
    "model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "input_signature = [tf.TensorSpec(\n",
    "    [None, x_train_preprocessed.shape[1]], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model=model, input_signature=input_signature, opset=10)\n",
    "onnx.save_model(onnx_model, str(model_dir+'model.onnx'))\n",
    "print(\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights= model.get_weights()\n",
    "#model.save_weights(weights_path,save_format='h5')\n",
    "# model.save_weights(weights_path,save_format='tf'),30\n",
    "# model.save(str(model_dir+'Temp_model.h5'), save_format='h5',include_optimizer=True,save_traces=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = summary.history['loss']\n",
    "val_loss = summary.history['val_loss']\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "                    print_grid=True,  vertical_spacing=0.02)\n",
    "fig.add_trace(go.Line(y=loss, name='Training Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=val_loss, name='Validation Loss'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Max_Loss', row=1, col=1)\n",
    "fig.update_layout(height=900, width=900, title_text='training_curve')\n",
    "fig.show()\n",
    "pio.write_image(fig, str(image_path+'training_curve.png'), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tolerance = 5\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline = x_steps + tolerance * np.sin(np.pi/4)\n",
    "# Create Tolerance Lines\n",
    "tolerance = 10\n",
    "x_steps = np.linspace(0, 50, 10)\n",
    "lowerline_actual = x_steps - tolerance * np.sin(np.pi/4)\n",
    "upperline_actual = x_steps + tolerance * np.sin(np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# model= build_Model(X_train= results,y_train= targets)\n",
    "# model.load_weights(filepath= weights_path)\n",
    "# model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "model = Sequential()\n",
    "model = load_model(filepath=str(model_dir+'model.h5'), compile=False)\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print(model.summary())\n",
    "print(model.input_shape)\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(x_train_preprocessed)\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of DENSE Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid()\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(y_train, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'training_results.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# model= build_Model(X_train= results,y_train= targets)\n",
    "# model.load_weights(filepath= weights_path)\n",
    "# model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "model = Sequential()\n",
    "model = load_model(filepath=str(model_dir+'model2.h5'), compile=False)\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print(model.summary())\n",
    "print(model.input_shape)\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred = model.predict(x_test_preprocessed)\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of DENSE Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid()\n",
    "plt.plot(x_steps, upperline, label='Upper Bound', color='green')\n",
    "plt.scatter(y_test, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, lowerline, label='lower Bound', color='green')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line')\n",
    "plt.plot(x_steps, lowerline_actual, label='Actual lower Bound', color='red')\n",
    "plt.plot(x_steps, upperline_actual, label='Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(image_path+'training_results.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(source_dir + 'validate_Data/')\n",
    "#image_path= str(source_dir + 'results/')\n",
    "pipeline = load(str(preprocessor_dir+preprocessor_name))\n",
    "print('preprocessor is loaded successfully')\n",
    "model = Sequential()\n",
    "model = load_model(str(model_dir+'model.h5'), compile=False)\n",
    "print('Model is loaded successfully')\n",
    "os.chdir(data_path)\n",
    "x_test: np.ndarray = None\n",
    "y_test: np.ndarray = None\n",
    "errors = []\n",
    "error_df = pd.DataFrame()\n",
    "scatter_mode= 'lines'\n",
    "#selected_Columns= ['t_motor','t_spindel', 'drhz']\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    signals = df\n",
    "    prev_time=None\n",
    "    drop_list= []\n",
    "    print('Before Filtering, ', len(signals))\n",
    "    for index,row in signals.iterrows():\n",
    "        current_time= datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        if prev_time is None:\n",
    "            prev_time= current_time\n",
    "        elif (current_time - prev_time).total_seconds() < 5:\n",
    "            drop_list.append(datetime.strftime(current_time, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            df= df[df['date'] != row['date']]\n",
    "        prev_time= current_time\n",
    "    signals_filtered= df\n",
    "    print('After Filtering, ', len(signals_filtered), ', deleted= ', len(drop_list))\n",
    "    #signals = signals[selected_Columns]\n",
    "    signals_filtered= smoothing_data(signals_filtered)\n",
    "    if signals_filtered.isnull().values.any():\n",
    "        print ('with nan')\n",
    "    #signals = preprocess_data(signals, kuelung=['M8', 'M121', 'M127', 'M7'])\n",
    "    input_signal= signals_filtered[selected_Columns].to_numpy()\n",
    "    output_signal= 1000 * signals_filtered[output_variable].to_numpy()\n",
    "    windowed_data_x, windowed_data_y= generateDataSource(signal_input= input_signal, output_length=1, signal_output=output_signal, window= window, shift=shift)\n",
    "    x_data_preprocessed= pipeline.transform(windowed_data_x)\n",
    "    predictions= model.predict(x_data_preprocessed)\n",
    "    y_true = windowed_data_y.reshape((-1,))\n",
    "    predictions = np.reshape(predictions, newshape= y_true.shape)\n",
    "    diff = np.abs(y_true - predictions)\n",
    "    mae_v = ' mae= ' + str(np.mean(mean_absolute_error(y_true, predictions)).round(2)) +' Micro_Meter '  \n",
    "    max_v = ' max= ' + str(np.max(np.abs(y_true - predictions)).round(2)) +' Micro_Meter '  \n",
    "    ######################\n",
    "    # Visualization of lf-data with predicted results\n",
    "    visualisation_df= signals_filtered[window-1:]\n",
    "    print(visualisation_df.columns)\n",
    "    visualisation_df['predictions'] = predictions\n",
    "    visualisation_df[output_variable]= y_true\n",
    "    fig = make_subplots(rows= 9, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    # ##t_bett\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_bett'], name='t_bett'), row= 1, col=1)\n",
    "    fig.update_yaxes(title_text='t_bett C°', row= 1, col=1)\n",
    "    ###t_motor\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_motor'], name='t_motor'), row= 2, col=1)\n",
    "    fig.update_yaxes(title_text='t_motor C°', row= 2, col=1)\n",
    "    ###t_spindel\n",
    "    fig.add_trace(go.Line(y=visualisation_df['t_spindle'], name='t_spindle'), row= 3, col=1)\n",
    "    fig.update_yaxes(title_text='t_spindel C°', row= 3, col=1)\n",
    "    ###M8\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M8'], name='M8'), row= 4, col=1)\n",
    "    fig.update_yaxes(title_text='M8', row= 4, col=1)\n",
    "    ###M121\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M121'], name='M121'), row= 5, col=1)\n",
    "    fig.update_yaxes(title_text='M121', row= 5, col=1)\n",
    "    ###M127\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M127'], name='M127'), row= 6, col=1)\n",
    "    fig.update_yaxes(title_text='M127', row= 6, col=1)\n",
    "    ###M7\n",
    "    fig.add_trace(go.Line(y=visualisation_df['M7'], name='M7'), row= 7, col=1)\n",
    "    fig.update_yaxes(title_text='M7', row= 7, col=1)\n",
    "    ###verlagerung\n",
    "    fig.add_trace(go.Line(y=  visualisation_df[output_variable], name='z_welle_Soll'), row= 8, col=1)\n",
    "    fig.add_trace(go.Line(y=  visualisation_df['predictions'], name='KI_z_welle_Ist'), row= 8, col=1)\n",
    "    #fig.add_trace(go.Line(y= 1000* visualisation_df['prediction2'], name='KI_z_welle_during_Manufacturing'), row= 8, col=1)\n",
    "    fig.update_yaxes(title_text='Verlagerung Mic-Meter', row= 8, col=1)\n",
    "    ###Fehler\n",
    "    experiment_errors = (visualisation_df[output_variable] - visualisation_df['predictions']).to_list()\n",
    "    fig.add_trace(go.Scatter( y=   experiment_errors, name='Prediction Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    ## Draw the tolerence +-5\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,5), name='+5 Max Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.add_trace(go.Scatter(y= np.full_like(experiment_errors,-5), name='-5 Min Error', mode= scatter_mode),  row= 9 , col= 1)\n",
    "    fig.update_yaxes(title_text='Fehler Mic-Meter', row= 9, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit (1= 2 Minuten)', row=9, col=1)\n",
    "    fig.update_layout(height=900, width=1200, title_text=str(file + mae_v + max_v))\n",
    "    fig.show()\n",
    "    pio.write_image(fig, str(image_path + file+'.png'), format='png')\n",
    "    current_error_df = pd.DataFrame(\n",
    "        {'verlagerung': visualisation_df['predictions'].to_list(), 'error': experiment_errors})\n",
    "    if error_df is None:\n",
    "        error_df = current_error_df\n",
    "    else:\n",
    "        error_df = pd.concat([error_df, current_error_df])\n",
    "    errors.extend(experiment_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a58a25343f99ae6e283c189afd7abc602dec6c63d356cacabf499812ae322086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
