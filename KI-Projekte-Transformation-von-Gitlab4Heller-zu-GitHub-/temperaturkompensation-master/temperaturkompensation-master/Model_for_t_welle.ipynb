{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import convert_sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import load_model\n",
    "import tf2onnx\n",
    "import onnx\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from joblib import dump, load\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM,Dense, GaussianNoise, BatchNormalization, Activation, PReLU, Dropout\n",
    "from keras.losses import mean_absolute_error as mae, mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from keras.optimizers import nadam_v2, rmsprop_v2, adamax_v2\n",
    "from keras.activations import silu,swish, linear, tanh,leaky_relu, sigmoid, relu,elu, selu, gelu, linear\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\New_Experiments\\\\ki_modell\\\\t_welle\\\\data\\\\'\n",
    "model_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\New_Experiments\\\\ki_modell\\\\t_welle\\\\modell\\\\'\n",
    "preprocessor_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\New_Experiments\\\\ki_modell\\\\t_welle\\\\preprocessor\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import LossFunctionWrapper, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import keras.backend as k\n",
    "\n",
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    #error= tf.multiply(tf.add(tf.multiply(tf.reduce_max(tf.abs((y_true - y_pred))),0.2) , mean_absolute_error(y_true , y_pred)),1000)\n",
    "    error= tf.add(tf.multiply(tf.reduce_max(tf.abs((y_true - y_pred))),1) , mean_absolute_error(y_true , y_pred))\n",
    "    return error\n",
    "special_loss= LossFunctionWrapper(max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSource(signal_input= None, input_columns: list = [], output_length: int=1, signal_output= None, window= 1 , shift= 1):\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    num_samples= int((Signal_Length - window +1 ) / shift)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y= np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range (num_samples):\n",
    "        x[i]= signal_input[i * shift : i * shift + window]\n",
    "        y[i]= signal_output[i * shift + window - 1 : i * shift + window - 1+ output_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_Model( X_train,y_train):\n",
    "    activation_Function= tanh\n",
    "    dropout= 0.0\n",
    "    l1_v= 0.00001\n",
    "    L2_v= 0.00001\n",
    "    size= 10\n",
    "    n_hidden_layers=4\n",
    "    unroll= False\n",
    "    optimizer= nadam_v2.Nadam()#\n",
    "    model= Sequential()\n",
    "    model.add(LSTM(size, input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences= True))#,unroll= unroll, dropout=dropout, kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(activation= activation_Function))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(PReLU())\n",
    "    for i in range(n_hidden_layers):\n",
    "      model.add(LSTM(size, return_sequences= True))#, unroll= unroll, dropout=dropout, kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "      #model.add(BatchNormalization())\n",
    "      model.add(Activation(activation= activation_Function))\n",
    "      #model.add(BatchNormalization())\n",
    "      #model.add(PReLU())\n",
    "    model.add(LSTM(size, return_sequences= False))#, unroll= unroll, dropout=dropout, kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(activation= activation_Function))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(PReLU())\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer= optimizer, loss= special_loss , metrics=[mae])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####MLP#####\n",
    "def build_Model_MLP(X_train,y_train, batch_size= 1):\n",
    "    dropout= 0.0\n",
    "    l1_v= 0.00001\n",
    "    L2_v= 0.00001\n",
    "    activation= relu\n",
    "    optimizer= nadam_v2.Nadam(learning_rate=0.001)\n",
    "    model= Sequential()\n",
    "    model.add(Dense(50, input_shape =  (X_train.shape[1],X_train.shape[2])))#,kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#, kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#, kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))#\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#,kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#,kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#,kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50))#,kernel_regularizer=L1L2(l1= l1_v, l2=L2_v)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer= optimizer, loss= max_loss , metrics=[mae])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement special preprocessing steps \n",
    "### 1 calculate new Feature\n",
    "### 2 make rounding\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def special_preprocess(data: np.ndarray):\n",
    "    delta_t_spindel_bett= data[:,1] - data[:,0]\n",
    "    delta_t_spindel_bett= delta_t_spindel_bett.reshape((-1,1))\n",
    "    result= np.concatenate(( data[:,2:5], delta_t_spindel_bett), axis=1)\n",
    "    return result\n",
    "\n",
    "def rounding(signals: np.ndarray):\n",
    "    return np.round(signals,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_Columns= ['diff', 'drhz','t_bett',]#,'t_kluelung']# , 'diff'\n",
    "# [ 't_bett','t_spindel', 't_y_achse', 't_z_achse', 'drhz', 't_motor','t_kss_tank', 't_raum', 't_spindel_vor', 't_spindel_rueck','t_schwenkantrieb']\n",
    "scaler= StandardScaler(with_mean= True, with_std= True)\n",
    "minmax= MinMaxScaler(feature_range=(-1,1))\n",
    "pca= PCA(n_components=2)\n",
    "##Data Parameters\n",
    "selected_Columns= ['t_motor']\n",
    "window=15\n",
    "shift= 1\n",
    "proposed_pipline= Pipeline(steps=[('special', FunctionTransformer(rounding))])# #Pipeline(steps=[('stdscaler', scaler), ('pca', pca)])# ('stdscaler', scaler),\n",
    "preprocessor_name= \"preprocessor.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(df:pd.DataFrame, name: str, features: list):\n",
    "    fig, ax= plt.subplots(figsize=(15,15))\n",
    "    print(name)\n",
    "    plt.title(name)\n",
    "    plt.grid()\n",
    "    i= 0\n",
    "    for feature in features:\n",
    "        i+=1\n",
    "        plt.subplot(7,1,i)\n",
    "        plt.plot(df[feature], label= feature)\n",
    "        plt.legend() \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram(df:pd.DataFrame, name: str, features: list):\n",
    "    fig, ax= plt.subplots(figsize=(15,15))\n",
    "    plt.title(name)\n",
    "    plt.grid()\n",
    "    i= 0\n",
    "    for feature in features:\n",
    "        i+=1\n",
    "        plt.subplot(7,1,i)\n",
    "        plt.hist(df[feature], label= feature)\n",
    "        plt.legend() \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymongo import MongoClient\n",
    "# #### Read LF data ##New data structure\n",
    "# host= \"localhost\"\n",
    "# port=27017\n",
    "# client = MongoClient(host= host, port=port)\n",
    "# db = client.h4s\n",
    "# event_list = db.lf_data.find().sort('date', 1)\n",
    "# lf_signals= None\n",
    "# for event in event_list:\n",
    "#     record_list= event['content']\n",
    "#     keys= None\n",
    "#     for record in record_list:\n",
    "#         keys= record['raw_data'].keys()\n",
    "#         for item in keys:\n",
    "#             record['raw_data'][item]= [record['raw_data'][item]]\n",
    "#         record['raw_data']['date']=[record['date']]\n",
    "#         record['raw_data']['DRZ'][0]= float(record['raw_data']['DRZ'][0])\n",
    "#         lf_signal_point= pd.DataFrame(record['raw_data'])\n",
    "#         if lf_signals is None:\n",
    "#             lf_signals= lf_signal_point\n",
    "#         else:\n",
    "#             lf_signals= lf_signals.append(lf_signal_point,ignore_index= True)\n",
    "# lf_signals['t_motor']= lf_signals['t_motor'].apply(np.float32)\n",
    "# lf_signals.reset_index(inplace= True)\n",
    "# lf_signals.to_csv(str(source_dir + 'lf2.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (lf_signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Data source from all signals and create Training dataset\n",
    "working_dir=  source_dir\n",
    "os.chdir(working_dir)\n",
    "results: np.ndarray= None\n",
    "targets: np.ndarray= None\n",
    "pipeline= None\n",
    "reset= True\n",
    "training_data= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df= pd.read_csv(file)\n",
    "    signals= df\n",
    "    signals= signals[selected_Columns]\n",
    "    if training_data is None:\n",
    "        training_data= signals\n",
    "    else:\n",
    "        training_data= pd.concat([training_data, signals])\n",
    "training_data= training_data.to_numpy()\n",
    "if pipeline  is None:\n",
    "    try:\n",
    "        if not reset:\n",
    "            pipeline= load(str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is loaded successfully')\n",
    "        else:\n",
    "            pipeline= proposed_pipline\n",
    "            pipeline= pipeline.fit(training_data)\n",
    "            dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "            print('preprocessor is saved successfully')\n",
    "            #print('explained_variance_ratio_= ', pipeline.steps[1][1].explained_variance_ratio_)\n",
    "    except FileNotFoundError as err:\n",
    "        pipeline= proposed_pipline\n",
    "        pipeline= pipeline.fit(training_data)\n",
    "        dump(pipeline, str(preprocessor_dir + preprocessor_name))\n",
    "        initial_type = [('float_input', FloatTensorType([None,  training_data.shape[1]]))]\n",
    "        onx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "        with open(str(preprocessor_dir + \"preprocessor.onnx\"), \"wb\") as f:\n",
    "            f.write(onx.SerializeToString())\n",
    "        print('preprocessor is saved successfully')\n",
    "columns= selected_Columns.copy()\n",
    "columns.append('t_welle')    \n",
    "for file in glob.glob('*.csv'):\n",
    "    if file == 'combined.csv':\n",
    "        continue\n",
    "    print('Current File: ', file)\n",
    "    df= pd.read_csv(file)\n",
    "    signals= df\n",
    "    signals= signals[selected_Columns].to_numpy()\n",
    "    signals= pipeline.transform(signals)\n",
    "    output= (df['t_welle'])\n",
    "    correlations= df[['DRZ','t_bett','t_schlitten','t_spindle', 't_motor','t_welle']].corr()\n",
    "    fig, ax= plt.subplots(figsize=(15,15))\n",
    "    sn.heatmap(correlations, annot=True, vmin=-1, vmax=1)\n",
    "    plt.show()\n",
    "    ######Visualize Data######\n",
    "    #create_histogram(df= df, name= file, features= columns)\n",
    "    #visualize_data(df= df, name= file, features= columns)\n",
    "    # temp_signal= pd.DataFrame(signals,columns=['p1'])\n",
    "    # temp_signal['t_welle']= output\n",
    "    #create_histogram(df= temp_signal, name= file, features= temp_signal.columns)\n",
    "    #visualize_data(df= temp_signal, name= file, features= temp_signal.columns)\n",
    "    ##########################\n",
    "    partitions, target= generateDataSource(signal_input= signals, input_columns= selected_Columns, output_length= 1,signal_output= output, window= window, shift= shift)\n",
    "    if results is None:\n",
    "        results= partitions\n",
    "        targets= target\n",
    "    else:\n",
    "        results= np.concatenate((results, partitions), axis= 0)\n",
    "        targets= np.concatenate((targets, target), axis= 0)\n",
    "    print('input: ', results.shape, ' output:', targets.shape)\n",
    "#results= np.reshape(results, (-1, window,))\n",
    "targets= np.reshape(targets, (-1,1))\n",
    "#print('input: ', results.shape, ' output:', targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Extract Data source from all signals\n",
    "# working_dir=  testdata_dir\n",
    "# os.chdir(working_dir)\n",
    "# test_results: np.ndarray= None\n",
    "# test_targets: np.ndarray= None\n",
    "# for file in glob.glob('*.csv'):\n",
    "#     if file == 'combined.csv':\n",
    "#         continue\n",
    "#     print('Current File: ', file)\n",
    "#     df= pd.read_csv(file)\n",
    "#     signals= df\n",
    "#     signals= signals[selected_Columns]\n",
    "#     signals= signals.to_numpy()\n",
    "#     signals= pipeline.transform(signals)\n",
    "#     #output= 1000000*(df['z_tcp_ok']).round(3)\n",
    "#     output= (df['z_tcp_ok']).round(5)\n",
    "#     partitions, target= generateDataSource(signal_input= signals, input_columns= selected_Columns, output_length= 1,signal_output= output, window= window, shift= shift)\n",
    "#     if test_results is None:\n",
    "#         test_results= partitions\n",
    "#         test_targets= target\n",
    "#     else:\n",
    "#         test_results= np.concatenate((test_results, partitions), axis= 0)\n",
    "#         test_targets= np.concatenate((test_targets, target), axis= 0)\n",
    "#     print('input: ', test_results.shape, ' output:', test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 50\n",
    "checkpoint= ModelCheckpoint(filepath= model_dir, monitor= 'val_loss', verbose=1,\n",
    "                                     save_best_only=True, save_weights_only= True, mode='min',save_freq='epoch',)\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "earlyStopping= EarlyStopping(monitor= 'val_loss', mode= 'min', patience=200,verbose=1)\n",
    "model= build_Model(X_train= results,y_train= targets)\n",
    "#model= build_Model_MLP(X_train= results,y_train= targets)\n",
    "try:\n",
    "    if not reset:\n",
    "        model.load_weights(filepath= model_dir)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print('Output shape ',model.output_shape)\n",
    "print(model.summary())\n",
    "summary= model.fit(x= results, y= targets,shuffle= False, batch_size= batch_size,  epochs= 1000, validation_split= 0.3,#validation_data=(test_results, test_targets),\n",
    "callbacks= [checkpoint, earlyStopping], verbose= 2, workers=32, use_multiprocessing= True)\n",
    "model.save(str(model_dir+'model.h5'), save_format='h5')\n",
    "input_signature= [tf.TensorSpec([None,results.shape[1], results.shape[2]],tf.float32, name= 'x')]\n",
    "onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "onnx.save_model(onnx_model, str(model_dir+'model.onnx'))\n",
    "print (\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Save Model as onnx from weights\n",
    "from keras.models import load_model\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "model= Sequential()\n",
    "model= load_model(filepath= str(model_dir+'model.h5'), compile= False)\n",
    "model.load_weights(model_dir)\n",
    "model.save(str(model_dir+'best_model.h5'), save_format='h5')\n",
    "input_signature= [tf.TensorSpec([None,results.shape[1], results.shape[2]],tf.float32, name= 'x')]\n",
    "onnx_model, _= tf2onnx.convert.from_keras(model= model, input_signature= input_signature, opset= 10)\n",
    "onnx.save_model(onnx_model, str(model_dir+'Best_model.onnx'))\n",
    "print (\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= summary.history['loss']\n",
    "val_loss= summary.history['val_loss']\n",
    "fig, ax= plt.subplots(figsize=(15,15))\n",
    "plt.title('Training Results')\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE (mm)')\n",
    "plt.plot(loss, label= 'Training Loss')\n",
    "plt.plot(val_loss, label= 'validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#from keras.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#model=  Sequential()\n",
    "#model= model.load_weights(model_dir,)#load_model(filepath= str(model_dir+'model.h5'), compile= False)\n",
    "#print(model.summary())\n",
    "# pred_train= model.predict(x_train)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_train,pred_train)), ' mae= ', np.mean(mean_absolute_error(y_train,pred_train)), ' mape= ', np.mean(mean_absolute_percentage_error(y_train,pred_train)))\n",
    "# pred_test= model.predict(x_test)\n",
    "# print ('mse= ',np.mean(mean_squared_error(y_test,pred_test)), ' mae= ', np.mean(mean_absolute_error(y_test,pred_test)), ' mape= ', np.mean(mean_absolute_percentage_error(y_test,pred_test)))\n",
    "pred= model.predict(results)\n",
    "print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred))\n",
    "fig, ax= plt.subplots(figsize=(15,15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid()\n",
    "# plt.plot(x_steps, upperline, label= 'Upper Bound', color='green')\n",
    "plt.scatter(targets,pred, label= 'predictions_train', color= 'blue')\n",
    "# plt.plot(x_steps, lowerline, label= 'lower Bound', color='green')\n",
    "# plt.plot(x_steps, x_steps, label= 'Optimal line')\n",
    "# plt.plot(x_steps, lowerline_actual, label= 'Actual lower Bound', color='red')\n",
    "# plt.plot(x_steps, upperline_actual, label= 'Actual Upper Bound', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame({'targets': np.reshape(targets,-1), 'pred': np.reshape(pred,-1)})\n",
    "fig= ex.line(df,y=['targets', 'pred'])\n",
    "#fig.update_layout(height=1200, width=1200, title_text=\"Laser-Data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff= abs(df['targets'] - df['pred'])\n",
    "fig= ex.line(y= diff)\n",
    "#fig.update_layout(height=1200, width=1200, title_text=\"Laser-Data\")\n",
    "fig.show()\n",
    "print ('Max= ', diff.max(),' Min= ', diff.min(), 'mean= ', diff.mean() )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a58a25343f99ae6e283c189afd7abc602dec6c63d356cacabf499812ae322086"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
