{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, KernelPCA,IncrementalPCA,SparsePCA\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import onnx\n",
    "import tf2onnx\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version= 'v0.0.7'\n",
    "workspace= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\\"\n",
    "validation_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\validation_data\"\n",
    "assests_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\assets\\\\'+version+'\\\\'\n",
    "if not os.path.exists(assests_dir):\n",
    "    print('Directory: {dir} is not exist! Creating Directory'.format(dir= assests_dir))\n",
    "    os.makedirs(name= assests_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(validation_data)\n",
    "df:pd.DataFrame= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    df_file = pd.read_csv(file)\n",
    "    df_file['file_name']= file\n",
    "    print('Current File: ', file)\n",
    "    if df is None:\n",
    "        df= df_file\n",
    "    else:\n",
    "        df= pd.concat([df, df_file], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the cooling power\n",
    "#df['kuehleistung']=(df['Ruecklauftemperatur']-df['Vorlauftempertatur'])*df['Volumenstrom_Kuehlung'] * 1.16 * 60/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filtering motor temperature\n",
    "df= df[df['T_MOTOR'] != 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing signals for each expriment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "       'T_BETT', 'magnet_temperature']\n",
    "selected_columns_with_units= ['Drehzahl (RPM)', 'Temp_Klemmung (C°)','Temp_Lager (C°)','Temp_Motor (C°)','T_BETT (C°)','Temperatur_Magnet (C°)']\n",
    "experiments= list(df['file_name'].unique())\n",
    "for experiment in experiments:\n",
    "    current_df= df[df['file_name'] == experiment]\n",
    "    fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns_with_units, vertical_spacing=0.02)\n",
    "    for j in range(len(selected_columns)):\n",
    "        fig.add_trace(go.Scatter(x= current_df['date'],y= current_df[selected_columns[j]], name=selected_columns[j], mode= 'lines'), row= j+1, col= 1)\n",
    "        #fig.update_yaxes(title_text= selected_columns[j], row= j+1, col= 1)\n",
    "    fig.update_xaxes(title_text= 'Zeit ',row= len(selected_columns), col= 1)\n",
    "    fig.update_layout(height=1200, width=1200, title_text='MillTurn-Motor {experiment}'.format(experiment= experiment))\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Distribution of Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= df[df['file_name']=='S1_500_M3_k.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= df.hist(figsize= (15,15),bins=50,xlabelsize=10, ylabelsize= 10)\n",
    "fig= ax[0][0].get_figure()\n",
    "plt.xlabel('values')\n",
    "plt.ylabel('counts')\n",
    "plt.savefig(assests_dir + 'row_data_hist.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= [ 'DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR', 'T_BETT',  'magnet_temperature']\n",
    "# df['T_MOTOR']= df['T_MOTOR'] -df['T_BETT']\n",
    "# df['T_LAGER']= df['T_LAGER'] -df['T_BETT']\n",
    "correlations= df[neededColumns].corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 15, 'color': 'black'})\n",
    "plt.savefig(assests_dir +'Korreation_Heatmap.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signalgättung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= [ 'T_MOTOR', 'T_LAGER', 'T_KLEMMUNG', 'magnet_temperature']\n",
    "correlations= df[neededColumns].corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "plt.savefig(assests_dir +'Korreation_Heatmap_kleinerform.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neededColumns= [ 'Feldstrom', 'Strom_Betrag',\n",
    "#        'Querspannung', 'Temp_Klemmung', 'Temp_Lager',\n",
    "#        'Temp_Motor', 'kuehleistung']#['Temp_Lager', 'Temp_Motor']\n",
    "neededColumns= [ 'T_MOTOR', 'T_LAGER', 'T_KLEMMUNG']\n",
    "target= df['magnet_temperature'].reset_index()\n",
    "reduced_data= df[neededColumns]\n",
    "reduced_data.reset_index(inplace= True)\n",
    "#print(reduced_data.info())\n",
    "#reduced_data= reduced_data.to_numpy(dtype= np.float64)\n",
    "pca1=PCA(n_components=3)\n",
    "transformed_data= pca1.fit_transform(reduced_data)\n",
    "new_df= pd.DataFrame(transformed_data,columns=['PC1', 'PC2', 'PC3'])\n",
    "new_df['Temperatur_Magnet']= target['magnet_temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= new_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "plt.savefig(workspace+'pca_Korrelation_heatmap.jpg' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assests_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\assets\\\\'\n",
    "preprocessor_name= 'preprocessor.p'\n",
    "window=20\n",
    "shift=1\n",
    "sample_rate=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(signals: np.ndarray):\n",
    "    return np.round(signals,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### window  represents time period by each entry in the buffer\n",
    "### Shift represents the jump from value to next one in the buffer\n",
    "### sampling rate \n",
    "def generateDataSource(signal_input=None, input_columns: list = [], output_length: int = 1, signal_output=None, window=1, shift=1, sample_rate=1):\n",
    "    #subsequence_len= (window -1) *shift + 1\n",
    "    subsequence_len= (window) *shift\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    num_samples = 1 + int((Signal_Length - subsequence_len) / sample_rate)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i] = np.asarray([signal_input[i*sample_rate + j * shift] for j in range(0,window)])\n",
    "        y[i] = signal_output[i*sample_rate + (window-1) * shift :i*sample_rate+ (window-1) * shift + output_length]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace+'training\\\\')\n",
    "df:pd.DataFrame= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    df_file = pd.read_csv(file)\n",
    "    df_file['file_name']= file\n",
    "    print('Current File: ', file)\n",
    "    if df is None:\n",
    "        df= df_file\n",
    "    else:\n",
    "        df= pd.concat([df, df_file], axis=0)\n",
    "###Filtering motor Temperature\n",
    "df= df[df['T_MOTOR'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_t_bett(data: np.ndarray):\n",
    "    result = data[:, 1:]\n",
    "    result[:, 0] = result[:, 0] - data[:, 0]\n",
    "    result[:, 1] = result[:, 1] - data[:, 0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler2 = StandardScaler(with_mean=True, with_std=True)\n",
    "pca= PCA(n_components= 3)\n",
    "pipeline = Pipeline(steps=[ ('stdscaler', scaler),('pca', pca),('rounding2', FunctionTransformer(rounding))])#('t_bett_removal', FunctionTransformer(remove_t_bett)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= ['T_LAGER', 'T_MOTOR', 'T_KLEMMUNG']\n",
    "target= df['magnet_temperature'].reset_index().round(decimals=2)\n",
    "reduced_data= df[neededColumns]\n",
    "reduced_data.reset_index(inplace= True)\n",
    "reduced_data= reduced_data[neededColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_t_bett(reduced_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data= pipeline.fit_transform(reduced_data)\n",
    "dump(pipeline, str(assests_dir + preprocessor_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.steps[1][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df= pd.DataFrame(transformed_data,columns=['PC1', 'PC2', 'PC3'])\n",
    "# new_df['Temperatur_Magnet']= target['Temperatur_Magnet']\n",
    "# correlations= new_df.corr()\n",
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "# ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "# plt.savefig(assests_dir +'best_pca_Korrelation_heatmap.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace+'training\\\\')\n",
    "df:pd.DataFrame= None\n",
    "x= None\n",
    "y= None\n",
    "batch_size= 1000000000\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file['magnet_temperature']\n",
    "    if batch_size > len(df_file):\n",
    "        batch_size= len(df_file)\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    if x is None:\n",
    "        x= partitions\n",
    "        y= target\n",
    "    else:\n",
    "        x= np.concatenate((x, partitions), axis= 0)\n",
    "        y= np.concatenate((y, target), axis= 0)\n",
    "    print('X: ', x.shape, ' y:', y.shape)\n",
    "    print('batch_size= ', batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace+'testing\\\\')\n",
    "df:pd.DataFrame= None\n",
    "xtest= None\n",
    "ytest= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file['magnet_temperature']\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    if xtest is None:\n",
    "        xtest= partitions\n",
    "        ytest= target\n",
    "    else:\n",
    "        xtest= np.concatenate((xtest, partitions), axis= 0)\n",
    "        ytest= np.concatenate((ytest, target), axis= 0)\n",
    "    print('X_test: ', xtest.shape, ' y_test:', ytest.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import selu\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Activation, Dense\n",
    "from keras.losses import LossFunctionWrapper, mean_absolute_error, mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import L1L2\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(\n",
    "                    tf.reduce_max(tf.abs((y_true - y_pred))),\n",
    "                    mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "def smoothed_max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    a=0.5\n",
    "    diffs= tf.abs((y_true - y_pred))\n",
    "    muls= tf.multiply(a, diffs)\n",
    "    exps= tf.exp(muls)\n",
    "    weighted_muls= tf.multiply(diffs, exps)\n",
    "    nominater= tf.reduce_sum(weighted_muls)\n",
    "    denominator= tf.reduce_sum(exps)\n",
    "    boltzmann_operator= nominater/denominator\n",
    "    error = tf.add(boltzmann_operator, mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "special_loss = LossFunctionWrapper(smoothed_max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Model_LSTM(input_shape, activation_Function = selu,\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.00,\n",
    "    l2_v = 0.00,\n",
    "    structure= [],\n",
    "    optimizer= 'Nadam'):\n",
    "    #structure=  [n_units for i in range(0,n_hidden_layers)]#[50,50,40,40,30,30,20]##   \n",
    "    unroll = False\n",
    "    kernal_init = 'he_normal'#RandomUniform()  # \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    for i in range(1,len(structure)+1):\n",
    "        layer_size= structure[i-1]\n",
    "        if i == len(structure):\n",
    "            model.add(LSTM(layer_size,stateful= False,return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        else:\n",
    "            model.add(LSTM(layer_size,stateful= False, return_sequences=True,unroll=unroll, kernel_initializer= kernal_init,dropout=dropout,  kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation_Function))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(PReLU())\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss= special_loss, metrics=[ max_loss])#'Adagrad'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(old_value, old_min, old_max, new_max, new_min):\n",
    "    new_value= ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "    old_min = 0\n",
    "    old_max = 0\n",
    "    y_train_rounded = np.round(y_train,decimals=3)\n",
    "    N = len(y_train_rounded)\n",
    "    y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "    weights = np.ones(shape=(N,),dtype= np.float32)\n",
    "    for i in range(0, N):\n",
    "        current_value = y_train_rounded[i]\n",
    "        occurences = np.count_nonzero(y_train_rounded == current_value)\n",
    "        if occurences <= occurance_threshold:\n",
    "             weights[i]=  N/(occurences + 1 )#N/(occurences + y_train_rounded[i])#N/occurences # 0\n",
    "        else:\n",
    "            weights[i] =  N/(occurences + 1)#(8 if y_train_rounded[i] < 20 else 1)\n",
    "    old_min= weights.min()\n",
    "    old_max= weights.max()\n",
    "    new_min = 1.2\n",
    "    new_max = 1.5\n",
    "    #weights_scaled= np.apply_along_axis(map, 1, weights)\n",
    "    weights_scaled= np.asanyarray([ map(weights[i], old_min, old_max, new_max, new_min) for i in range(0, weights.shape[0])]).reshape(weights.shape)\n",
    "    #weights = weights - weights.min() + 1\n",
    "    #weights = (weights - weights.min())/(weights.max() - weights.min())\n",
    "    fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=weights_scaled,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 1, col= 1)\n",
    "    fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 2, col= 1)\n",
    "    fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "    fig.show()\n",
    "\n",
    "    return weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=str(assests_dir+'best_model.h5'), monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain, xtest, ytrain, ytest = train_test_split(xtest,ytest,shuffle= False, test_size=0.1, random_state=49)\n",
    "xtrain= x\n",
    "ytrain= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = calculate_weights(y_train=ytrain, occurance_threshold= 5)\n",
    "model = build_Model_LSTM( (xtrain.shape[1], xtrain.shape[2]),activation_Function = 'elu',\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.001,\n",
    "    l2_v = 0.001,\n",
    "    structure=[5],\n",
    "    optimizer= 'Nadam')\n",
    "reset= True##############################################################################################################\n",
    "try:\n",
    "    if not reset:\n",
    "        #model.set_weights(model_w)\n",
    "        #model.load_weights(filepath=weights_path)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print(model.output_shape)\n",
    "print(model.summary())\n",
    "summary = model.fit(x=xtrain, y=ytrain, shuffle=False, batch_size= int(batch_size/2),  epochs=15000, validation_data=(xtest, ytest),  #int(batch_size * 0.5)\n",
    "                    callbacks=[earlyStopping], verbose=2, workers=32, use_multiprocessing=True)#, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(assests_dir+'model.h5'), save_format='h5')\n",
    "input_signature = [tf.TensorSpec([None, xtrain.shape[1], xtrain.shape[2]], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model=model, input_signature=input_signature, opset=10)\n",
    "onnx.save_model(onnx_model, str(assests_dir+'model.onnx'))\n",
    "print(\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(filepath=weights_path)\n",
    "metric_loss = summary.history['loss']\n",
    "metric_val_loss = summary.history['val_loss']\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "                    print_grid=True,  vertical_spacing=0.02)\n",
    "fig.add_trace(go.Line(y=metric_loss, name='Training Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=metric_val_loss , name='Validation Loss'), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=mae_loss, name='Training {}'.format('mae')), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=val_mae_loss, name='Validation {}'.format('mae')), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_layout(height=900, width=900, title_text='Training Curve')\n",
    "fig.show()\n",
    "pio.write_image(fig, str(assests_dir+'trainingCurve.jpg'), format='jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_steps = np.linspace(0, 130, 10)\n",
    "print(model.summary())\n",
    "pred = model.predict(xtrain)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#org_targets= post_processer.inverse_transform(targets_post)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True (C°)')\n",
    "plt.ylabel('Predicted (C°)')\n",
    "pred.reshape((-1,))\n",
    "plt.scatter(ytrain, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(assests_dir+'training_results.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_steps = np.linspace(0, 130, 10)\n",
    "print(model.summary())\n",
    "pred = model.predict(xtest)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#org_targets= post_processer.inverse_transform(targets_post)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True (C°)')\n",
    "plt.ylabel('Predicted (C°)')\n",
    "pred.reshape((-1,))\n",
    "print(ytest.shape)\n",
    "plt.scatter(ytest, pred, label='predictions_Testing', color='blue')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line', color= 'red')\n",
    "plt.legend()\n",
    "plt.savefig(str(assests_dir+'testing_results.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model= load_model(str(assests_dir+'model.h5'), compile= False)\n",
    "pipeline= load(str(assests_dir + preprocessor_name))\n",
    "print(model.input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(validation_data)\n",
    "df:pd.DataFrame= None\n",
    "x= None\n",
    "ytest= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file['magnet_temperature']\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    pred= model.predict(partitions)\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=target.flatten(), name='Magent_temp_True'), row=1, col=1)\n",
    "    fig.add_trace(go.Line(y=pred.flatten(), name='Magent_temp_Pred'), row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Magnet Temp (C°)', row=1, col=1)\n",
    "    fig.add_trace(go.Line(y=target.flatten() - pred.flatten(), name='Prediction Error (True -Predicted)'), row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Prediction Error (C°)', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit 1 = 2 Sek', row=2, col=1)\n",
    "    fig.update_layout(height=900, width=900, title_text='Vorhersage der Magnet-Temp {file}'.format(file = file))\n",
    "    fig.show()\n",
    "    pio.write_image(fig, str(assests_dir+'{file}.jpg'.format(file= file)), format='jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
