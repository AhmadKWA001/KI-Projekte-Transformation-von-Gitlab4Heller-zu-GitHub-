{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from joblib import dump, load\n",
    " \n",
    "import tensorflow \n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, GRU, LSTM\n",
    "from keras.activations import relu, tanh, linear\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import json\n",
    "#from interpolate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpolate import interpolate\n",
    "from plot import plot\n",
    "from readIn import readIn\n",
    "from plotHeatmap import plotHeatmap\n",
    "from removeT_bett import removeT_bett\n",
    "from trainDTR import trainDTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "do_interpolation = False\n",
    "scatter_mode = \"lines\"\n",
    "plot_ipo = True\n",
    "\n",
    "#Which models to use?\n",
    "useDTR = False\n",
    "use_sk_NN = False\n",
    "use_keras_NN = False\n",
    "use_keras_LSTM = True\n",
    "\n",
    "# Nr of timesteps to consider for LSTM\n",
    "timesteps = 10\n",
    "\n",
    "# How to split the data into training, validation and test set?\n",
    "split_evenly = True\n",
    "split_by_coolant = False\n",
    "split_by_profile = False\n",
    "\n",
    "# Use pipeline?\n",
    "usePipeline = True\n",
    "\n",
    "# Plot?\n",
    "plotNN = False\n",
    "plotDTR = False\n",
    "\n",
    "# Directories\n",
    "#workdir = \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\row_files\\\\combines\\\\smoothed\\\\With_cooling_feature_for_Model_training_5s\\\\validate_Data\"\n",
    "workdir = \"X:\\\\KI Praktikum\\\\validate_Data\"\n",
    "#goaldir = \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Temperaturkompensation\\\\2021_Spindelwachstumskompensation_KI_HSU_SC63\\\\Messungen DC100 H5000 M57002\\\\row_files\\\\combines\\\\smoothed\\\\With_cooling_feature_for_Model_training_5s\\\\interpolated_Data_time\"\n",
    "goaldir = \"X:\\\\KI Praktikum\\\\validate_Data_time_ipo\"\n",
    "dir_ipo = \"X:\\KI Praktikum\\\\validate_Data\\ipo_time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in several csv files and concatenate one large dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged = readIn(workdir, ipo=True, safeSingleCsv =False)\n",
    "df_merged = readIn(dir_ipo, ipo=False, safeSingleCsv = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.2\"]) #, \"Unnamed: 0.3\"\n",
    "df_merged.to_csv(\"X:\\\\KI Praktikum\\\\validate_Data\\\\merged_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bett_abgezogen = True\n",
    "z_interpoliert = True\n",
    "show=False\n",
    "\n",
    "plt = plotHeatmap(df_merged, show=show, t_bett_abgezogen=t_bett_abgezogen, z_interpoliert=z_interpoliert)\n",
    "if plt != None:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show distribution of the data using a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = False\n",
    "if hist:\n",
    "    df_merged.hist(figsize=(15,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the entire dataframe into test, training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = df_merged[\"filename\"].unique()\n",
    "#print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_evenly:\n",
    "    validation_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch10_12_2022_trocken_iso_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch22_082022_trocken_iso_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_11_2022_M8_iso_shifted_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch17_01_2023_M127_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_01_2023_M8_M127_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_092022_trocken_prof1_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch22_092022_trocken_prof3_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_01_2023_M121_iso__time_ipo.csv'\n",
    "                        ]\n",
    "    test_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_11_2022_trocken_iso_time_ipo.csv',\n",
    "                'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch12_11_2022_M8_iso__time_ipo.csv',\n",
    "                'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_11_2022_M8_prof2_time_ipo.csv',\n",
    "                'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch21_092022_trocken_prof2_5s_time_ipo.csv',\n",
    "                'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_092022_trocken_prof2_5s_time_ipo.csv']\n",
    "\n",
    "if split_by_coolant:\n",
    "    validation_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch11_12_2022_M8_iso_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch12_11_2022_M127_iso_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch12_11_2022_M8_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch13_01_2023_M127_iso_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch15_11_2022_M7_iso_shifted_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch17_01_2023_M121_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch17_01_2023_M127_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_01_2023_M121_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_01_2023_M8_M127_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_082022_M8_iso_5s_time_ipo.csv']\n",
    "    test_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_11_2022_M8_iso_shifted_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_01_2023_M8_M121_isoteil_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_11_2022_M8_prof2_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_11_2022_M8_prof4_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch2023_01_12_14_10_M8_iso__time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch2023_01_13_09_30_M7_iso__time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch2023_01_17_17_04_M127_iso__time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch2023_01_18_06_00_M121_iso__time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch20_11_2022_M8_prof3_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_11_2022_M8_iso_time_ipo.csv']\n",
    "if split_by_profile:\n",
    "    validation_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_11_2022_M8_prof2_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch19_11_2022_M8_prof4_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch20_092022_trocken_prof1_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch20_11_2022_M8_prof3_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch21_092022_trocken_prof1_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch21_092022_trocken_prof2_5s_time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch18_01_2023_M8_M127_iso__time_ipo.csv',\n",
    "                        'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_11_2022_trocken_iso_time_ipo.csv']\n",
    "    test_files = ['X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch22_092022_trocken_prof3_5s_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_092022_trocken_prof1_5s_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch24_092022_trocken_prof2_5s_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch25_092022_trocken_prof3_5s_time_ipo.csv',\n",
    "                    'X:\\\\KI Praktikum\\\\validate_Data\\\\ipo_time\\\\filtered_interpolated_Versuch17_01_2023_M127_iso__time_ipo.csv']\n",
    "    \n",
    "df_train = df_merged[df_merged[\"filename\"].isin(validation_files) == False]\n",
    "#print(\"w/o validation \" + str(df_train.info()))\n",
    "df_train = df_train[df_train[\"filename\"].isin(test_files) == False]\n",
    "df_validation = df_merged[df_merged[\"filename\"].isin(validation_files) == True]\n",
    "df_test = df_merged[df_merged[\"filename\"].isin(test_files) == True]\n",
    "#print(\"w/o test \" + str(df_train.info()))\n",
    "\n",
    "df_train[\"welle_z_ipo_time\"] = 1000*df_train[\"welle_z_ipo_time\"]\n",
    "df_validation[\"welle_z_ipo_time\"] = 1000*df_validation[\"welle_z_ipo_time\"]\n",
    "df_test[\"welle_z_ipo_time\"] = 1000*df_test['welle_z_ipo_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a preprocessing pipeline and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"welle_z_ipo_time\"]\n",
    "parameters = [\"t_bett\", \"t_motor\", \"t_spindle\", \"M8\", \"M121\", \"M127\", \"M7\"]\n",
    "x_train = df_train[parameters].to_numpy()\n",
    "\n",
    "y_validation = df_validation[\"welle_z_ipo_time\"]\n",
    "x_validation = df_validation[parameters].to_numpy()\n",
    "\n",
    "y_test = df_test[\"welle_z_ipo_time\"]\n",
    "x_test = df_test[parameters].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_steps = [(\"remove_t_bett\", FunctionTransformer(removeT_bett)), (\"standardize_values\", StandardScaler()), (\"pca\", PCA(6))]\n",
    "pipeline = Pipeline(steps=seq_steps)\n",
    "\n",
    "#print(np.argwhere(np.isnan(x_train)))\n",
    "if usePipeline:\n",
    "    x_transform = pipeline.fit_transform(x_train)\n",
    "    x_validation = pipeline.transform(x_validation)\n",
    "    x_test = pipeline.transform(x_test)\n",
    "else:\n",
    "    x_transform = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname = \"X:\\\\KI Praktikum\\\\pipeline.p\"\n",
    "dump(pipeline, pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if usePipeline:\n",
    "    newpipeline = load(pathname)\n",
    "    newpipeline.steps[2][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useDTR:\n",
    "    score_dtr, figTrain, figValidation = trainDTR(x_transform, y_train, x_validation, y_validation, scatter_mode)\n",
    "    if plotDTR:\n",
    "        figTrain.show()\n",
    "        figValidation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sk_NN:\n",
    "    use_train_and_validation=True\n",
    "\n",
    "    if use_train_and_validation:\n",
    "        x_cv = np.concatenate((x_transform, x_validation))\n",
    "        y_cv = np.concatenate((y_train, y_validation))\n",
    "\n",
    "    else:\n",
    "        x_cv = x_transform\n",
    "        y_cv = y_train\n",
    "    nn = MLPRegressor(solver= 'adam', n_iter_no_change = 15, random_state=0, verbose=True)\n",
    "\n",
    "    parameters = {'hidden_layer_sizes': [(5, 5, 5, 5, 5, 5), (10, 10), (3, 3, 3, 3), (15, 15)],\n",
    "                'activation': ['tanh', 'relu'],\n",
    "                'max_iter': [500, 1000, 5000, 6000, 7000]}\n",
    "\n",
    "    randCV = RandomizedSearchCV(estimator=nn, param_distributions=parameters, cv =5, n_iter = 4, n_jobs=-1)\n",
    "    randCV.fit(x_cv, y_cv)\n",
    "\n",
    "    # print(randCV.best_estimator_)\n",
    "    # print('score: ', randCV.score)\n",
    "    # print('best parameters: ', randCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sk_NN:\n",
    "    randCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sk_NN:\n",
    "    nn = randCV.best_estimator_\n",
    "    nn.fit(x_transform, y_train)\n",
    "    score_nn = nn.score(x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sk_NN and useDTR:\n",
    "      print(\"Score Decision Tree Regressor: \", score_dtr, \"\\n\",\n",
    "      \"Score Neural Network: \",  score_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotNN:\n",
    "    nn_pred_train = nn.predict(x_transform)\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter( y= y_train[100000:120000], name= 'y-values', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter( y= nn_pred_train[100000:120000], name= 'y-predicted', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'y-train', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Predictions for training data neural network\")\n",
    "    #fig.write_image(\"X:\\\\KI Praktikum\\\\validate_Data\\\\Bilder\\\\Ungleiche_Datenverteilung\\\\NN_training_coolant.jpeg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotNN:\n",
    "    nn_pred_validation = nn.predict(x_validation)\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter( y= y_validation, name= 'y-values', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter( y= nn_pred_validation, name= 'y-predicted', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'y-train', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Predictions for validation data neural network\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto') \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=3, activation = relu))\n",
    "    model.add(Dense(units=2, activation = relu))\n",
    "    model.add(Dense(units=1, activation = linear))\n",
    "\n",
    "    model.compile(optimizer = \"Nadam\", loss = ks.losses.mean_absolute_error)\n",
    "\n",
    "    summary = model.fit(x_transform, y_train, verbose=1, batch_size = 200, epochs=100, validation_data = (x_validation, y_validation), callbacks = [earlyStop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\n",
    "    train_loss = summary.history[\"loss\"]\n",
    "    validation_loss = summary.history[\"val_loss\"]\n",
    "\n",
    "    model_pred_validation = model.predict(x_validation)\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter( y= train_loss, name= 'training loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter( y= validation_loss, name= 'validation loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'loss', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Training loss curve\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"X:\\\\KI Praktikum\\\\validate_Data\\\\models\\\\keras_NN_321_Nadam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_train_and_validation=True\n",
    "\n",
    "if use_train_and_validation:\n",
    "    x_cv = np.concatenate((x_transform, x_validation))\n",
    "    y_cv = np.concatenate((y_train, y_validation))\n",
    "\n",
    "else:\n",
    "    x_cv = x_transform\n",
    "    y_cv = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model(layers=[5, 5, 5], dropout=0, learnRate=0.001, activation = \"relu\", optimizer=\"Nadam\", first_activ = \"relu\"):\n",
    "    model = Sequential()\n",
    "    first = True\n",
    "    for layer in layers:\n",
    "        if first:\n",
    "            model.add(Dense(layer, activation=first_activ, input_dim=6))\n",
    "            first = False\n",
    "        else:\n",
    "            model.add(Dense(layer, activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=ks.losses.mean_absolute_error)\n",
    "    # return compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\n",
    "    model = KerasRegressor(build_fn=get_mlp_model, verbose=1, layers=[5, 5, 5], dropout=0, activation = \"relu\", optimizer=\"Nadam\", first_activ = \"relu\")     \n",
    "                    #, first_rec_activ=\"sigmoid\", rec_activ=\"sigmoid\"\n",
    "    model.fit(x_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\n",
    "    grid = dict( \n",
    "        layers = [ [500, 500, 300], [100, 150, 50], \n",
    "                [700, 500, 300], [700, 700, 500]],    #[500, 400, 200], [700, 700, 500, 300], [700, 700, 500, 100],[25, 25, 25, 20, 20, 20, 10, 10],  [75, 75, 50, 50, 50, 40, 40, 20]\n",
    "        dropout = [0, 0.1, 0.3, 0.2],\n",
    "        first_activ = [\"relu\", \"tanh\"],\n",
    "        activation = [\"relu\", \"tanh\"],\n",
    "        #first_rec_activ = [\"sigmoid\", \"tanh\"],\n",
    "        #rec_activ = [\"sigmoid\", \"tanh\"],\n",
    "        optimizer = [\"Nadam\", \"Adam\"]\n",
    "    )\n",
    "\n",
    "    searcher = RandomizedSearchCV(estimator=model, n_jobs=1, cv=3,\n",
    "        param_distributions=grid, scoring='neg_mean_absolute_error', n_iter = 5)\n",
    "    searchResults = searcher.fit(x_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\t\n",
    "\tbestScore = searchResults.best_score_\n",
    "\tbestParams = searchResults.best_params_\n",
    "\tprint(\"[INFO] best score is {:.2f} using {}\".format(bestScore,\n",
    "\tbestParams))\n",
    "\n",
    "\tbestModel = searchResults.best_estimator_\n",
    "\tbestModel.fit(x_transform, y_train)\n",
    "\tscore = bestModel.score(x_validation, y_validation)\n",
    "\tprint(\"score: {:.2f}%\".format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_NN:\n",
    "    best_pred = bestModel.predict(x_validation)\n",
    "    error_avg = np.mean(abs(y_validation - best_pred))\n",
    "    error_max = max(abs(y_validation - best_pred))\n",
    "    print(\"max: \", error_max)\n",
    "    print(\"avg: \", error_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tensorflow.compat.v1.Session(config=tensorflow.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data for the training of a LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_transform.shape)\n",
    "print(df_train.shape)\n",
    "print(len(df_train[\"filename\"].unique()))\n",
    "print(\"longest sample: \", max(df_train[\"filename\"].value_counts()))\n",
    "print(\"shortest sample: \", min(df_train[\"filename\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr2d_to_arr3d(x_2d, y, window, stride):\n",
    "    \n",
    "    x_2d = x_2d[:-1, :]\n",
    "    y = np.roll(y, -1, axis=0)\n",
    "    y = y[window-1:-1]\n",
    "\n",
    "    isample = window-1\n",
    "    x_3d = np.zeros((1, window, x_2d.shape[1]))\n",
    "    # for each sample in the data frame create the corresponding batch consisting of the earlier samples\n",
    "    # loop for each sample\n",
    "    while isample < x_2d.shape[0]:\n",
    "        i = 0\n",
    "        arr_sample =np.zeros((1, window, x_2d.shape[1]))\n",
    "        # loop to create frame of previous and current samples\n",
    "        while i > -window:\n",
    "            singleSample = x_2d[isample+i, :] \n",
    "            arr_sample[0, window-1+i, :] = singleSample\n",
    "            # print(\"arr_sample: \", arr_sample)\n",
    "            i= i-1\n",
    "        \n",
    "        if isample == window-1:\n",
    "            x_3d = arr_sample\n",
    "        else:\n",
    "            x_3d = np.concatenate((x_3d, arr_sample), axis = 0)\n",
    "\n",
    "        isample += stride\n",
    "    return x_3d, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateDataSource() -> 3D array von den Daten\n",
    "### window  represents time period by each entry in the buffer\n",
    "### Shift represents the jump from value to next one in the buffer\n",
    "### sampling rate\n",
    "\n",
    "def generateDataSource(signal_input=None, input_columns: list = [], output_length: int = 1, signal_output=None, window=1, shift=1, sample_rate=1):\n",
    "    #subsequence_len= (window -1) *shift + 1\n",
    "    subsequence_len= (window) *shift\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    num_samples = 1 + int((Signal_Length - subsequence_len) / sample_rate)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i] = np.asarray([signal_input[i*sample_rate + j * shift] for j in range(0,window)])\n",
    "        y[i] = signal_output[i*sample_rate + (window-1) * shift :i*sample_rate+ (window-1) * shift + output_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 3\n",
    "# stride = 1\n",
    "# x = np.array([[0, 1, 2, 3, 4],\n",
    "#      [5, 6, 7, 8, 9],\n",
    "#      [10, 11, 12, 13, 14],\n",
    "#      [15, 16, 17, 18, 19],\n",
    "#      [20, 21, 22, 23, 24],\n",
    "#      [25, 26, 27, 28, 29],\n",
    "#      [30, 31, 32, 33, 34],\n",
    "#      [35, 36, 37, 38, 39]])\n",
    "# y = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# x_3d, y = arr2d_to_arr3d(x, y, window, stride)\n",
    "# print(x_3d, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_3d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using my code\n",
    "# window = 10\n",
    "# stride = 1\n",
    "\n",
    "# if use_keras_LSTM:\n",
    "#     x_cv = np.concatenate((x_transform, x_validation))\n",
    "#     y_cv = np.concatenate((y_train, y_validation))\n",
    "    \n",
    "#     x_cv, y_cv = arr2d_to_arr3d(x_cv, y_cv, 10, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D arrays mit verschiedenen Window-sizes als json abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mustafas function\n",
    "def SaveJsonWindows(filenames, window):\n",
    "    i = 0\n",
    "    for w in windows:\n",
    "        x_cv, y_cv = generateDataSource(signal_input = np.concatenate((x_transform, x_validation)),\n",
    "                                        signal_output=np.concatenate((y_train, y_validation)), \n",
    "                                        window = w, shift=1, sample_rate=1)\n",
    "        out_x = open(filenames[i][0], mode=\"x\")\n",
    "        json.dump(x_cv.tolist(), out_x)\n",
    "        out_x.close()\n",
    "        out_y = open(filenames[i][1], mode=\"x\")\n",
    "        json.dump(y_cv.tolist(), out_y)\n",
    "        out_y.close()\n",
    "        i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json File auslesen und als 3D Array verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenJsontoArr(path):\n",
    "    file = open(path)           #\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_x_cv.json\"\n",
    "    x_3d = json.load(file)\n",
    "    file.close()\n",
    "    x_3d = np.asarray(x_3d)\n",
    "    return x_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_x = open(\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_x_cv.json\", mode=\"x\")\n",
    "json.dump(x_cv.tolist(), out_x)\n",
    "out_x.close()\n",
    "out_y = open(\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_y_cv.json\", mode=\"x\")\n",
    "json.dump(y_cv.tolist(), out_y)\n",
    "out_y.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Modell definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(layers=[5, 5, 5], dropout=0, activation = \"relu\", input_shape = (1, 1)):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        if i == len(layers)-1:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=False))\n",
    "        else:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=True))\n",
    "        #model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        loss=ks.losses.mean_absolute_error,\n",
    "        optimizer = \"Nadam\")\n",
    "    # return compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Code Mustafa\n",
    "# def build_Model_LSTM(input_shape, activation_Function = selu,\n",
    "#     dropout = 0.0,\n",
    "#     l1_v = 0.00,\n",
    "#     l2_v = 0.00,\n",
    "#     structure= [],\n",
    "#     optimizer= 'Nadam'):\n",
    "    \n",
    "#     #structure=  [n_units for i in range(0,n_hidden_layers)]#[50,50,40,40,30,30,20]##  \n",
    "#     unroll = False\n",
    "#     kernal_init = RandomNormalV2()#'he_normal'#RandomUniform()  #\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=input_shape))\n",
    "#     # model.add(Dense(50, activation=activation_Function))\n",
    "#     model.add(Dense(20, activation=activation_Function))\n",
    "#     model.add(Dense(10, activation=activation_Function))\n",
    "#     for i in range(1,len(structure)+1):\n",
    "#         layer_size= structure[i-1]\n",
    "#         if i == len(structure):\n",
    "#             model.add(LSTM(layer_size,stateful= False,return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "#         else:\n",
    "#             model.add(LSTM(layer_size,stateful= False, return_sequences=True,unroll=unroll, kernel_initializer= kernal_init,dropout=dropout,  kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "#         #model.add(BatchNormalization())\n",
    "#         model.add(Activation(activation=activation_Function))\n",
    "#         #model.add(BatchNormalization())\n",
    "#         #model.add(PReLU())\n",
    "#     model.add(Dense(10, activation=activation_Function))\n",
    "#     model.add(Dense(5, activation=activation_Function))\n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "#     model.compile(optimizer=optimizer, loss= mean_squared_error, metrics=[ max_loss])#'Adagrad'\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # x_cv = np.concatenate((x_train_3d, x_validation_3d), 0)\n",
    "    # y_cv = np.concatenate((y_train, y_validation), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmmodel = get_lstm_model(layers=[5, 5, 5], dropout=0, activation = \"relu\",      \n",
    "                    input_shape = (10,6))\n",
    "x_lstm_train, x_lstm_test, y_lstm_train, y_lstm_test = train_test_split(x_cv, y_cv, shuffle = False, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = lstmmodel.fit(x_lstm_train, y_lstm_train, batch_size=500, workers = 10, use_multiprocessing=True, validation_data=(x_lstm_test, y_lstm_test), epochs = 500, stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot loss curve\n",
    "train_loss = summary.history[\"loss\"]\n",
    "validation_loss = summary.history[\"val_loss\"]\n",
    "\n",
    "fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "fig.add_trace(go.Scatter( y= train_loss, name= 'training loss', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.add_trace(go.Scatter( y= validation_loss, name= 'validation loss', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.update_yaxes(title_text= 'loss', row= 1, col= 1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Training loss curve\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between predicted and meassured y - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #y_train_pred = lstmmodel.predict(x_lstm_train)\n",
    "y_test_pred = lstmmodel.predict(x_lstm_test)\n",
    "\n",
    "fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "fig.add_trace(go.Scatter(y= y_test_pred.flatten(), name= 'y-predicted', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.add_trace(go.Scatter(y= y_lstm_test, name= 'y-test', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.update_yaxes(title_text= 'welle_z', row= 1, col= 1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Difference between predicted and meassured y - Test\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between predicted and meassured y - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred = lstmmodel.predict(x_lstm_train)\n",
    "#y_test_pred = lstmmodel.predict(x_lstm_test)\n",
    "\n",
    "fig= make_subplots(rows=1, cols=1,shared_xaxes= True, print_grid= True)\n",
    "\n",
    "fig.add_trace(go.Scatter( y= y_train_pred.flatten(), name= 'y-predicted', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.add_trace(go.Scatter( y= y_lstm_train, name= 'y-train', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.update_yaxes(title_text= 'welle_z', row= 1, col= 1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Difference between predicted and meassured y - Training\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_avg = np.mean(abs(y_lstm_test - y_test_pred.flatten()))\n",
    "error_max = max(abs(y_lstm_test - y_test_pred.flatten()))\n",
    "print(\"max: \", error_max)\n",
    "print(\"avg: \", error_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter suchen f√ºr Daten mit verschiedenen Batchsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [5, 10, 15, 20]\n",
    "filenames = [[\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window5_x.json\", \"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window5_y.json\"], \n",
    "            [\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window10_x.json\", \"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window10_y.json\"],\n",
    "            [\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window15_x.json\", \"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window15_y.json\"],\n",
    "            [\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window20_x.json\", \"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_window20_y.json\"]]\n",
    "\n",
    "modelpaths = [\"X:\\\\KI Praktikum\\\\validate_Data\\\\modelle2023-08-25\\\\window5.h5\",\n",
    "                \"X:\\\\KI Praktikum\\\\validate_Data\\\\modelle2023-08-25\\\\window10.h5\",\n",
    "                \"X:\\\\KI Praktikum\\\\validate_Data\\\\modelle2023-08-25\\\\window15.h5\",\n",
    "                \"X:\\\\KI Praktikum\\\\validate_Data\\\\modelle2023-08-25\\\\window20.h5\"]\n",
    "\n",
    "SaveJsonWindows(filenames=filenames, window=windows)\n",
    "i = 0\n",
    "\n",
    "for file in filenames:\n",
    "    print(file[0])\n",
    "    x_cv = OpenJsontoArr(file[0])\n",
    "    y_cv = OpenJsontoArr(file[1])\n",
    "    y_cv = y_cv.flatten()\n",
    "    \n",
    "    earlyStop = EarlyStopping(monitor='loss', patience=10, verbose =1, mode = \"auto\")\n",
    "    model = KerasRegressor(build_fn=get_lstm_model, verbose=1, layers=[5, 5, 5], dropout=0, activation = \"relu\", \n",
    "                            input_shape = (windows[i], 6))\n",
    "    model.fit(x_cv, y_cv)\n",
    "    \n",
    "    grid = dict(layers = [[2], [5], [2, 2], [2, 5], [5, 2], [5, 5], [10, 5], [10, 10], [2, 2, 2]],\n",
    "                dropout = [0, 0.1, 0.3, 0.2],\n",
    "                activation = [\"relu\", \"tanh\", \"selu\", \"elu\"])\n",
    "\n",
    "    searcher = RandomizedSearchCV(estimator=model, n_jobs=1, cv=3, param_distributions=grid, scoring='neg_mean_absolute_error', n_iter = 5)\n",
    "    searchResults = searcher.fit(x_cv, y_cv, batch_size = 500, epochs=500, callbacks = [earlyStop], use_multiprocessing = True)\n",
    "\n",
    "    bestScore = searchResults.best_score_\n",
    "    bestParams = searchResults.best_params_\n",
    "    bestModel = searchResults.best_estimator_\n",
    "    print(\"[INFO] best score is {:.2f} using {}\".format(bestScore, bestParams))\n",
    "    bestModel.save(modelpaths[i])\n",
    "\n",
    "    best_pred = bestModel.predict(x_cv)\n",
    "    difference = y_cv - best_pred\n",
    "    error_avg = np.mean(abs(difference))\n",
    "    error_max = max(abs(difference))\n",
    "    print(\"max: \", error_max)\n",
    "    print(\"avg: \", error_avg)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_LSTM:\n",
    "    model = KerasRegressor(build_fn=get_lstm_model, verbose=1, layers=[5, 5, 5], dropout=0, activation = \"relu\",      \n",
    "                    input_shape = (10,6))\n",
    "    model.fit(x_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_LSTM:\n",
    "    grid = dict( \n",
    "        layers = [[500, 500, 300], [100, 150, 50], \n",
    "                [700, 500, 300], [700, 700, 500], [500, 400, 200], \n",
    "                [700, 700, 500, 300], [700, 700, 500, 100],\n",
    "                [25, 25, 25, 20, 20, 20, 10, 10],  [75, 75, 50, 50, 50, 40, 40, 20]],\n",
    "        dropout = [0, 0.1, 0.3, 0.2],\n",
    "        activation = [\"relu\", \"tanh\", \"selu\", \"elu\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    searcher = RandomizedSearchCV(estimator=model, n_jobs=1, cv=3,\n",
    "        param_distributions=grid, scoring='neg_mean_absolute_error', n_iter = 5)\n",
    "    searchResults = searcher.fit(x_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_LSTM:\t\n",
    "\tbestScore = searchResults.best_score_\n",
    "\tbestParams = searchResults.best_params_\n",
    "\tprint(\"[INFO] best score is {:.2f} using {}\".format(bestScore,\n",
    "\tbestParams))\n",
    "\n",
    "\tbestModel = searchResults.best_estimator_\n",
    "\tbestModel.fit(x_3d, y_train)\n",
    "\tscore = bestModel.score(x_validation, y_validation)\n",
    "\tprint(\"score: {:.2f}%\".format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_LSTM:\n",
    "    best_pred = bestModel.predict(x_validation)\n",
    "    error_avg = np.mean(abs(y_validation - best_pred))\n",
    "    error_max = max(abs(y_validation - best_pred))\n",
    "    print(\"max: \", error_max)\n",
    "    print(\"avg: \", error_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
