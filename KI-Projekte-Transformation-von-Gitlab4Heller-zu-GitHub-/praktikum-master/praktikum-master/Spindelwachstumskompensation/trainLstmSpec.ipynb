{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    " \n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, GRU, LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, TerminateOnNaN, ModelCheckpoint\n",
    "import json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightedValues import weightValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to train an LSTM Model with specific parameters defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define parameters'''\n",
    "x_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\\\\s6_w60_w70\", \"*_x.json\"))\n",
    "y_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\\\\s6_w60_w70\", \"*_y.json\"))\n",
    "x_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\\\\s6_w60_w70\", \"*_x.json\"))\n",
    "y_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\\\\s6_w60_w70\", \"*_y.json\"))\n",
    "\n",
    "window_size = [60, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenJsontoArr(path):\n",
    "    file = open(path)           #\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_x_cv.json\"\n",
    "    x_3d = json.load(file)\n",
    "    file.close()\n",
    "    x_3d = np.asarray(x_3d)\n",
    "    return x_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(layers=[5, 5, 5], dropout=0, activation = \"relu\", input_shape = (1, 1), loss = \"mean_squared_error\"):\n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        if i == len(layers)-1:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=False, kernel_initializer=ks.initializers.RandomNormal(stddev = 0.005)))\n",
    "        else:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=True, kernel_initializer=ks.initializers.RandomNormal(stddev = 0.005)))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=ks.initializers.RandomNormal(stddev = 0.005)))\n",
    "    model.compile(\n",
    "        loss= loss,\n",
    "        optimizer = tf.keras.optimizers.Nadam())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom loss function to reduce the absolute error of the individual sample as well as the \n",
    "maximum error\"\"\"\n",
    "\n",
    "def customLoss(y_true, y_pred):\n",
    "    weight = 1.5\n",
    "    difference = tf.abs(y_true - y_pred)\n",
    "    exponent = tf.exp(tf.multiply(weight, difference))\n",
    "    weighted_muls = tf.multiply(difference, exponent)\n",
    "    boltzmann_op = tf.reduce_sum(weighted_muls) / tf.reduce_sum(exponent)\n",
    "    loss = tf.add(boltzmann_op, tf.losses.mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(x_train, x_test, y_train, y_test):\n",
    "    x_train = OpenJsontoArr(x_train)\n",
    "    x_test = OpenJsontoArr(x_test)\n",
    "    y_train = OpenJsontoArr(y_train)\n",
    "    y_test = OpenJsontoArr(y_test)\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "\n",
    "    df_y_train = pd.DataFrame({\"y\": y_train})\n",
    "    df_weights_train = weightValues(df_y_train, weightMin=1, weightMax=1.1, namey=\"y\", nameWeights=\"weights\")\n",
    "    df_y_test = pd.DataFrame({\"y\": y_test})\n",
    "    df_weights_test = weightValues(df_y_test, weightMin=1, weightMax=1.1, namey=\"y\", nameWeights=\"weights\")\n",
    "\n",
    "    y_test = np.array(df_weights_test)\n",
    "    y_test = y_test[:, :2]\n",
    "    y_train = np.array(df_weights_train)\n",
    "    y_train = y_train[:, :2]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to plot the prediction compared with the real y-values'''\n",
    "\n",
    "def plotPredError(y_pred, y_true):\n",
    "    scatter_mode = 'lines'\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1, shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter(y= y_pred.flatten(), name= 'prediction', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter(y= y_true.flatten(), name= 'trace', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'y-value', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Prediction error\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to plot a loss curve'''\n",
    "\n",
    "def plotTrainValLoss(summary):\n",
    "    scatter_mode = \"lines\"\n",
    "    train_loss = summary.history[\"loss\"]\n",
    "    validation_loss = summary.history[\"val_loss\"]\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1, shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter(y= train_loss, name= 'training loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter(y= validation_loss, name= 'validation loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'loss', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Training loss curve\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = EarlyStopping(monitor='val_loss', patience=100, verbose =1, mode = \"auto\")\n",
    "stopNaN = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM(x_train, x_test, y_train, y_test, window, filepath):\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    lstmmodel = get_lstm_model(layers=[5, 2], dropout=0, activation = \"elu\", input_shape = (window, 6), loss = tf.keras.losses.MeanSquaredError()) #tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    print(lstmmodel.summary())\n",
    "    print(lstmmodel.input_shape)\n",
    "    \n",
    "    summary = lstmmodel.fit(x_train, y_train[:,0], validation_data=(x_test, y_test), sample_weight = y_train[:,1], \n",
    "                            epochs = 1000, batch_size = 1500, callbacks=[earlyStop, stopNaN, checkpoint], shuffle = False, verbose = 0)\n",
    "    print(lstmmodel.summary())\n",
    "    #bestModel = ks.models.load_model(filepath, compile=False)\n",
    "    \n",
    "    y_pred_test = lstmmodel.predict(x_test)\n",
    "    y_pred_train = lstmmodel.predict(x_train)\n",
    "\n",
    "    plotPredError(y_pred_test, y_test[:, 0])\n",
    "\n",
    "    diff_test = y_test[:, 0] - y_pred_test.flatten()\n",
    "    error_avg_test = np.mean(abs(diff_test))\n",
    "    error_max_test = max(abs(diff_test))\n",
    "\n",
    "    diff_train = y_train[:, 0] - y_pred_train.flatten()\n",
    "    error_avg_train = np.mean(abs(diff_train))\n",
    "    error_max_train = max(abs(diff_train))\n",
    "\n",
    "    print(\"Validation scores: \")\n",
    "    print(\"max error: \", error_max_test)\n",
    "    print(\"mean abs error: \", error_avg_test)\n",
    "    print(\"Error over the training data: \")\n",
    "    print(\"max error: \", error_max_train)\n",
    "    print(\"mean abs error: \", error_avg_train)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"model_\" +\".h5\")\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    filepath = os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"model_\" + x_train[i][-19:-7] +\".h5\")\n",
    "    print(x_train[i])\n",
    "    x_traini, x_testi, y_traini, y_testi = preprocessData(x_train[i], x_test[i], y_train[i], y_test[i])\n",
    "    \n",
    "    if np.isnan(x_traini).any():\n",
    "        print(\"nan\")\n",
    "    if np.isnan(x_testi).any():\n",
    "        print(\"nan\") \n",
    "    summary = trainLSTM(x_traini, x_testi, y_traini, y_testi, window_size[i], filepath=filepath)\n",
    "    plotTrainValLoss(summary)\n",
    "\n",
    "# print(x_train)\n",
    "# x_traini, x_testi, y_traini, y_testi = preprocessData(x_train[0], x_test[0], y_train[0], y_test[0])\n",
    "# summary = trainLSTM(x_traini, x_testi, y_traini, y_testi, window_size)\n",
    "# plotTrainValLoss(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"*s6_w70_x.json\"))\n",
    "# y_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"*s6_w70_y.json\"))\n",
    "# x_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\", \"*s6_w70_x.json\"))\n",
    "# y_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\", \"*s6_w70_y.json\"))\n",
    "\n",
    "# window_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train)\n",
    "# x_traini, x_testi, y_traini, y_testi = preprocessData(x_train[0], x_test[0], y_train[0], y_test[0])\n",
    "# summary = trainLSTM(x_traini, x_testi, y_traini, y_testi, window_size)\n",
    "# plotTrainValLoss(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
