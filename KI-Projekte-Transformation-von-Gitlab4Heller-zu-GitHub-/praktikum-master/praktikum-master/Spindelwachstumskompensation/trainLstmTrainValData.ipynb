{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    " \n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, GRU, LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, TerminateOnNaN\n",
    "import json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightedValues import weightValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenJsontoArr(path):\n",
    "    file = open(path)           #\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_x_cv.json\"\n",
    "    x_3d = json.load(file)\n",
    "    file.close()\n",
    "    x_3d = np.asarray(x_3d)\n",
    "    return x_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(layers=[5, 5, 5], dropout=0, activation = \"relu\", input_shape = (1, 1), loss = \"mean_squared_error\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        if i == len(layers)-1:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=False))\n",
    "        else:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=True))\n",
    "        #model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        loss= loss,\n",
    "        #metrics=[ \"max_loss\"],\n",
    "        optimizer = tf.keras.optimizers.Adamax())\n",
    "    # return compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom loss function to reduce the absolute error of the individual sample as well as the \n",
    "maximum error\"\"\"\n",
    "def customLoss(y_true, y_pred):\n",
    "    weight = 1.5\n",
    "    difference = tf.abs(y_true - y_pred)\n",
    "    exponent = tf.exp(tf.multiply(weight, difference))\n",
    "    weighted_muls = tf.multiply(difference, exponent)\n",
    "    boltzmann_op = tf.reduce_sum(weighted_muls) / tf.reduce_sum(exponent)\n",
    "    loss = tf.add(boltzmann_op, tf.losses.mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(x_train, x_test, y_train, y_test):\n",
    "    x_train = OpenJsontoArr(x_train)\n",
    "    x_test = OpenJsontoArr(x_test)\n",
    "    y_train = OpenJsontoArr(y_train)\n",
    "    y_test = OpenJsontoArr(y_test)\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "\n",
    "    df_y_train = pd.DataFrame({\"y\": y_train})\n",
    "    df_weights_train = weightValues(df_y_train, weightMin=1, weightMax=2, namey=\"y\", nameWeights=\"weights\")\n",
    "    df_y_test = pd.DataFrame({\"y\": y_test})\n",
    "    df_weights_test = weightValues(df_y_test, weightMin=1, weightMax=2, namey=\"y\", nameWeights=\"weights\")\n",
    "\n",
    "    y_test = np.array(df_weights_test)\n",
    "    y_test = y_test[:, :2]\n",
    "    y_train = np.array(df_weights_train)\n",
    "    y_train = y_train[:, :2]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM(x_train, x_test, y_train, y_test, window):\n",
    "    #i = 0\n",
    "    #summary = np.array(len(losses))\n",
    "    #for loss in losses:\n",
    "    lstmmodel = get_lstm_model(layers=[1], dropout=0, activation = \"elu\", input_shape = (window,6), loss = customLoss)\n",
    "\n",
    "    summary = lstmmodel.fit(x_train, y_train[:,0], validation_data=(x_test, y_test), sample_weight = y_train[:,1], \n",
    "                            epochs = 2500, batch_size = 4000, callbacks=[earlyStop, stopNaN], shuffle = False, verbose = 0)\n",
    "\n",
    "    y_pred_test = lstmmodel.predict(x_test)\n",
    "    y_pred_train = lstmmodel.predict(x_train)\n",
    "\n",
    "    diff_test = y_test[:, 0] - y_pred_test.flatten()\n",
    "    error_avg_test = np.mean(abs(diff_test))\n",
    "    error_max_test = max(abs(diff_test))\n",
    "\n",
    "    diff_train = y_train[:, 0] - y_pred_train.flatten()\n",
    "    error_avg_train = np.mean(abs(diff_train))\n",
    "    error_max_train = max(abs(diff_train))\n",
    "\n",
    "    #mean_absolute = tf.keras.losses.mean_absolute_error(y_cv[:,0], y_pred.flatten())\n",
    "    #print(\"mean absolute: \", mean_absolute)\n",
    "    print(\"Validation scores: \")\n",
    "    print(\"max error: \", error_max_test)\n",
    "    print(\"mean abs error: \", error_avg_test)\n",
    "    print(\"Error over the training data: \")\n",
    "    print(\"max error: \", error_max_train)\n",
    "    print(\"mean abs error: \", error_avg_train)\n",
    "    #i+=1\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainValLoss(summary):\n",
    "    scatter_mode = \"lines\"\n",
    "    train_loss = summary.history[\"loss\"]\n",
    "    validation_loss = summary.history[\"val_loss\"]\n",
    "\n",
    "    fig= make_subplots(rows=1, cols=1, shared_xaxes= True, print_grid= True)\n",
    "\n",
    "    fig.add_trace(go.Scatter(y= train_loss, name= 'training loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.add_trace(go.Scatter(y= validation_loss, name= 'validation loss', mode= scatter_mode), row= 1, col= 1)\n",
    "    fig.update_yaxes(title_text= 'loss', row= 1, col= 1)\n",
    "\n",
    "    fig.update_layout(height=600, width=1200, title_text=\"Training loss curve\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses = [customLoss()]\n",
    "#,          tf.keras.losses.MeanSquaredError(),\n",
    "#           tf.keras.losses.MeanAbsoluteError()]\n",
    "\n",
    "earlyStop = EarlyStopping(monitor='val_loss', patience=100, verbose =1, mode = \"auto\")\n",
    "#csvLogger = CSVLogger('X:\\\\KI Praktikum\\\\csvLoggerCustomLoss.xlsx')\n",
    "stopNaN = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"*_x.json\"))\n",
    "y_train = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\training_data\", \"*_y.json\"))\n",
    "x_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\", \"*_x.json\"))\n",
    "y_test = glob.glob(os.path.join(\"C:\\\\Users\\\\wch002\\\\Desktop\\\\validation_data\", \"*_y.json\"))\n",
    "window_size = [50, 60, 70, 50, 60, 70, 50, 60, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    print(x_train[i])\n",
    "    x_traini, x_testi, y_traini, y_testi = preprocessData(x_train[i], x_test[i], y_train[i], y_test[i])\n",
    "    summary = trainLSTM(x_traini, x_testi, y_traini, y_testi, window_size[i])\n",
    "    plotTrainValLoss(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y_train = pd.DataFrame({\"y\": y_train})\n",
    "# print(df_y_train.head(5))\n",
    "# df_weights_train = weightValues(df_y_train, weightMin=1, weightMax=2, namey=\"y\", nameWeights=\"weights\")\n",
    "\n",
    "# df_y_test = pd.DataFrame({\"y\": y_test})\n",
    "# print(df_y_test.head(5))\n",
    "# df_weights_test = weightValues(df_y_test, weightMin=1, weightMax=2, namey=\"y\", nameWeights=\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_weights = df_weights.drop(\"0\", axis=1)\n",
    "# #print(df_weights.head(10))\n",
    "# # x_weighted = np.zeros((x_cv.shape[0], x_cv.shape[1], x_cv.shape[2]+1))\n",
    "# # x_weighted[:,:,:6] = x_cv\n",
    "# # x_weighted[:,:,-1] = df_weights\n",
    "# y_test = np.array(df_weights_test)\n",
    "# y_test = y_test[:, :2]\n",
    "# y_train = np.array(df_weights_train)\n",
    "# y_train = y_train[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = \"X:\\\\KI Praktikum\\\\model\\\\23_09_01lstm_CustomLoss_max12_8.h5\"\n",
    "# lstmmodel.save(modelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
