{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, GRU, LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, TerminateOnNaN\n",
    "import json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightedValues import weightValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenJsontoArr(path):\n",
    "    file = open(path)           #\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darray_x_cv.json\"\n",
    "    x_3d = json.load(file)\n",
    "    file.close()\n",
    "    x_3d = np.asarray(x_3d)\n",
    "    return x_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(layers=[5, 5, 5], dropout=0, activation = \"relu\", input_shape = (1, 1), loss = \"mean_squared_error\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        if i == len(layers)-1:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=False))\n",
    "        else:\n",
    "            model.add(LSTM(layers[i], stateful = False, dropout=dropout, activation = activation, return_sequences=True))\n",
    "        #model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        loss= loss,\n",
    "        #metrics=[ \"max_loss\"],\n",
    "        optimizer = tf.keras.optimizers.Adamax())\n",
    "    # return compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_files = glob.glob(os.path.join(\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darrays\", \"*15_x.json\"))\n",
    "y_files = glob.glob(os.path.join(\"X:\\\\KI Praktikum\\\\validate_Data\\\\3darrays\", \"*15_y.json\"))\n",
    "x_cv = OpenJsontoArr(x_files[0])\n",
    "y_cv = OpenJsontoArr(y_files[0])\n",
    "y_cv = y_cv.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_cv[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.DataFrame({\"y\": y_cv})\n",
    "print(df_y.head(15))\n",
    "df_weights = weightValues(df_y, weightMin=1, weightMax=2, namey=\"y\", nameWeights=\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weights = df_weights.drop(\"0\", axis=1)\n",
    "print(df_weights.head(10))\n",
    "# x_weighted = np.zeros((x_cv.shape[0], x_cv.shape[1], x_cv.shape[2]+1))\n",
    "# x_weighted[:,:,:6] = x_cv\n",
    "# x_weighted[:,:,-1] = df_weights\n",
    "y_cv = np.array(df_weights)\n",
    "y_cv = y_cv[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom loss function to reduce the absolute error of the individual sample as well as the \n",
    "maximum error\"\"\"\n",
    "def customLoss(y_true, y_pred):\n",
    "    weight = 1.6\n",
    "    difference = tf.abs(y_true - y_pred)\n",
    "    exponent = tf.exp(tf.multiply(weight, difference))\n",
    "    weighted_muls = tf.multiply(difference, exponent)\n",
    "    boltzmann_op = tf.reduce_sum(weighted_muls) / tf.reduce_sum(exponent)\n",
    "    loss = tf.add(boltzmann_op, tf.losses.mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses = [customLoss()]\n",
    "#,          tf.keras.losses.MeanSquaredError()] #,\n",
    "#           tf.keras.losses.MeanAbsoluteError(),\n",
    "#           tf.keras.losses.CosineSimilarity()]\n",
    "x_lstm_train, x_lstm_test, y_lstm_train, y_lstm_test = train_test_split(x_cv, y_cv, shuffle = False, test_size=0.1)\n",
    "\n",
    "earlyStop = EarlyStopping(monitor='val_loss', patience=100, verbose =1, mode = \"auto\")\n",
    "csvLogger = CSVLogger('X:\\\\KI Praktikum\\\\csvLoggerCustomLoss.xlsx')\n",
    "stopNaN = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#summary = np.array(len(losses))\n",
    "#for loss in losses:\n",
    "lstmmodel = get_lstm_model(layers=[1], dropout=0, activation = \"elu\", input_shape = (15,6), loss = customLoss)\n",
    "\n",
    "summary = lstmmodel.fit(x_lstm_train, y_lstm_train[:,0], validation_data=(x_lstm_test, y_lstm_test), sample_weight = y_lstm_train[:,1], \n",
    "                        epochs = 500, batch_size = 4000, callbacks=[earlyStop, csvLogger, stopNaN], shuffle = False)\n",
    "\n",
    "y_pred = lstmmodel.predict(x_cv)\n",
    "\n",
    "difference = y_cv[:, 0] - y_pred.flatten()\n",
    "error_avg = np.mean(abs(difference))\n",
    "error_max = max(abs(difference))\n",
    "#mean_absolute = tf.keras.losses.mean_absolute_error(y_cv[:,0], y_pred.flatten())\n",
    "#print(\"mean absolute: \", mean_absolute)\n",
    "print(\"max: \", error_max)\n",
    "print(\"mean abs error: \", error_avg)\n",
    "#i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_mode = \"lines\"\n",
    "train_loss = summary.history[\"loss\"]\n",
    "validation_loss = summary.history[\"val_loss\"]\n",
    "\n",
    "fig= make_subplots(rows=1, cols=1, shared_xaxes= True, print_grid= True)\n",
    "\n",
    "fig.add_trace(go.Scatter( y= train_loss, name= 'training loss', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.add_trace(go.Scatter( y= validation_loss, name= 'validation loss', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.update_yaxes(title_text= 'loss', row= 1, col= 1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Training loss curve\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"X:\\\\KI Praktikum\\\\model\\\\23_09_01lstm_CustomLoss_max12_8.h5\"\n",
    "lstmmodel.save(modelPath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
