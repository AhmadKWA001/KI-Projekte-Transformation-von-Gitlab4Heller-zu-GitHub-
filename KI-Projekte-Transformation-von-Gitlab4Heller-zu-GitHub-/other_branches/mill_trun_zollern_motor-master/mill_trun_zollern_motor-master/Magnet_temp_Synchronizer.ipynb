{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Skript dient sich zur Synchronisierung der Daten aus der Daten bank mit dem Siganl der Magnet-Temperatur.\n",
    "Schritte:\n",
    "1- Magnet-Temperatur Upsampling\n",
    "2- Aggregate Abfrage zwischen beiden Dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\rough_data\\\\Magnet_temperature\\\\2023_07_19_rotor_hochtemp.csv\"\n",
    "put_File_name= \"Versuch_19072023_Sync.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set time priod needed from Database (UTC-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start= datetime.strptime(\"2023-09-06T00:00:00.000+0000\", \"%Y-%m-%dT%H:%M:%S.%f+0000\")\n",
    "end= datetime.strptime (\"2023-09-07T13:00:00.000+0000\", \"%Y-%m-%dT%H:%M:%S.%f+0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DB Config\n",
    "host= \"m53763edge\"\n",
    "port=27017\n",
    "collection= \"thermal_data\"\n",
    "all_data= True\n",
    "get_preprocessed_data= False\n",
    "time_shift= +2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prameters for upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sample_rate= 5 ## Seconds\n",
    "new_sample_rate= 0.1 ## Seconds\n",
    "count_insertions= int(current_sample_rate / new_sample_rate) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block(start_date, period: int, count_insertions: int):\n",
    "    insertion_block= pd.DataFrame({'date': [ start_date + timedelta(seconds= (i+1) * period) for i in range(count_insertions)],\n",
    "                                   'magnet_temperature':[np.NaN for i in range(count_insertions)]})\n",
    "    return insertion_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_block(original_data: pd.DataFrame, period:int,  count_insertions: int):\n",
    "    output_df= pd.DataFrame(columns=original_data.columns.to_list())\n",
    "    for index, row in original_data.iterrows():\n",
    "        if index + 1 == len(original_data):\n",
    "            output_df= output_df.append(row, ignore_index= True)\n",
    "        else:\n",
    "            start_date= row['date']\n",
    "            block= create_block(start_date= start_date, period= period, count_insertions= count_insertions)\n",
    "            output_df= output_df.append(row.to_dict(),  ignore_index= True)\n",
    "            output_df= output_df.append(block, ignore_index= True)\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Magnet Temperature and add 1-Hour to the time stamp to get it in local time\n",
    "- Apply upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(file_path)\n",
    "df['date']= df['date'].apply( lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\") + timedelta(hours=+1))\n",
    "df.drop(columns= ['No'],inplace= True)\n",
    "to_up_sampled_data= df\n",
    "to_up_sampled_data= insert_block(to_up_sampled_data, period= new_sample_rate, count_insertions= count_insertions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation of the upsampled file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_up_sampled_data['magnet_temperature']= to_up_sampled_data['magnet_temperature'].interpolate(method='linear', order=5, axis= 0,inplace= False)\n",
    "to_up_sampled_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data source from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read raw and preprocessing data\n",
    "client = MongoClient(host= host, port=port)\n",
    "db = client.h4ai\n",
    "event_list = db[collection].find({ \"date\" : { '$gte' : start, '$lt' : end} }).sort('date', 1)\n",
    "signals= None\n",
    "i=0\n",
    "last_prediction= -500\n",
    "prev_time= None\n",
    "current_time= None\n",
    "stop= False\n",
    "### Go through events in DB\n",
    "for event in event_list:\n",
    "    i+=1\n",
    "    record_list= event['content']\n",
    "    keys= None\n",
    "    ### Go through records in each event\n",
    "    for record in record_list:\n",
    "        ### Get right data between start / end \n",
    "        if record['date']< start or record['date'] > end:\n",
    "            continue\n",
    "        current_time= record['date']+ timedelta(hours= time_shift)\n",
    "        if prev_time is not None  and (current_time -prev_time).total_seconds()> 5.5:\n",
    "            print('Period= ', (current_time -prev_time).total_seconds())\n",
    "            print('prev_time', prev_time)\n",
    "            print('current_time', current_time)\n",
    "            print('#########################################################')\n",
    "        for item in record['raw_data'].keys():\n",
    "            record['raw_data'][item]= [np.double(np.round(record['raw_data'][item],decimals=2))]\n",
    "        record['raw_data']['date']=[current_time]\n",
    "        record['raw_data']['given2model']=[record['given2model']]\n",
    "        record['raw_data']['prediction']= [record['prediction']]\n",
    "        ####################################\n",
    "        #print ('After ',record)\n",
    "        lf_signal_point= pd.DataFrame(record['raw_data'])\n",
    "        if signals is None:\n",
    "            signals= lf_signal_point\n",
    "        else:\n",
    "            signals= signals.append(lf_signal_point,ignore_index= True)\n",
    "        prev_time= current_time\n",
    "signals.reset_index(inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unneeded Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals.drop(columns=['index', 'given2model'], inplace= True)\n",
    "signals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to create property list of a dataframe suitable for an SQL query\n",
    "def get_field_list(df: pd.DataFrame, df_name: str):\n",
    "    df_logs_fields= df.columns.to_list()\n",
    "    fields= \"\"\n",
    "    for field in df_logs_fields:\n",
    "        if fields == \"\":\n",
    "            fields =  \"{df_name}.\".format(df_name= df_name) + field\n",
    "        else:\n",
    "            fields += \", {df_name}.\".format(df_name= df_name) + field\n",
    "    return fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate field list for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_fields= get_field_list(signals, 'signals')\n",
    "to_up_sampled_data_fields= get_field_list(to_up_sampled_data, 'to_up_sampled_data')\n",
    "table_signals= 'signals'\n",
    "table_to_up_sampled_data= 'to_up_sampled_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_up_sampled_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Query to combine data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Apply Query\n",
    "query= \"\"\"\n",
    "select * from \n",
    "    (select {t_a}.start as start, {t_a}.end as end,{table_a_fields}, min({t_b}.ts)  as ts from \n",
    "        (\n",
    "            (select  {table_a_fields}, {t_a}.date as start, ({t_a}.date + interval 5 Second) as end from {t_a})  {t_a}\n",
    "            join\n",
    "            (select {table_b_fields}, {t_b}.date as ts from {t_b})  {t_b}\n",
    "            on ({t_b}.ts >= {t_a}.start) and ( {t_b}.ts < {t_a}.end)\n",
    "        )\n",
    "    group by {t_a}.start, {t_a}.end, {table_a_fields}) combined_logs\n",
    "join\n",
    "    {t_b}\n",
    "on (combined_logs.ts = {t_b}.date)\n",
    "order by start Asc\n",
    "\"\"\".format(t_a= table_signals, t_b= table_to_up_sampled_data, table_a_fields= signals_fields, table_b_fields= to_up_sampled_data_fields)\n",
    "results= ddb.query(query)\n",
    "result_df= results.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_colums= ['date','start', 'end', 'ts','T_KLEMMUNG', 'T_LAGER', 'T_MOTOR', 'V_LAENGS', 'DRZ5',\n",
    "       'Analytic_sol', 'I_MOMENT_G', 'I_FELD', 'DRZ', 'I_IST_BETR', 'V_QUER',\n",
    "       'T_BETT',  'prediction', 'magnet_temperature']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulted Data frame after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[selected_colums]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Data of resulted Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_mode= 'lines'#'lines'# 'lines+markers'# 'markers'\n",
    "selected_Columns= ['date','DRZ5','T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "       'T_BETT',  'Analytic_sol','prediction', 'magnet_temperature']\n",
    "df= result_df[selected_colums]\n",
    "fig= make_subplots(rows=6,cols=1,shared_xaxes= True, print_grid= True, vertical_spacing=0.02)\n",
    "##DRZ\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['DRZ5'], name='given2model', mode= scatter_mode), row= 1, col= 1)\n",
    "fig.update_yaxes(title_text= 'DRZ (RPM)', row= 1, col= 1)\n",
    "##T_KLEMMUNG\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['T_KLEMMUNG'], name='T_KLEMMUNG', mode= scatter_mode), row= 2, col= 1)\n",
    "fig.update_yaxes(title_text= 'T_KLEMMUNG C°', row= 2, col= 1)\n",
    "##T_LAGER\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['T_LAGER'], name='T_LAGER', mode= scatter_mode), row= 3, col= 1)\n",
    "fig.update_yaxes(title_text= 'T_LAGER C°', row= 3, col= 1)\n",
    "##'T_MOTOR'\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['T_MOTOR'], name='T_MOTOR', mode= scatter_mode), row= 4, col= 1)\n",
    "fig.update_yaxes(title_text= 'T_MOTOR C°', row= 4, col= 1)\n",
    "##'prediction_variable'\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['magnet_temperature'], name='magnet_temperature', mode= scatter_mode), row= 5, col= 1)\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['prediction'], name='prediction', mode= scatter_mode), row= 5, col= 1)\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['Analytic_sol'], name='Analytic_sol', mode= scatter_mode), row= 5, col= 1)\n",
    "fig.update_yaxes(title_text= 'Rotor Temperature C°', row= 5, col= 1)\n",
    "##Restfehler\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['magnet_temperature']- df['Analytic_sol'], name=' Error-Analytic', mode= scatter_mode),  row= 6 , col= 1)\n",
    "fig.add_trace(go.Scatter(x= df['date'], y= df['magnet_temperature']- df['prediction'], name=' Error-Data-Driven', mode= scatter_mode),  row= 6 , col= 1)\n",
    "    ## Draw the tolerence +-5\n",
    "#fig.add_trace(go.Scatter(x= signals['date'], y= np.full_like(signals['db_prediction_abs_error'],5), name='+5 Obere Grenze', mode= scatter_mode),  row= 10 , col= 1)\n",
    "#fig.add_trace(go.Scatter(x= signals['date'], y= np.full_like(signals['db_prediction_abs_error'],-5), name='- 5 Obere Grenze', mode= scatter_mode),  row= 10 , col= 1)\n",
    "fig.update_yaxes(title_text= 'Error in Rotor Temperature C°', row= 6, col= 1)\n",
    "fig.update_layout(height=1200, width=1400, title_text= 'M57002 Machine Data')\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it as .CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\rough_data\\\\database_with_sensor\\\\Synchronization_results\\\\{file_name}\".format(file_name= put_File_name)\n",
    "selected_colums= ['date' ,'ts','T_KLEMMUNG', 'T_LAGER', 'T_MOTOR', 'V_LAENGS', 'DRZ5',\n",
    "       'Analytic_sol', 'I_MOMENT_G', 'I_FELD', 'DRZ', 'I_IST_BETR', 'V_QUER',\n",
    "       'T_BETT',  'prediction', 'magnet_temperature']\n",
    "result_df[selected_colums].to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_path=\"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\assets\\\\v0.0.10\\\\preprocessor.onnx\"\n",
    "sess= ort.InferenceSession(preprocessor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.get_inputs()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data= np.asarray([[0,0, 0,0]],  dtype=np.float32)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_outputs = sess.run(None, {'signals': data})\n",
    "onnx_output = onnx_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
