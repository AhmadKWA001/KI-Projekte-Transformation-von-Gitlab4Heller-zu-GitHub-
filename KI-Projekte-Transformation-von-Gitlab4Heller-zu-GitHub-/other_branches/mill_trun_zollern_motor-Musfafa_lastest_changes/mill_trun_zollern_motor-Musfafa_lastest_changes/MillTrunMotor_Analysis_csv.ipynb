{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as ex\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, KernelPCA,IncrementalPCA,SparsePCA\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.layers import PReLU\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import onnx\n",
    "import tf2onnx\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For new Motor\n",
    "# workspace= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor2_8Sensoren\"\n",
    "# validation_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor2_8Sensoren\"\n",
    "# training_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor2_8Sensoren\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version= 'v0.0.12'\n",
    "workspace= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor1\"\n",
    "testing_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor1\\\\testing\"\n",
    "validation_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor1\\\\validation_data\"\n",
    "training_data= \"G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\data\\\\Motor1\\\\training\"\n",
    "assests_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\assets\\\\'+version+'\\\\'\n",
    "to_smooth_signals='magnet_temperature'\n",
    "nominated_signal= 'magnet_temperature'\n",
    "target_variable= 'magnet_temperature_smoothed'#'magnet_temperature'\n",
    "smoothing_window= 20\n",
    "if not os.path.exists(assests_dir):\n",
    "    print('Directory: {dir} is not exist! Creating Directory'.format(dir= assests_dir))\n",
    "    os.makedirs(name= assests_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(input: pd.DataFrame, window= 240, signals=''):\n",
    "    input[str(signals+'_smoothed')]= input[signals].rolling(window= window).mean().shift(-int(window/2)).round(2)\n",
    "    input= input[window:-window]\n",
    "    return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(training_data)\n",
    "df:pd.DataFrame= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    df_file = pd.read_csv(file)\n",
    "    print(\"first Columns: \", df_file.columns)\n",
    "    if nominated_signal in df_file.columns.to_list():\n",
    "        df_file[to_smooth_signals]=df_file[nominated_signal]\n",
    "    else:\n",
    "        print('--')\n",
    "    df_file= smoothing_data(df_file, window= smoothing_window, signals=to_smooth_signals)\n",
    "    #print (df_file.isna().any())\n",
    "    df_file['file_name']= file\n",
    "    print('Current File: ', file)\n",
    "    if df is None:\n",
    "        df= df_file\n",
    "    else:\n",
    "        df= pd.concat([df, df_file], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filtering motor temperature\n",
    "df= df[df['T_MOTOR'] != 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing signals for each expriment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "# #        'T_BETT', 'magnet_temperature']\n",
    "# selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "#        'T_BETT', 'I_MOMENT_G','I_FELD','I_IST_BETR','V_QUER','T_BETT', 'magnet_temperature']\n",
    "# selected_columns_with_units= ['Drehzahl (RPM)', 'Temp_Klemmung (C°)','Temp_Lager (C°)','Temp_Motor (C°)','T_BETT (C°)','Temperatur_Magnet (C°)']\n",
    "# experiments= list(df['file_name'].unique())\n",
    "# fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns_with_units, vertical_spacing=0.02)\n",
    "# for experiment in experiments:\n",
    "#     current_df= df[df['file_name'] == experiment]\n",
    "#     for j in range(len(selected_columns)):\n",
    "#         fig.add_trace(go.Scatter(y= current_df[selected_columns[j]], name= str(selected_columns[j]+ experiment), mode= 'lines'), row= j+1, col= 1)\n",
    "#         #fig.update_yaxes(title_text= selected_columns[j], row= j+1, col= 1)\n",
    "#     #current_df['magnet_temperature_smoothed']=  current_df ['magnet_temperature'].rolling(window= 10).mean().shift(-5)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['magnet_temperature_smoothed'], name='smoothed', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['prediction'], name='Prediction', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     fig.update_xaxes(title_text= 'Zeit ',row= len(selected_columns), col= 1)\n",
    "#     fig.update_layout(height=1200, width=1200, title_text='MillTurn-Motor {experiment}'.format(experiment= experiment))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "# #        'T_BETT', 'magnet_temperature']\n",
    "# selected_columns= ['l6']\n",
    "# selected_columns_with_units= ['l6']\n",
    "# df['l1']= df['DRZ5'].abs() * df['T_MOTOR']\n",
    "# df['l2']= df['I_IST_BETR'] * df['V_QUER']\n",
    "# df['l3']= df['I_FELD'] * df['V_QUER']\n",
    "# df['l4']= df['I_MOMENT_G'] * df['V_QUER']\n",
    "# df['l5']= df['I_MOMENT_G'] * df['V_QUER']\n",
    "# df['l6']=  df['I_FELD']*df['I_FELD'] + df['I_MOMENT_G']* df['I_MOMENT_G'] #df['I_MOMENT_G'] * df['V_QUER'] / df['DRZ5']\n",
    "# df['l7']= df['I_IST_BETR'] * df['I_IST_BETR']\n",
    "# experiments= list(df['file_name'].unique())\n",
    "\n",
    "# for experiment in experiments:\n",
    "#     current_df= df[df['file_name'] == experiment]\n",
    "#     fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns_with_units, vertical_spacing=0.02)\n",
    "#     for j in range(len(selected_columns)):\n",
    "#         fig.add_trace(go.Scatter(y= current_df[selected_columns[j]], name= str(selected_columns[j]+ experiment), mode= 'lines'), row= j+1, col= 1)\n",
    "#         #fig.update_yaxes(title_text= selected_columns[j], row= j+1, col= 1)\n",
    "#     #current_df['magnet_temperature_smoothed']=  current_df ['magnet_temperature'].rolling(window= 10).mean().shift(-5)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['magnet_temperature_smoothed'], name='smoothed', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['prediction'], name='Prediction', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     fig.add_trace(go.Scatter(y= current_df['l7'], name= 'l7', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     fig.update_xaxes(title_text= 'Zeit ',row= len(selected_columns), col= 1)\n",
    "#     fig.update_layout(height=1200, width=1200, title_text='MillTurn-Motor {experiment}'.format(experiment= experiment))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "# #        'T_BETT', 'magnet_temperature']\n",
    "# selected_columns= ['DRZ5', 'V_LAENGS','I_MOMENT_G', 'magnet_temperature']# 'I_IST_BETR','I_FELD','I_MOMENT_G','V_QUER','V_LAENGS',\n",
    "# selected_columns_with_units= ['DRZ5','V_LAENGS','I_MOMENT_G','Temperatur_Magnet (C°)']#,'I_IST_BETR','I_FELD','I_MOMENT_G','V_QUER','V_LAENGS',\n",
    "# df['l1']= df['DRZ5'].abs() * df['T_MOTOR']\n",
    "# df['l2']= df['I_IST_BETR'] * df['V_QUER']\n",
    "# df['l3']= df['I_FELD'] * df['V_QUER']\n",
    "# df['l4']= df['I_MOMENT_G'] * df['V_LAENGS']\n",
    "# df['l5']= df['I_MOMENT_G'] * df['V_QUER']\n",
    "# #df['l6']= df['I_MOMENT_G'] * df['V_QUER'] / df['DRZ5']\n",
    "# experiments= list(df['file_name'].unique())\n",
    "# fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns_with_units, vertical_spacing=0.02)\n",
    "# for experiment in experiments:\n",
    "#     current_df= df[df['file_name'] == experiment]\n",
    "#     for j in range(len(selected_columns)):\n",
    "#         fig.add_trace(go.Scatter(y= current_df[selected_columns[j]].abs(), name= str(selected_columns[j]+ experiment), mode= 'lines'), row= j+1, col= 1)\n",
    "#         #fig.update_yaxes(title_text= selected_columns[j], row= j+1, col= 1)\n",
    "#     #current_df['magnet_temperature_smoothed']=  current_df ['magnet_temperature'].rolling(window= 10).mean().shift(-5)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['magnet_temperature_smoothed'], name='smoothed', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     #fig.add_trace(go.Scatter(y= current_df['prediction'], name='Prediction', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "#     fig.update_xaxes(title_text= 'Zeit ',row= len(selected_columns), col= 1)\n",
    "#     fig.update_layout(height=1200, width=1200, title_text='MillTurn-Motor {experiment}'.format(experiment= experiment))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "#        'T_BETT', 'magnet_temperature']\n",
    "selected_columns= ['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR',\n",
    "       'T_BETT', 'I_MOMENT_G','I_FELD','I_IST_BETR','V_QUER','T_BETT', 'magnet_temperature']\n",
    "selected_columns_with_units= ['Drehzahl (RPM)', 'Temp_Klemmung (C°)','Temp_Lager (C°)','Temp_Motor (C°)','T_BETT (C°)','Temperatur_Magnet (C°)']\n",
    "experiments= list(df['file_name'].unique())\n",
    "for experiment in experiments:\n",
    "    current_df= df[df['file_name'] == experiment]\n",
    "    fig= make_subplots(rows=len(selected_columns) ,cols=1,shared_xaxes= True, print_grid= True, subplot_titles= selected_columns_with_units, vertical_spacing=0.02)\n",
    "    for j in range(len(selected_columns)):\n",
    "        fig.add_trace(go.Scatter(x= current_df['date'],y= current_df[selected_columns[j]], name=selected_columns[j], mode= 'lines'), row= j+1, col= 1)\n",
    "        #fig.update_yaxes(title_text= selected_columns[j], row= j+1, col= 1)\n",
    "    #current_df['magnet_temperature_smoothed']=  current_df ['magnet_temperature'].rolling(window= 10).mean().shift(-5)\n",
    "    fig.add_trace(go.Scatter(x= current_df['date'],y= current_df['magnet_temperature_smoothed'], name='smoothed', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "    fig.add_trace(go.Scatter(x= current_df['date'],y= current_df['prediction'], name='Prediction', mode= 'lines'), row= len(selected_columns), col= 1)\n",
    "    fig.update_xaxes(title_text= 'Zeit ',row= len(selected_columns), col= 1)\n",
    "    fig.update_layout(height=1200, width=1200, title_text='MillTurn-Motor {experiment}'.format(experiment= experiment))\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Distribution of Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= df.hist(figsize= (15,15),bins=50,xlabelsize=10, ylabelsize= 10)\n",
    "fig= ax[0][0].get_figure()\n",
    "plt.xlabel('values')\n",
    "plt.ylabel('counts')\n",
    "plt.savefig(assests_dir + 'row_data_hist.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= [ 'DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR', 'T_BETT',  'I_MOMENT_G','I_FELD','I_IST_BETR','V_QUER','magnet_temperature']\n",
    "#neededColumns= [ 'V_LAENGS','V_QUER','magnet_temperature']\n",
    "# df['T_MOTOR']= df['T_MOTOR'] -df['T_BETT']\n",
    "# df['T_LAGER']= df['T_LAGER'] -df['T_BETT']\n",
    "correlations= df[neededColumns].corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 15, 'color': 'black'})\n",
    "plt.savefig(assests_dir +'Korreation_Heatmap.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signalgättung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= [ 'T_MOTOR', 'T_LAGER', 'T_KLEMMUNG', target_variable,'magnet_temperature']\n",
    "correlations= df[neededColumns].corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "plt.savefig(assests_dir +'Korreation_Heatmap_kleinerform.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neededColumns= [ 'Feldstrom', 'Strom_Betrag',\n",
    "#        'Querspannung', 'Temp_Klemmung', 'Temp_Lager',\n",
    "#        'Temp_Motor', 'kuehleistung']#['Temp_Lager', 'Temp_Motor']\n",
    "neededColumns=  ['I_MOMENT_G', 'T_LAGER', 'T_MOTOR', 'DRZ5']#['DRZ5', 'T_KLEMMUNG', 'T_LAGER', 'T_MOTOR', 'T_BETT',  'I_MOMENT_G','I_FELD','I_IST_BETR','V_QUER']#[ 'T_MOTOR', 'T_LAGER', 'T_KLEMMUNG']\n",
    "target= df[target_variable].reset_index()\n",
    "reduced_data= df[neededColumns]\n",
    "reduced_data.reset_index(inplace= True)\n",
    "#print(reduced_data.info())\n",
    "#reduced_data= reduced_data.to_numpy(dtype= np.float64)\n",
    "pca1=PCA(n_components=4)\n",
    "transformed_data= pca1.fit_transform(reduced_data)\n",
    "new_df= pd.DataFrame(transformed_data,columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "new_df[target_variable]= target[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= new_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "plt.savefig(workspace+'pca_Korrelation_heatmap.jpg' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assests_dir= 'G:\\\\Innovations@HELLER\\\\DN\\\\KI\\\\Zollern-FH-MillTrunMotor\\\\Datasets\\\\workspace\\\\assets\\\\'\n",
    "preprocessor_name= 'preprocessor.p'\n",
    "window=40\n",
    "shift=1\n",
    "sample_rate=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(signals: np.ndarray):\n",
    "    return np.round(signals,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### window  represents time period by each entry in the buffer\n",
    "### Shift represents the jump from value to next one in the buffer\n",
    "### sampling rate \n",
    "def generateDataSource(signal_input=None, input_columns: list = [], output_length: int = 1, signal_output=None, window=1, shift=1, sample_rate=1):\n",
    "    #subsequence_len= (window -1) *shift + 1\n",
    "    subsequence_len= (window) *shift\n",
    "    Signal_Length = signal_input.shape[0]\n",
    "    num_samples = 1 + int((Signal_Length - subsequence_len) / sample_rate)\n",
    "    x = np.zeros(shape=(num_samples, window, signal_input.shape[1]))\n",
    "    y = np.zeros(shape=(num_samples, output_length, 1))\n",
    "    for i in range(num_samples):\n",
    "        x[i] = np.asarray([signal_input[i*sample_rate + j * shift] for j in range(0,window)])\n",
    "        y[i] = signal_output[i*sample_rate + (window-1) * shift :i*sample_rate+ (window-1) * shift + output_length]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(training_data)\n",
    "df:pd.DataFrame= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    df_file = pd.read_csv(file)\n",
    "    if nominated_signal in df_file.columns.to_list():\n",
    "        df_file[to_smooth_signals]=df_file[nominated_signal]\n",
    "    df_file= smoothing_data(df_file, window= smoothing_window, signals=to_smooth_signals)\n",
    "    df_file['file_name']= file\n",
    "    print('Current File: ', file)\n",
    "    if df is None:\n",
    "        df= df_file\n",
    "    else:\n",
    "        df= pd.concat([df, df_file], axis=0)\n",
    "###Filtering motor Temperature\n",
    "df= df[df['T_MOTOR'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_t_bett(data: np.ndarray):\n",
    "    result = data[:, 1:]\n",
    "    result[:, 0] = result[:, 0] - data[:, 0]\n",
    "    result[:, 1] = result[:, 1] - data[:, 0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.asanyarray([[-5,5,-10],\n",
    "#                [-5,5,-9],\n",
    "#                [-5,5,-8]])\n",
    "# a[:,-1]= np.abs(a[:,-1])\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drz_abs_value(data):\n",
    "    if type(data) is pd.DataFrame:\n",
    "        result= data.to_numpy()\n",
    "    else:    \n",
    "        result= data\n",
    "    result[:,-1]= np.abs(result[:,-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler2 = StandardScaler(with_mean=True, with_std=True)\n",
    "pca= PCA(n_components= 3)\n",
    "pipeline = Pipeline(steps=[ ('abs_drz', FunctionTransformer(get_drz_abs_value)),('stdscaler', scaler),('pca', pca),('scaler', scaler2)])#('abs_drz', FunctionTransformer(get_abs_value)),,('pca', pca),('scaler', scaler2)#('t_bett_removal', FunctionTransformer(remove_t_bett)),,('rounding2', FunctionTransformer(rounding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neededColumns= ['I_FELD', 'T_LAGER', 'T_MOTOR']#['DRZ5',  'T_LAGER', 'T_MOTOR','T_KLEMMUNG', 'T_BETT',  'I_MOMENT_G','I_FELD','I_IST_BETR','V_QUER']#['T_LAGER', 'T_MOTOR', 'I_FELD']#, 'T_KLEMMUNG'\n",
    "target= df[target_variable].reset_index().round(decimals=2)\n",
    "reduced_data= df[neededColumns]\n",
    "reduced_data.reset_index(inplace= True)\n",
    "reduced_data= reduced_data[neededColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_t_bett(reduced_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data= pipeline.fit_transform(reduced_data)\n",
    "dump(pipeline, str(assests_dir + preprocessor_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.steps[1][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df= pd.DataFrame(transformed_data,columns=['PC1', 'PC2', 'PC3'])\n",
    "# new_df['Temperatur_Magnet']= target['Temperatur_Magnet']\n",
    "# correlations= new_df.corr()\n",
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# plt.title(' Heat-Map für die Korrelationsmatrix')\n",
    "# ax= sn.heatmap(correlations, annot=True, vmin=-1, vmax=1, cmap='rainbow', annot_kws={\"size\": 10, 'color': 'black'})\n",
    "# plt.savefig(assests_dir +'best_pca_Korrelation_heatmap.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(training_data)\n",
    "df:pd.DataFrame= None\n",
    "x= None\n",
    "y= None\n",
    "batch_size= 1000000000\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    if nominated_signal in df_file.columns.to_list():\n",
    "        df_file[to_smooth_signals]=df_file[nominated_signal]\n",
    "    #df_file['l1']= df_file['T_LAGER'].abs() * df_file['T_MOTOR']\n",
    "    df_file= smoothing_data(df_file, window= smoothing_window, signals=to_smooth_signals)\n",
    "    # plt.plot(df_file[to_smooth_signals])\n",
    "    # plt.plot(df_file[target_variable])\n",
    "    # plt.show()\n",
    "    #print('Has Nans= ', df_file.isna().any())\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file[target_variable]\n",
    "    if batch_size > len(df_file):\n",
    "        batch_size= len(df_file)\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    if x is None:\n",
    "        x= partitions\n",
    "        y= target\n",
    "    else:\n",
    "        x= np.concatenate((x, partitions), axis= 0)\n",
    "        y= np.concatenate((y, target), axis= 0)\n",
    "    print('X: ', x.shape, ' y:', y.shape)\n",
    "    print('batch_size= ', batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(validation_data)\n",
    "df:pd.DataFrame= None\n",
    "xtest= None\n",
    "ytest= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    df_file['l1']= df_file['T_LAGER'].abs() * df_file['T_MOTOR']\n",
    "    df_file= smoothing_data(df_file, window= smoothing_window, signals=to_smooth_signals)\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file[target_variable]\n",
    "    # plt.plot(df_file[to_smooth_signals])\n",
    "    # plt.plot(df_file[target_variable])\n",
    "    # plt.show()\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    if xtest is None:\n",
    "        xtest= partitions\n",
    "        ytest= target\n",
    "    else:\n",
    "        xtest= np.concatenate((xtest, partitions), axis= 0)\n",
    "        ytest= np.concatenate((ytest, target), axis= 0)\n",
    "    print('X_test: ', xtest.shape, ' y_test:', ytest.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import selu\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Activation, Dense, BatchNormalization\n",
    "from keras.losses import LossFunctionWrapper, mean_absolute_error, mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import L1L2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    error = tf.add(\n",
    "                    tf.reduce_max(tf.abs((y_true - y_pred))),\n",
    "                    mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "def smoothed_max_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    a=0.1\n",
    "    diffs= tf.abs((y_true - y_pred))\n",
    "    muls= tf.multiply(a, diffs)\n",
    "    exps= tf.exp(muls)\n",
    "    weighted_muls= tf.multiply(diffs, exps)\n",
    "    nominater= tf.reduce_sum(weighted_muls)\n",
    "    denominator= tf.reduce_sum(exps)\n",
    "    boltzmann_operator= nominater/denominator\n",
    "    error = tf.add(boltzmann_operator, mean_absolute_error(y_true, y_pred))\n",
    "    return error\n",
    "special_loss = LossFunctionWrapper(smoothed_max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###v0.0.10\n",
    "# from keras.initializers import RandomNormalV2, RandomUniform\n",
    "# def build_Model_LSTM(input_shape, activation_Function = selu,\n",
    "#     dropout = 0.0,\n",
    "#     l1_v = 0.00,\n",
    "#     l2_v = 0.00,\n",
    "#     structure= [],\n",
    "#     optimizer= 'Nadam'):\n",
    "#     #structure=  [n_units for i in range(0,n_hidden_layers)]#[50,50,40,40,30,30,20]##   \n",
    "#     unroll = False\n",
    "#     kernal_init = RandomNormalV2()#'he_normal'#RandomUniform()  # \n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=input_shape))\n",
    "#     # model.add(Dense(50, activation=activation_Function))\n",
    "#     # model.add(Dense(30, activation=activation_Function))\n",
    "#     model.add(Dense(20, activation=activation_Function))\n",
    "#     for i in range(1,len(structure)+1):\n",
    "#         layer_size= structure[i-1]\n",
    "#         if i == len(structure):\n",
    "#             model.add(LSTM(layer_size,stateful= False,return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "#         else:\n",
    "#             model.add(LSTM(layer_size,stateful= False, return_sequences=True,unroll=unroll, kernel_initializer= kernal_init,dropout=dropout,  kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "#         #model.add(BatchNormalization())\n",
    "#         model.add(Activation(activation=activation_Function))\n",
    "#         #model.add(BatchNormalization())\n",
    "#         #model.add(PReLU())\n",
    "#     model.add(Dense(10, activation=activation_Function))\n",
    "#     #model.add(Dense(5, activation=activation_Function))\n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "#     model.compile(optimizer=optimizer, loss= mean_squared_error, metrics=[ max_loss])#'Adagrad'\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomNormalV2, RandomUniform\n",
    "def build_Model_LSTM(input_shape, activation_Function = selu,\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.00,\n",
    "    l2_v = 0.00,\n",
    "    structure= [],\n",
    "    optimizer= 'Nadam'):\n",
    "    #structure=  [n_units for i in range(0,n_hidden_layers)]#[50,50,40,40,30,30,20]##   \n",
    "    unroll = False\n",
    "    kernal_init = RandomNormalV2()#'he_normal'#RandomUniform()  # \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    # model.add(Dense(50, activation=activation_Function))\n",
    "    #model.add(Dense(20, activation=activation_Function))\n",
    "    #model.add(Dense(10, activation=activation_Function))\n",
    "    for i in range(1,len(structure)+1):\n",
    "        layer_size= structure[i-1]\n",
    "        if i == len(structure):\n",
    "            model.add(LSTM(layer_size,stateful= False,return_sequences=False,unroll=unroll,kernel_initializer= kernal_init, dropout=dropout, kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        else:\n",
    "            model.add(LSTM(layer_size,stateful= False, return_sequences=True,unroll=unroll, kernel_initializer= kernal_init,dropout=dropout,  kernel_regularizer=L1L2(l1=l1_v, l2=l2_v)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation_Function))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(PReLU())\n",
    "    #model.add(Dense(10, activation=activation_Function))\n",
    "    #model.add(Dense(5, activation=activation_Function))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=optimizer, loss= mean_squared_error, metrics=[ max_loss])#'Adagrad'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(old_value, old_min, old_max, new_max, new_min):\n",
    "    new_value= ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "    old_min = 0\n",
    "    old_max = 0\n",
    "    y_train_rounded = np.round(y_train,decimals=3)\n",
    "    N = len(y_train_rounded)\n",
    "    y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "    weights = np.ones(shape=(N,),dtype= np.float32)\n",
    "    for i in range(0, N):\n",
    "        current_value = y_train_rounded[i]\n",
    "        if current_value >= 60:\n",
    "            weights[i]=  current_value#(current_value/max(y_train_rounded)) * 2\n",
    "        else:\n",
    "            weights[i]=  current_value\n",
    "    old_min= weights.min()\n",
    "    old_max= weights.max()\n",
    "    new_min = 1\n",
    "    new_max = 2\n",
    "    #weights_scaled= np.apply_along_axis(map, 1, weights)\n",
    "    weights_scaled= np.asanyarray([ map(weights[i], old_min, old_max, new_max, new_min) for i in range(0, weights.shape[0])]).reshape(weights.shape)\n",
    "    #weights = weights - weights.min() + 1\n",
    "    #weights = (weights - weights.min())/(weights.max() - weights.min())\n",
    "    fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=weights_scaled,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 1, col= 1)\n",
    "    fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "    , row= 2, col= 1)\n",
    "    fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "    fig.show()\n",
    "\n",
    "    return weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_weights(y_train: np.ndarray, occurance_threshold= 0):\n",
    "#     old_min = 0\n",
    "#     old_max = 0\n",
    "#     y_train_rounded = np.round(y_train)\n",
    "#     N = len(y_train_rounded)\n",
    "#     y_train_rounded = np.reshape(y_train_rounded, newshape=(N,))\n",
    "#     weights = np.ones(shape=(N,),dtype= np.float32)\n",
    "#     for i in range(0, N):\n",
    "#         current_value = y_train_rounded[i]\n",
    "#         occurences = np.count_nonzero(y_train_rounded == current_value)\n",
    "#         if occurences <= occurance_threshold:\n",
    "#              weights[i]=  y_train_rounded[i] *N/(occurences+1)#N/occurences # 0\n",
    "#         else:\n",
    "#             weights[i] = y_train_rounded[i]*N/(occurences+1)#(8 if y_train_rounded[i] < 20 else 1)\n",
    "#     old_min= weights.min()\n",
    "#     old_max= weights.max()\n",
    "#     new_min = 1\n",
    "#     new_max = 2\n",
    "#     #weights_scaled= np.apply_along_axis(map, 1, weights)\n",
    "#     weights_scaled= np.asanyarray([ map(weights[i], old_min, old_max, new_max, new_min) for i in range(0, weights.shape[0])]).reshape(weights.shape)\n",
    "#     #weights = weights - weights.min() + 1\n",
    "#     #weights = (weights - weights.min())/(weights.max() - weights.min())\n",
    "#     fig= make_subplots(rows=2,cols=1,shared_xaxes= True, print_grid= True,  vertical_spacing=0.02)\n",
    "#     fig.add_trace(go.Line(y=weights_scaled,name='weight of Labels')#visualisation_selected_Columns[-2])\n",
    "#     , row= 1, col= 1)\n",
    "#     fig.add_trace(go.Line(y=np.reshape(y_train, newshape=(N,)),name='Labels')#visualisation_selected_Columns[-2])\n",
    "#     , row= 2, col= 1)\n",
    "#     fig.update_layout(height=900, width=900, title_text= 'weight of Labels')\n",
    "#     fig.show()\n",
    "\n",
    "#     return weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=str(assests_dir+'best_model.h5'), monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', mode='min', patience=1500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain, xtest, ytrain, ytest = train_test_split(x,y,shuffle= False, test_size=0.1, random_state=490)\n",
    "xtrain= x\n",
    "ytrain= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(xtrain).any())\n",
    "print(np.isnan(ytrain).any())\n",
    "print(np.isnan(xtest).any())\n",
    "print(np.isnan(ytest).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = calculate_weights(y_train=ytrain, occurance_threshold= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_Model_LSTM( (xtrain.shape[1], xtrain.shape[2]),activation_Function = 'tanh',\n",
    "    dropout = 0.0,\n",
    "    l1_v = 0.000,\n",
    "    l2_v = 0.000,\n",
    "    structure=[5,5],\n",
    "    optimizer= 'Nadam')#, loss_weights=weights\n",
    "reset= True##############################################################################################################\n",
    "try:\n",
    "    if not reset:\n",
    "        #model.set_weights(model_w)\n",
    "        #model.load_weights(filepath=weights_path)\n",
    "        print('Previous weights loaded Successfully')\n",
    "except:\n",
    "    print('No Previous weights')\n",
    "print('input shape ', model.input_shape)\n",
    "print(model.output_shape)\n",
    "print(model.summary())\n",
    "summary = model.fit(x=xtrain, y=ytrain, shuffle=False, batch_size= int(batch_size),  epochs=15000, validation_data=(xtest, ytest),  #int(batch_size * 0.5)\n",
    "                    callbacks=[earlyStopping], verbose=2, workers=32, use_multiprocessing=True, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(assests_dir+'model.h5'), save_format='h5')\n",
    "model.save(str(assests_dir+'model.keras'))\n",
    "input_signature = [tf.TensorSpec([None, xtrain.shape[1], xtrain.shape[2]], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model=model, input_signature=input_signature, opset=10)\n",
    "onnx.save_model(onnx_model, str(assests_dir+'model.onnx'))\n",
    "print(\".onnx model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(filepath=weights_path)\n",
    "metric_loss = summary.history['loss']\n",
    "metric_val_loss = summary.history['val_loss']\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True,\n",
    "                    print_grid=True,  vertical_spacing=0.02)\n",
    "fig.add_trace(go.Line(y=metric_loss, name='Training Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Line(y=metric_val_loss , name='Validation Loss'), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=mae_loss, name='Training {}'.format('mae')), row=1, col=1)\n",
    "#fig.add_trace(go.Line(y=val_mae_loss, name='Validation {}'.format('mae')), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_layout(height=900, width=900, title_text='Training Curve')\n",
    "fig.show()\n",
    "pio.write_image(fig, str(assests_dir+'trainingCurve.jpg'), format='jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_steps = np.linspace(0, 130, 10)\n",
    "print(model.summary())\n",
    "pred = model.predict(xtrain)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#org_targets= post_processer.inverse_transform(targets_post)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True (C°)')\n",
    "plt.ylabel('Predicted (C°)')\n",
    "pred.reshape((-1,))\n",
    "plt.scatter(ytrain, pred, label='predictions_train', color='blue')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line', color='red')\n",
    "plt.legend()\n",
    "plt.savefig(str(assests_dir+'training_results.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_steps = np.linspace(0, 130, 10)\n",
    "print(model.summary())\n",
    "pred = model.predict(xtest)\n",
    "#print (' For All Data mse= ',mean_squared_error(targets,pred), ' mae= ', mean_absolute_error(targets,pred), ' mape= ', mean_absolute_percentage_error(targets,pred))\n",
    "#pred= post_processer.inverse_transform(pred)\n",
    "#org_targets= post_processer.inverse_transform(targets_post)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Results of LSTM Algorithm')\n",
    "plt.xlabel('True (C°)')\n",
    "plt.ylabel('Predicted (C°)')\n",
    "pred.reshape((-1,))\n",
    "print(ytest.shape)\n",
    "plt.scatter(ytest, pred, label='predictions_Testing', color='blue')\n",
    "plt.plot(x_steps, x_steps, label='Optimal line', color= 'red')\n",
    "plt.legend()\n",
    "plt.savefig(str(assests_dir+'testing_results.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model= load_model(str(assests_dir+'model.h5'), compile= False)\n",
    "pipeline= load(str(assests_dir + preprocessor_name))\n",
    "print(model.input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(validation_data)\n",
    "df:pd.DataFrame= None\n",
    "for file in glob.glob('*.csv'):\n",
    "    print('Read File= ', file)\n",
    "    df_file = pd.read_csv(file)\n",
    "    df_file[to_smooth_signals]= df_file[nominated_signal]\n",
    "    df_file['l1']= df_file['DRZ5'].abs() * df_file['T_MOTOR']\n",
    "    df_file= smoothing_data(df_file, window= smoothing_window, signals=to_smooth_signals)\n",
    "    rough_data= df_file[neededColumns]\n",
    "    target= df_file[target_variable]\n",
    "    transformed_data= pipeline.transform(rough_data)\n",
    "    partitions, target = generateDataSource(signal_input=transformed_data, input_columns=neededColumns, output_length=1, signal_output=target, window=window, shift=shift, sample_rate=sample_rate)\n",
    "    pred= model.predict(partitions)\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, print_grid=True,  vertical_spacing=0.02)\n",
    "    fig.add_trace(go.Line(y=target.flatten(), name='Magent_temp_True'), row=1, col=1)\n",
    "    fig.add_trace(go.Line(y=pred.flatten(), name='Magent_temp_Pred'), row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Magnet Temp (C°)', row=1, col=1)\n",
    "    fig.add_trace(go.Line(y=target.flatten() - pred.flatten(), name='Prediction Error (True -Predicted)'), row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Prediction Error (C°)', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Zeit 1 = 2 Sek', row=2, col=1)\n",
    "    fig.update_layout(height=900, width=900, title_text='Vorhersage der Magnet-Temp {file}'.format(file = file))\n",
    "    fig.show()\n",
    "    pio.write_image(fig, str(assests_dir+'{file}.jpg'.format(file= file)), format='jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
